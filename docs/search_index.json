[["index.html", "Mathematics for Scientists and Engineers Preface", " Mathematics for Scientists and Engineers Dr Mark Callaway 2022-11-08 Preface These notes have been compiled for use alongside the University of Exeter modules CSM1027, CSM1033, CSM1040 and CSM1041. They are a work in progress – please let me know if you find any errors or have any feedback. Contact: m.callaway@exeter.ac.uk Recommended text: Engineering Mathematics, John Bird. Available online through the UoE library here: https://encore.exeter.ac.uk/iii/encore/record/C__Rb4511752 "],["foundations.html", "Chapter 1 Foundations 1.1 Numbers 1.2 Algebra", " Chapter 1 Foundations We begin with some foundational material that forms the basis of all applied mathematics. 1.1 Numbers We deal with various number systems in mathematics. The set of natural numbers are the familiar counting numbers: \\[1, 2, 3,\\dotsc\\] We denote these by the symbol \\(\\mathbb{N}\\). The integers are the “whole numbers”, which include zero and negative numbers: \\[\\dots,-2,-1,0,1,2,\\dots\\] We denote these by the symbol \\(\\mathbb{Z}\\). We sometimes want to focus on just the positive integers, which is another name for the natural numbers. The non-negative integers are the numbers: \\[0, 1, 2, 3,\\dotsc\\] (the natural numbers and zero). The negative integers are the numbers: \\[-1, -2, -3,\\dotsc\\] The rational numbers are numbers which can be written in the form \\(\\frac{a}{b}\\) where \\(a\\) and \\(b\\) are integers, with \\(b\\neq 0\\), so numbers like \\[\\frac{1}{2}, \\frac{3}{2}, -\\frac{4}{5}, \\dotsc\\] Note these include the integers since they can be written as \\(\\frac{a}{1}\\). Rational numbers are denoted \\(\\mathbb{Q}\\). The ancient Greeks discovered that there are numbers that are not rational – so-called irrational numbers. For example, \\(\\sqrt{2}, \\pi\\) and \\(e\\) are all irrational numbers. One way of characterising irrational numbers is that they have a decimal representation that never repeats… \\[ \\pi=3.141592653589793238462643383279502884197169399375105820\\dots \\] In contrast, all rational numbers have an eventually repeating decimal expansion (after perhaps some initial jumble of digits, there is a repeating pattern), for example: Rational Number Repeating pattern \\(\\frac{1}{3}=0.333\\dots\\) repeating \\(3\\)’s \\(\\frac{3}{2}=1.5000\\dots\\) repeating \\(0\\)’s after the first decimal digit, which we normally don’t write down! \\(\\frac{219}{1750}=0.125142857142857\\dots\\) repeating sequence \\(142857\\) of length \\(6\\) after the first \\(3\\) decimal digits The real numbers consist of all rational and irrational numbers together. We can think of real numbers as points on a continuous straight line, or as numbers that possibly have a decimal representation requiring an infinite number of digits1. The set of real numbers is denoted by \\(\\mathbb{R}\\). We learn about all of the above numbers early on in our mathematical education. However, there is a further set of numbers that is very useful in science and engineering that you may not have encountered before: the complex numbers, denoted by \\(\\mathbb{C}\\). These are not as intuitive as real numbers and we explore them in detail in section 5. 1.2 Algebra This section contains some basic algebraic rules. Fluency with notation and manipulations allows us to progress to higher level mathematics with ease. We are familiar with the basic rules of arithmetic: \\(+, -, \\times, \\div\\). When an expression contains multiple operations we can use brackets to indicate the order in which the operations are executed. We also commonly use powers (exponents). When evaluating an expression, remember to use the mnemonic BEDMAS, which defines the precedence of these operations: B - Brackets E - Exponents D - Division M - Multiplication A - Addition S - Subtraction Division and Multiplication can be carried out in either order, as can Addition and Subtraction. We’ll recap the rules of Brackets and Exponents in the next sections. Example 1.1 (BEDMAS) We have \\[3+2\\times 4=11,\\] whilst \\[(3+2)\\times 4 = 20.\\] 1.2.1 Rules of Exponents In their most basic form, we use exponents as a shorthand for multiplication of a number by itself some positive integer number of times. For example: \\(5\\times 5 = 5^2\\) spoken as “5 squared” \\(5\\times 5\\times 5=5^3\\) spoken as “5 cubed” and more generally, for any real number \\(x\\) and positive integer \\(n\\), \\(\\underbrace{x\\times \\dotsb \\times x}_{\\text{with }n\\, x\\text{&#39;s}} = x^n\\) spoken as “x to the power \\(n\\)” The number \\(x\\) is known as the base and the number \\(n\\) is known as the power, index or exponent. Now for any real number \\(x\\) and positive integers \\(a\\) and \\(b\\), it should be clear that the following rule holds: Theorem 1.1 (Rule 1) \\[x^ax^b=x^{a+b}\\] since we are multiplying \\(a\\) \\(x\\)’s by a further \\(b\\) \\(x\\)’s to give a total of \\(a+b\\) \\(x\\)’s multiplied together. Extending this idea, we also have: Theorem 1.1 (Rule 2) \\[(x^a)^b=x^{ab}\\] Exercise 1.1 Why does Rule 2 hold? Can you prove it using Rule 1? Furthermore, show that \\((x^a)^b=(x^b)^a\\). So far we have allowed the exponents to be positive integers. Can we make sense of negative integers as exponents? Consider for \\(a\\) and \\(b\\) as positive integers the product \\[x^a\\frac{1}{x^b}=\\frac{x^a}{x^b}\\] For arguments sake let’s take \\(a&gt;b\\), then since we have \\(a\\) \\(x\\)’s on the top and \\(b\\) \\(x\\)’s on the bottom, we can cancel the common factors to leave \\(a-b\\) \\(x\\)’s. So, \\[\\begin{equation} x^a\\frac{1}{x^b}=\\frac{x^a}{x^b}=x^{a-b} \\tag{1.1} \\end{equation}\\] and following Rule 1 we could think of this as \\(x^{a-b}=x^{a+(-b)}=x^ax^{-b}\\), and therefore using a negative exponent \\(-b\\) gives the reciprocal of \\(x^b\\). Theorem 1.1 (Rule 3) \\[x^{-a}=\\frac{1}{x^a}\\] Next, we can consider the value of \\(x^0\\). In (1.1) with the particular case \\(b=a\\), we have \\[1=\\frac{x^a}{x^a}=x^{a-a}=x^0\\] so we get that for any non-zero2 value of \\(x\\): Theorem 1.2 (Rule 4) \\[x^0=1\\] Now lets try to make sense of a power of the form \\(x^{\\frac{a}{b}}\\) where \\(a\\) is an integer and \\(b\\) is a positive integer. That is, we now allow for fractional (otherwise known as rational) exponents. Since we have \\(\\frac{a}{b}=a\\frac{1}{b}\\) we can write (using Rule 2): \\[x^{\\frac{a}{b}}=(x^a)^{\\frac{1}{b}}.\\] In the case \\(b=a\\), we therefore have \\[x=x^1=x^{\\frac{a}{a}}=(x^{\\frac{1}{a}})^a.\\] This means that raising \\(x^\\frac{1}{a}\\) to the power \\(a\\) gets us back to \\(x\\), hence the number \\(x^\\frac{1}{a}\\) is the \\(a^\\text{th}\\) root of \\(x\\). For example \\((3^\\frac{1}{2})^2=3^{2\\frac{1}{2}}=3^1=3\\), so we must have \\(3^\\frac{1}{2}=\\sqrt{3}\\). Theorem 1.3 (Rule 5) \\[x^{\\frac{1}{a}}=\\sqrt[a]{x}.\\] In particular the square root is \\(x^{\\frac{1}{2}}=\\sqrt{x}\\) and the cube root is \\(x^{\\frac{1}{3}}=\\sqrt[3]{x}\\); we tend to use the root notation for expressions containing square or cube roots on their own, and use the power notation when combining different roots. It turns out that we can extend exponents beyond rational numbers to any real number, with all of the above rules still holding. Proving this is beyond the scope of these notes. Something’s missing: we said \\(x\\) can be any real number, but then how do we interpret something like \\(y=(-1)^\\frac{1}{2}\\)? That is, \\(y\\) must be a number such that when it is multiplied by itself it gives the negative number \\(-1\\), but this is impossible for a real number! Answering this conundrum requires us to invent a new set of numbers, called complex numbers – see section 5. A summary of the rules: Theorem 1.4 (Rules of Exponents) \\(x^ax^b=x^{a+b}\\) \\((x^a)^b=x^{ab}\\) \\(x^{-a}=\\frac{1}{x^a}\\) \\(x^0=1\\) \\(x^{\\frac{1}{a}}=\\sqrt[a]{x}.\\) 1.2.2 Brackets We use brackets to set the precedence (or order) of evaluating the different parts of an expression. We evaluate the innermost brackets first. Note we usually omit the multiplication sign when multiplying a bracketed expression. Example 1.2 (Evaluating brackets) \\[5(((2+4)\\div (5-2))+1)=5((6\\div 3)+1)=5(2+1)=5\\times 3 = 15\\] Sometimes you might find it helpful to add “redundant” brackets to make the calculation clearer. For example, we could write \\[3+2\\times 4=11\\] as \\[3+(2\\times 4)=11.\\] The brackets are not really needed in the second version since we get the same result from following BEDMAS, but adding brackets can make an expression easier to read and evaluate. 1.2.2.1 Expansion Recall that we can expand expressions involving multiplication of brackets such as the following: \\[a(b+c)=ab+ac\\] \\[(a+b)(c+d)=ac+ad+bc+bd\\] \\[(a+b)^2=(a+b)(a+b)=a^2+ab+ba+b^2=a^2+b^2+2ab\\] \\[(ab)^2=(ab)(ab)=a^2b^2.\\] We often want to expand expressions in order to simplify them by grouping together like-terms, for example: \\[(a+b)(b+c)+2ac=ab+ac+b^2+bc+2ac=ab+3ac+b^2+bc.\\] In the case of the sum of two numbers (a binomial term) to a postive integer power we have a general formula for the expansion, i.e. something of the form \\[(a+b)^n.\\] which involves multiplying out the brackets \\(n\\) times. We start by looking at a slightly simpler form \\[(1+x)^n.\\] Let’s take a look at the expansion for the first few values of \\(n\\). \\(n=0:\\) \\((1+x)^0 = 1\\) \\(n=1:\\) \\((1+x)^1=1+x\\) \\(n=2:\\) \\((1+x)^2=(1+x)(1+x)=x^2+2x+1\\) \\(n=3:\\) \\((1+x)^3=(1+x)(1+x)^2=(1+x)(x^2+2x+1)=x^3+3x^2+3x+1\\) The coefficients of \\(x\\) form the \\(n^\\text{th}\\) row of Pascal’s triangle. The general formula is: \\[(1+x)^n = \\binom{n}{0}x^0+\\binom{n}{1}x^1+\\binom{n}{2}x^2+\\dotsb+\\binom{n}{n-1}x^{n-1}+\\binom{n}{n}x^n\\] where \\(\\binom{n}{k}\\) is the binomial coefficient \\[\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\] and \\(n!\\) is the factorial of \\(n\\): \\[n!=n\\times(n-1)\\times(n-2)\\times\\dotsb \\times3\\times 2\\times 1\\]. Given the more general form \\((a+b)^n\\), we can first extract a factor of \\(a\\) to give \\[(a+b)^n=(a(1+\\frac{b}{a}))^n=a^n(1+\\frac{b}{a})^n\\] so we now just need to evaluate \\((1+\\frac{b}{a})^n\\) using the formula and substituting \\(x\\) with \\(\\frac{b}{a}\\). Alternatively, the full formula is: \\[(x+y)^n = \\binom{n}{0}x^ny^0+\\binom{n}{1}x^{n-1}y^1+\\binom{n}{2}x^{n-2}y^2+\\dotsb+\\binom{n}{n-1}x^1y^{n-1}\\binom{n}{n}x^0y^n.\\] 1.2.2.2 Factorisation At other times, we wish to simplify by extracting common factors from an expression, which we call factorisation. In the simplest form, this could just be factorising a single number \\[2x+2y=2(x+y)\\] Or, it could involve extracting more complicated factors, for example \\[2xy+6x^2y=2xy(1+3x)\\] \\[x^2-3x-10=(x+2)(x-5)\\] The second example here is factorising a quadratic which we can do systematically by finding the roots – we will see how to do this later in section 2.1.3. 1.2.3 Simplifiying Expressions In order to simplify an expression we can carry out the following proceedures: Expand brackets Collect together any like-terms, e.g. \\(2a+3b+a=3a+3b\\), or \\(a^3a^2+b^2=a^5+b^2\\) Simplify any fractions: use common denominators, e.g. \\(\\frac{a}{b}+\\frac{c}{d}=\\frac{da+bc}{bd}\\); cancel any common factors, e.g. \\(\\frac{ab}{ac}=\\frac{b}{c}\\) Factorise Note: these are “rules of thumb”, as there usually does not exist such a thing as a “simplest” form for a given algebraic expression. The meaning of “simple” is dependent on what we want to gain from it. The intention is to put the expression in a form that is easier to understand (in the given context) and which usually uses fewer symbols (but not always). Real numbers require quite a bit of work to define in a rigorous mathematical way and we will not do this here; the interested reader should look up “construction of real numbers by Dedekind cuts” or “construction of real numbers by Cauchy sequences”.↩︎ We need to be more careful when \\(x=0\\), since we would have \\(\\frac{0^a}{0^a}=\\frac{0}{0}\\), which is an indeterminate form so the argument is not valid; it turns out \\(0^0\\) is also an indeterminate form.↩︎ "],["equations-and-inequalities.html", "Chapter 2 Equations and Inequalities 2.1 Equations 2.2 Inequalities 2.3 Common mistakes!", " Chapter 2 Equations and Inequalities 2.1 Equations An equation indicates the equality of two mathematical expressions: what is on the left of the \\(=\\) sign is the same as what is on the right of the \\(=\\) sign. When manipulating equations, we must perform the same operation to each side in order to maintain equality – think about a traditional mechanical balance scale, where if we were to change the mass on one side of the balance we would need to do the same to the other side in order to maintain balance. We will use the abreviations l.h.s. for left-hand-side and r.h.s. for right-hand-side. There are a few subtly different types of equations; the main two are formulas and conditional equations. 2.1.1 Formulas Equations are often regarded as formulas, which can be used to calculate the value of a mathematical or physical quantity in terms of other known quantities. In this case, we write a single dependent variable on the left hand side and the right hand side will contain independent variables and constants. The value of the dependent variable depends on the values of the independent variables and is obtained by evaluating the right hand side when the values of the independent variables are known. For example, the formula for the area \\(A\\) of a circle is \\[A=\\pi r^2\\] where the independent variable \\(r\\) is the radius of the cirle and \\(\\pi\\) is the mathematical constant \\(3.141...\\). By plugging in a particular value for \\(r\\) we then obtain a value for the area \\(A\\). We often want to re-arrange equations to make a different variable the dependent variable, or subject, of the formula, i.e. to place it alone on the left hand-side. Re-arranging formulae is also known as transposition. For example, if we knew the area of a cirlce and wanted to calculate the corresponding radius, we can re-arrange the equation to make \\(r\\) the subject: \\[r=\\sqrt{\\frac{A}{\\pi}}.\\] In obtaining this, we carry out the “opposite” of the operations on the right hand side to both sides of the equation. These opposites are: addition and subtraction; multiplication and division; powers and roots. Let’s see this step by step in our example. We start with \\[A=\\pi r^2.\\] Then, divide both sides by \\(\\pi\\) \\[\\frac{A}{\\pi}=\\frac{\\pi}{\\pi}r^2=r^2\\] so that \\(r^2\\) is no longer multiplied by \\(\\pi\\). Now, to get from \\(r^2\\) to \\(r\\) we need to take the square root of both sides \\[\\sqrt{\\frac{A}{\\pi}}=\\sqrt{r^2}=r\\] and finally, we put the subject on the left hand side (since we read from left to right it is simply convention to do this, but mathematically it makes no difference since it is still an equality). 2.1.2 Conditional equations Another type of equation is a conditional equation. These may not hold true for all values of the variables and we will want to find the values for which equality is true. For example, \\[5x+4=19\\] is only true when \\(x=3\\). We can solve this equation by making \\(x\\) the subject: subract \\(4\\) from both sides \\[5x=15\\] divide both sides by \\(5\\) \\[x=3.\\] Note that an equation may have more than one solution, or perhaps no solutions. For example, \\[x^2=4\\] has two possible solutions, either \\(x=2\\) or \\(x=-2\\). On the other hand, the equation \\[x+2=x+3\\] has no solutions. To see this, subtract \\(x+2\\) from both sides and we obtain \\[0=1\\] which is false, so no value of \\(x\\) can give us equality. 2.1.3 Quadratic equations A particularly common form of equations that arise in science and engineering are quadratic equations. These are equations of the form \\[ax^2+bx+c=0\\] where \\(a\\), \\(b\\) and \\(c\\) are constants and we wish to find the values of \\(x\\) satisfying this equation. We explore three different methods below. 2.1.3.1 Factorisation by inspection First consider a quadratic where \\(a=1\\), that is \\[x^2+bx+c=0.\\] If we can write a quadratic equation as a product of two simple linear factors in the form3: \\[x^2+bx+c = (x+\\alpha)(x+\\beta)\\] then \\(x=-\\alpha\\) and \\(x=-\\beta\\) are solutions to \\(ax^2+bx+c=0\\), since for \\(x=-\\alpha\\) \\[(-\\alpha+\\alpha)(-\\alpha+\\beta)=0(-\\alpha+\\beta)=0\\] and similarly for \\(x=-\\beta\\). Furthermore, these will be the only two solutions: it is possible there are two different solutions \\(\\alpha\\neq\\beta\\), one “repeated” solution where \\(\\alpha=\\beta\\) so that the quadratic factorises as \\((x+\\alpha)(x+\\alpha)=(x+\\alpha)^2\\), or there may be no solutions4. In many cases it is easy to spot the values of \\(\\alpha\\) and \\(\\beta\\). To see this, let’s work backwards and expand the brackets: \\[(x+\\alpha)(x+\\beta)=x^2+\\beta x +\\alpha x +\\alpha\\beta = x^2 + (\\alpha + \\beta)x + \\alpha\\beta\\] so we just need to find the values of \\(\\alpha\\) and \\(\\beta\\) that sum to \\(b\\) and multiply together to give \\(c\\). Example 2.1 If we have \\[x^2+5x+6=0\\] we are looking for two numbers that sum to make \\(5\\) and multiply to make \\(6\\), so the answer is clearly \\(x=2\\) and \\(x=3\\), giving the factorisation \\[x^2+5x+6=(x+2)(x+3).\\] If we have a more general quadratic where \\(a\\neq1\\), then we could start by taking out a factor of \\(a\\) \\[ax^2+bx+c=a\\left(x^2+\\frac{b}{a}x+\\frac{c}{a}\\right)=0\\] and then dividing both sides by \\(a\\) \\[x^2+\\frac{b}{a}x+\\frac{c}{a}=0\\] The values of \\(x\\) satisfying this new equation will be the same as those for the original, so we can now use the above method to find the solutions. However, sometimes we not only want the solution, but also want to know how to factorise the quadratic. In this case we shall have to keep the factor of \\(a\\). To factorise we just need to find the values of the coefficients in the factorisation that will give the answer we need when we expand the brackets. Example 2.2 Solve \\(4x^2 + 8x + 3 = 0\\) by factorising. We need the \\(x\\) terms to multiply together to make \\(4x^2\\), so we could have either \\[(4x + \\star)(x + \\star)\\] or \\[(2x + \\star)(2x + \\star)\\] and we need the constant terms to multiply together to make \\(3\\), so we could have either \\[(\\star + 3)(\\star + 1)\\] or \\[(\\star - 3)(\\star - 1).\\] The sum of the products of the outer terms and inner terms needs to be \\(8x\\) and the only way this is possible is with \\[(2x+3)(2x+1).\\] The solutions then come from solving for each factor being \\(0\\), giving \\[x=-\\frac{3}{2}\\quad\\text{ or }\\quad x= -\\frac{1}{2}\\] Not all quadratics will factorise “nicely”, that is to say, where the coefficients in the linear factors will be integers. We can first test if the quadratic will factorise with integer coefficients by computing the quantity \\[\\Delta = b^2 - 4ac\\] which is known as the discriminant of a quadratic: if this number is a perfect square, then the linear factors will have integer coefficients; if it is not a perfect square, the coefficients will not be integers. In the case where the coefficients are not integers it can be difficult to determine the coefficients by inspection and the following two methods can be applied instead. 2.1.3.2 Completing the square In this method, we first rewrite a quadratic equation \\[ax^2+bx+c=0\\] into the form \\[a(x-h)^2+k=0\\] that is, a squared term plus a constant, hence the name “completing the square”. We then solve by rearranging and taking sqaure roots \\[\\begin{align*} (x-h)^2+k&amp;=0\\\\ (x-h)^2&amp;=-k\\\\ x-h&amp;=\\pm\\sqrt{-k}\\\\ x&amp;=h\\pm\\sqrt{-k} \\end{align*}\\] where \\(\\pm\\) means there is one solution coming from taking the \\(+\\) option and the other from taking the \\(-\\) option. We just need to find \\(h\\) and \\(k\\). They are given by \\[h=-\\frac{b}{2a},\\quad k=c-\\frac{b^2}{4a}.\\] It is not necessary to remember these formulae, we can instead figure out the coefficients as we go along. Example 2.3 (Completing the Square) Consider the quadratic equation \\[2x^2+12x-26=0.\\] We first take out the factor \\(a=2\\) \\[2x^2+12x-26=2(x^2+6x-13)\\] Next we can make a square term that agrees with the first two terms by taking half the \\(x\\) coefficient (i.e. \\(6\\div2=3\\)): \\[(x+3)^2=x^2+6x+9.\\] This differs from the required result in the constant term by \\[-13-9=-22\\] and so we need to “complete the square” by adding \\(-22\\), to obtain \\[2x^2+6x+13=2[(x+3)^2-22]=2(x+3)^2-44.\\] The solutions are then \\[\\begin{align*} 2(x+3)^2-44&amp;=0\\\\ 2(x+3)^2&amp;=44\\\\ (x+3)^2&amp;=22\\\\ x&amp;=-3\\pm\\sqrt{22}. \\end{align*}\\] 2.1.3.3 The quadratic formula Sometimes it is difficult to find the solutions using the above methods and so thankfully we have a forumula for finding them (or to determine if there are no real number solutions). Theorem 2.1 (Quadratic Formula) The solutions to the quadratic equation \\[ax^2+bx+c=0\\] are given by \\[x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}.\\] If the quantity \\(\\Delta = b^2-4ac\\), known as the discriminant, is such that \\(\\Delta = 0\\) then there will be one repeated solution; if \\(\\Delta\\) is negative, then there are no real number solutions (since we cannot take the square root of a negative number). Exercise 2.1 Derive the quadratic formula. Hint: use the method of “completing the square” on a general quadratic equation. 2.1.4 Simultaneous equations Sometimes a problem will be formulated in terms of multiple equations with multiple, shared variables. The set of equations usually represent constraints between the variables and we need to determine which values of the variables are valid solutions, i.e. which values satisfy all of the equations simultaneously – hence such sets of equations are known as simultaneous equations. Sometimes we can solve simultaneous equations by rearranging one equation to set one of the variables as the subject and then substitute this into another equation in order to eliminate one of the variables. Example 2.4 Given the following two simultaneous equations in the variables \\(x\\) and \\(y\\) \\[\\begin{align*} 3x+2y&amp;=5\\\\ x-4y&amp;=1 \\end{align*}\\] we could rearrange the second equation to make \\(x\\) the subject \\[x=1+4y\\] and then substitute this back in for \\(x\\) in the first equation \\[3(1+4y)+2y=5\\] and then solve for \\(y\\) \\[\\begin{align*} 3(1+4y)+2y&amp;=5\\\\ 3+12y+2y&amp;=5\\\\ 14y&amp;=2\\\\ y&amp;=\\frac{1}{7}. \\end{align*}\\] Now that we have the value for \\(y\\) we can substitute this back into either equation to find \\(x\\). Since we already rearranged the second equation to make \\(x\\) the subject, we might as well use that one: \\[x=1+4y=1+\\frac{4}{7}=\\frac{11}{7}.\\] Hence the solution is \\[x=\\frac{11}{7},\\quad y=\\frac{1}{7}.\\] The above example shows a particular type of simultaneous equations called linear simultaneous equations (if we were to plot a graph of each of the equations we would see that they form straight lines, hence the term linear). We shall look at a systematic way to deal with linear simultaneous equations in section ??. Example 2.5 Given the following two simultaneous equations in the variables \\(x\\) and \\(y\\) \\[\\begin{align*} 2x+y&amp;=7\\\\ x^2-xy&amp;=6 \\end{align*}\\] it will be easiest to rearrange the first equation for \\(y\\), to get \\[y=7-2x\\] and then substituting this into the second equation \\[\\begin{align*} x^2-x(7-2x)&amp;=6\\\\ 3x^2-7x-6&amp;=0\\\\ (3x+2)(x-3)&amp;=0\\\\ \\end{align*}\\] so the factorisation tells us \\[x=-\\frac{2}{3},\\text{ or } x = 3.\\] Now substiting these two possibilities back into the first equation gives \\[\\begin{align*} 2(-\\frac{2}{3})+y&amp;=7\\\\ y&amp;=\\frac{25}{3} \\end{align*}\\] and \\[\\begin{align*} 2(3)+y&amp;=7\\\\ y&amp;=1. \\end{align*}\\] To summarise, the solutions are: \\[x = -\\frac{2}{3},\\quad y = \\frac{25}{3}\\] and \\[x=3,\\quad y=1.\\] The examples presented above are relatively simple, and in some real-world scenarios equations might be very complicated, or even impossible, to solve by hand. In these cases we need to turn to computational techniques. We’ll look at some computational techniques in section ??. 2.2 Inequalities We are familiar with the symbols \\(&lt;, \\leq, &gt;\\) and \\(\\geq\\), (less than, less than or equal to, greater than and greater than or equal to) which are used to define inequalities. Example 2.6 \\[x&lt;5\\] denotes all numbers that are strictly less that \\(5\\) (so not including \\(5\\) itself). \\[-1&lt; x \\leq 1\\] denotes all numbers that are strictly greater than \\(-1\\) but less that or equal to \\(1\\) (so \\(x=1\\) is a possibility). When solving inequalities (i.e. finding values of \\(x\\) which satisfy the expression) we need to be a bit more careful with their manipulation than with equalities. If we perform the same operation to both sides in an equality then the equality still holds true, but this is not the case in general for inequalities – in particular we need to be careful with multiplication by negative numbers which “flips” the inequality. Let \\(a,u,v,x,y,z\\) be real numbers, then some basic relations that hold true are: if \\(x&gt;y\\) and \\(y&gt;z\\), then \\(x&gt;z\\); if \\(x&gt;y\\), then \\(x+z&gt;y+z\\) and \\(x-z&gt;y-z\\); if \\(x&gt;y\\) and \\(u&gt;v\\), then \\(x+u&gt;y+v\\); if \\(x&gt;y\\), then \\(ax&gt;ay\\) if \\(a&gt;0\\), and \\(ax &lt; ay\\) if \\(a&lt;0\\); if \\(x&gt;y\\), then \\(\\dfrac{x}{a}&gt;\\dfrac{y}{a}\\) if \\(a&gt;0\\), and \\(\\dfrac{x}{a}&lt;\\dfrac{y}{a}\\) if \\(a&lt;0\\); if \\(x&gt;y&gt;0\\) and \\(u&gt;v&gt;0\\), then \\(xu&gt;yv\\) and \\(\\dfrac{x}{v}&gt;\\dfrac{y}{u}\\); if \\(x&gt;y&gt;0\\), then \\(\\dfrac{1}{x}&lt;\\dfrac{1}{y}\\). It should not be necessary to memorise all of these, it just requires a little thought when manipulating inequalities: the important ones to be careful with are multiplying or dividing by negative values. Assume we are comparing two expressions \\(P\\) and \\(Q\\) that depend on the variable \\(x\\) and need to find, for example, the values of \\(x\\) for which \\(P&gt;Q\\). Let us take a look at a range of examples to see how to solve these types of problems. Example 2.7 We wish to find all values of \\(x\\) satisfying the following inequalities. \\(2x+6&lt;18.\\) Here we can easily re-arrange the inequality (subtract \\(6\\) from each side and divide by the positive number \\(2\\) – which preserves the direction of the inequality) to obtain \\[x&lt;6.\\] \\(x^2-7x+12&gt;0\\) Factorise the l.h.s. to give \\((x-3)(x-4)&gt;0\\), so to be positive the factors on the left need to be either both positive or both negative, i.e.: \\((x-3)&gt;0\\) and \\((x-4)&gt;0\\), so we must have both \\(x&gt;4\\) and \\(x&gt;3\\), which means that we must take \\(x&gt;4\\). \\((x-3)&lt;0\\) and \\((x-4)&lt;0\\), so we require both \\(x&lt;3\\) and \\(x&lt;4\\), which means we must take \\(x&lt;3\\). Since either situation a or b satisfies the inequality the solution is \\(x&lt;3\\) or \\(x&gt;4\\). \\(\\left|10-3x\\right|\\leq 8\\), where \\(|\\cdot|\\) denotes the absolute value. We must have both: \\[\\begin{align*} 10-3x&amp;\\leq 8\\\\ -3x&amp;\\leq-2\\\\ x&amp;\\geq \\frac{2}{3} \\end{align*}\\] i.e. the interval \\(x\\geq\\dfrac{2}{3}.\\) And, \\(-(10-3x)\\leq 8\\), leading to \\(x\\leq 6\\). Hence \\(\\dfrac{2}{3}\\leq x \\leq 6\\). \\(\\dfrac{1}{x-3}&gt;\\dfrac{1}{x-2}\\) Let’s first do this the wrong way to illustrate a point. Let’s multiply both sides by \\((x-2)\\) to obtain \\[\\begin{gather*} \\frac{x-2}{x-3}&gt;1\\\\ \\frac{x-2}{x-3}-1&gt;0\\\\ \\frac{(x-2)-(x-3)}{x-3}=\\frac{1}{x-3}&gt;0 \\end{gather*}\\] giving the solution \\(x&gt;3\\). We can check the solution graphically by plotting the l.h.s. and r.h.s. and seeing where the l.h.s. is greater than the r.h.s. Figure 2.1: A plot of the inequality \\(\\frac{1}{x-3}&gt;\\frac{1}{x-2}\\): the solid red line is the l.h.s and the dashed blue line is the r.h.s.. We are interested in finding the values of \\(x\\) where the red line is above the blue line (the red-shaded region). [Open graph in browser.] It would appear that something has gone wrong! There is a second interval \\(x&lt;2\\) where the inequality holds true. The problem is that we multiplied by the factor \\((x-2)\\) forgetting the fact that \\(x\\) is a variable, so this factor could be positive or negative depending on the value of \\(x\\) and hence can flip the inequality. We could still perform this multiplication so long as we keep track of the two possible cases when \\((x-2)\\) is positive or negative. However, a safer strategy is to first subtract the r.h.s. from the l.h.s., then factorise as much as possible: \\[\\frac{1}{x-3}-\\frac{1}{x-2}=\\frac{(x-2)-(x-3)}{(x-3)(x-2)}=\\frac{1}{(x-3)(x-2)}&gt;0.\\] Hence for the denominator to be positive we must have either \\(x&gt;3\\) and \\(x&gt;2\\), or \\(x&lt;3\\) and \\(x&lt;2\\). Together these yield the solution \\(x&lt;2\\) or \\(x&gt;3\\). The general strategy for finding \\(P&gt;Q\\) (or similar) is therefore: (1) Consider \\(P-Q\\) and factorise as much as possible; (2) Determine the sign of each factor of \\(P-Q\\) for varying \\(x\\); (3) Determine when \\(P-Q&gt;0\\) (and hence when \\(P&gt;Q\\)). 2.3 Common mistakes! We finish this section with some common algebraic errors to watch out for! Invent your own examples to convince yourself that these things are true. Fractions: \\(\\frac{a}{b+c}\\neq \\frac{a}{b}+\\frac{a}{c}\\) and \\(\\frac{a+b}{c+b}\\neq\\frac{a}{c}\\) (can’t “cancel” the \\(b\\)’s.). Inequalities: \\(ab&gt;c\\) does not imply that \\(a&gt;\\frac{c}{b}\\) (what happens if \\(b\\) is negative?). Powers: \\((a+b)^2\\neq a^2+b^2\\). Roots: \\(\\sqrt{a+b}\\neq \\sqrt{a}+\\sqrt{b}\\). In fact, any quadratic can be factorised as a product of two linear factors, but we may need complex numbers to do this; more on this in section 5.↩︎ That is, no solutions for which \\(\\alpha\\) and \\(\\beta\\) are real numbers, but there will instead be complex number solutions.↩︎ "],["functions-and-graphs.html", "Chapter 3 Functions and Graphs 3.1 Lines 3.2 Polynomials 3.3 Rational functions 3.4 Root functions 3.5 Trigonometric functions 3.6 Exponentials and logarithms 3.7 Hyperbolic functions", " Chapter 3 Functions and Graphs A function \\(f\\) can be thought of as a machine that takes an input value \\(x\\) and returns an output value \\(y\\). The output value \\(y\\) corresponding to the input \\(x\\) is also denoted by \\(f(x)\\) (read “\\(f\\) of \\(x\\)”). We most commonly encounter functions whose input is a real number and output is a real number, but there are many other types of functions in mathematics. We are familiar with plotting such functions on a graph, where we draw the coordinates \\((x,f(x))\\), or \\((x,y)\\), against perpendicular axes – this is known as the Cartesian coordinate system. In this section we shall take a look at some useful functions, their graphs, and some of their other interesting properties. 3.1 Lines Consider the set of coordinates \\((x,y)\\) that satisfy the equation \\[ax+by+c=0\\] for some fixed values \\(a,b\\) and \\(c\\). When \\(b\\neq0\\), solving the equation gives \\(y=-\\dfrac{a}{b}x-\\dfrac{c}{b}\\), which is of the form \\[y=mx+d\\] i.e. it is a straight line with gradient \\(m\\) which intercepts the \\(y\\)-axis at the value \\(d\\). On the other hand, when \\(b=0\\) we have the solution \\(x=-\\dfrac{c}{a}\\). This is a straight vertical line; let’s set the constant \\(A=-\\dfrac{c}{a}\\), then it is all of the coordinates where \\(x=A\\), and since this equation does not depend on \\(y\\) it includes all possible \\(y\\) values. Note that we cannot write such a line in the form \\(y=mx+d\\), since it effectively has an “infinite” gradient. We can also have horizontal lines. This corresponds to an equation of the form \\(y=B\\) (this happens when \\(a=0\\) and where we have set \\(B=-\\dfrac{c}{b}\\)), so that \\(y\\) does not depend on \\(x\\) and equals the constant value \\(B\\) for all possible values of \\(x\\). Figure 3.1: A general line \\(ax+by+c=0\\) (red line) a line specified by a gradient and intercept \\(y=mx+d\\) (blue line), a vertical line \\(x=A\\) (green line) and a horizontal line \\(y=B\\) (orange line). Adjust the sliders to change the constants and check which values of \\(m,d,A,\\) and \\(B\\) correspond to \\(a,b\\) and \\(c\\). [Open graph in browser.] Specifying a single point \\((x_1,y_1)\\) and a gradient \\(m\\) is enough to define a (non-vertical) line and we can write the equation of this line as \\[ y-y_1=m(x-x_1). \\] Also, a line can be defined by specifying any two points \\((x_1,y_1)\\) and \\((x_2,y_2)\\) that lie on the line. We can calculate the gradient of this line from \\[ m=\\frac{y_2-y_1}{x_2-x_1} \\] but there is a problem if \\(x_2=x_1\\) and we have an “infinite” gradient – since \\(x_1=x_2\\) this, of course, corresponds to a vertical line, which we can write as the equation \\(x=x_1\\). The form \\(ax+by+c=0\\) is a general way to express any line in the plane and includes both vertical and non-vertical lines. The \\(y=mx+d\\) form is useful when thinking about non-vertical lines as we can easily identify the gradient and intercept. In section ?? we will be using the more general form for solving simultaneous linear equations. 3.1.1 Parallel lines A line makes an angle \\(\\theta\\) from the positive \\(x\\)-axis, the angle of inclination. This is related to the gradient \\(m\\) by \\[ m=\\frac{\\Delta y}{\\Delta x}=\\tan(\\theta). \\] Two lines are parallel if they have the same angle of inclination \\(\\theta\\), or same gradient \\(m\\). Figure 3.2: The angle of inclination of a line. Hence if two (non-vertical) lines are parallel they are of the form \\[\\begin{align*} y&amp;=mx + d_1\\\\ y&amp;=mx + d_2 \\end{align*}\\] 3.1.2 Perpendicular lines Two lines are perpendicular, or normal, if they meet at a right angle. We have \\[\\begin{align*} m_1&amp;=\\tan(\\theta)\\quad\\text{and,}\\\\ m_2&amp;=\\tan\\Big(\\theta + \\frac{\\pi}{2}\\Big)=\\frac{\\sin(\\theta + \\frac{\\pi}{2})}{\\cos(\\theta + \\frac{\\pi}{2})}\\\\ &amp;=\\frac{\\sin(\\theta)\\cos(\\frac{\\pi}{2}) + \\cos(\\theta)\\sin(\\frac{\\pi}{2})}{\\cos(\\theta)\\cos(\\frac{\\pi}{2}) - \\sin(\\theta)\\sin(\\frac{\\pi}{2})}\\\\ &amp;=\\frac{\\cos(\\theta)}{-\\sin(\\theta)}\\\\ &amp;=-\\cot(\\theta) \\end{align*}\\] Figure 3.3: Perpendicular lines. Thus, two (non-vertical) lines are perpendicular if \\[\\begin{equation*} m_1m_2=-1 \\end{equation*}\\] and hence they have the forms \\[\\begin{align*} y&amp;=mx + d_1,\\\\ y&amp;=-\\frac{1}{m}x + d_2. \\end{align*}\\] Alternatively, if we describe a line by \\(ax+by+c=0\\) then \\[\\begin{align*} ax+by+p&amp;=0\\quad\\text{is a parallel line}\\\\ bx+ay+q&amp;=0\\quad\\text{is a perpendicular line} \\end{align*}\\] where \\(p\\) and \\(q\\) are constants. [Exercise: Verify this last statement.] 3.2 Polynomials A polynomial is a function made up of non-negative integer powers of the independent variable \\(x\\) multiplied by constant coefficients, i.e. it has the form \\[f(x)=a_0 + a_1 x + a_2 x^2 + \\dotsb + a_n x^n\\] where the fixed numbers \\(a_0,\\dots,a_n\\) are the coefficients (which could be positive, negative or zero). For example, the following are all polynomials: \\[\\begin{align*}f(x)&amp;=x^2\\\\ g(x)&amp;=2+\\frac{1}{3}x^4 - x^5\\\\ h(x)&amp;=-3.6 + x^2 -2x^10 \\end{align*}\\] The number \\(n\\) corresponding to the highest power in the polynomial is called the order or degree of the polynomial. For the functions \\(f, g\\) and \\(h\\) above the degrees are \\(2, 5\\) and \\(10\\). We have a few special names for polynomials of low degree: Degree 1: a linear polynomial, since its graph is a line Degree 2: a quadratic polynomial Degree 3: a cubic polynomial Degree 4: a quartic polynomial Degree 5: a quintic polynomial The higher the degree of the polynomial, the more “wavey” its graph can be; in fact, a degree \\(n\\) polynomial can have at most \\(n-1\\) turning points. A polynomial which has only one term is called a monomial, for example \\[f(x)=2\\\\ g(x)=3x^2\\\\ h(x)=-4x^5 \\] are all monomials. Figure 3.4: A plot of a polynomial of at most degree 5, \\(f(x)=a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5\\). Adjust the sliders to change the coefficients \\(a_0\\) to \\(a_5\\) to get a feel for the possible shapes of a degree 5 polynomial; set the higher degree coefficients to \\(0\\) to look at lower degree polynomials; also take a look at the monomials up to degree 5. [Open graph in browser.] We are often interested in finding the values of \\(x\\) for which \\(f(x)=0\\), known as the roots of a polynomial. We already know how to find roots of a quadratic using the quadratic formula (section 2.1.3). We will now look at the more general factor theorem that can help in finding roots of higher degree polynomials. If we factorise the quadratic equation \\[x^2-3x-10=0\\] we obtain \\[(x+2)(x-5)=0.\\] If the product of two numbers is zero, then either one or both of the numbers must be zero. So the possible roots are \\[(x+2)=0 \\quad \\implies\\quad x=-2\\] or \\[(x-5)=0\\quad \\implies\\quad x=5.\\] More generally, a factor \\((x-a)\\) corresponds to a root \\(x=a\\). Knowing the factors tells us the roots, and vice versa. It turns out this holds more generally for any degree polynomial. Theorem 3.1 (Factor Theorem) The value \\(x=a\\) is a root of the polynomial equation \\(f(x)=0\\) if and only if \\((x-a)\\) is a factor of the polynomial \\(f(x)\\). We already have a fool-proof way to solve quadratic equations – use the quadratic formula. There are also formulae for cubic and quartic polynomials, but these are a bit more complicated. Furthermore, it turns out it is impossible to derive a formula for quintic and higher degree polynomials! The factor theorem can be helpful in finding roots of polynomials greater than degree 2. Here is an example. Example 3.1 (Factor theorem applied to a cubic polynomial) Find the roots of the cubic polynomial \\[f(x)=x^3-7x-6.\\] After a little trial and error with small integer values of \\(x\\) we find \\[f(3)=3^3-7(3)-6=0\\] which by the factor theorem implies that \\((x-3)\\) is a factor. Now applying polynomial division by the factor \\((x-3)\\) we find \\[\\frac{x^3-7x-6}{x-3}=x^2+3x+2\\] so that \\[x^3-7x-6=(x-3)(x^2+3x+2).\\] Now that we have taken out one linear factor we are left with a quadratic factor, which we know how to deal with. Factorising the quadratic factor yields \\[x^3-7x-6=(x-3)(x+1)(x+2).\\] Hence the roots are \\[x=3, x=-1,\\,\\text{ and } x=-2.\\] 3.3 Rational functions A rational function is one that can be expressed as the quotient of two polynomials, for example \\[f(x)=\\frac{1+x}{2-x+3x^2}\\] is the quotient of a degree 1 polynomial and a degree 2 polynomial. They often have values of \\(x\\) where they are undefined due to the denominator evaluating to zero. Furthermore, they often have asymptotes, which means the graph approaches a straight line. For example, the function \\(f(x)=\\frac{x^{2}}{x^{2}-4}\\) has asmptotes at the vertical lines \\(x=2\\) and \\(x=-2\\) and at the horizontal line \\(y=1\\). Figure 3.5: A plot of the rational function \\(f(x)=\\frac{x^{2}}{x^{2}-4}\\) (solid red line) and its asymptotes (dashed blue lines). [Open graph in browser.] 3.4 Root functions Taking the \\(n^\\text{th}\\) root of a number \\(\\sqrt[n]{x}\\) returns the answer to the question “what number multiplied by itself \\(n\\) times equals \\(x\\)?”. We saw in section 1.2.1 that this is the same as taking the power \\(x^\\frac{1}{n}\\). Taking the \\(n^\\text{th}\\) root can also be thought of as reversing the operation of taking the \\(n^\\text{th}\\) power (and vice versa). In this sense, the functions \\(f(x)=x^n\\) and \\(g(x)=\\sqrt[n]{x}\\) are inverse functions – they each “undo” the action of the other to return to the original value \\(x\\). If we first apply \\(f\\) and then \\(g\\) (known as the composition of \\(f\\) and \\(g\\)), or the other way around, then the result is to get back to \\(x\\): \\[g(f(x)) = \\sqrt[n]{x^n}=x\\] and \\[f(g(x)) = (\\sqrt[n]{x})^n=x.\\] Note that when \\(n\\) is even the root function only makes sense for positive values of \\(x\\), whilst if \\(n\\) is an odd number then it also makes sense for negative values of \\(x\\) (Why? Think about this!). Graphically, the inverse of a function is the reflection in the line \\(y=x\\) (Why?). Figure 3.6: The functions \\(f(x)=x^2\\) (red line) \\(g(x)=\\sqrt{x}\\) (blue) are reflections of each other in the line \\(y=x\\) (dashed black). [Open graph in browser.] 3.5 Trigonometric functions The three basic trigonometric functions are sine (\\(\\sin\\)), cosine (\\(\\cos\\)) and tangent (\\(\\tan\\)), which we are familiar with from trigonometry for calculations involving angles. We shall explore trigonometry in more detail in section 4. For now, we will just look at some of their properties as functions. Sine and cosine look similar, like “waves”, with one graph shifted along the \\(x\\) axis from the other. They oscillate between their maximum value of \\(1\\) and minimum value of \\(-1\\). Figure 3.7: The functions \\(\\sin(x)\\) (red curve) and \\(\\cos(x)\\) (blue dashed line). [Open graph in browser.] In figure 3.7 we have plotted the functions with the \\(x\\) axis in units of radians. This is the natural mathematical choice of units, but in applications we tend to use degrees. We’ll understand more about radians and see how these units are related to degrees in section 4. Note the graphs repeat every \\(2\\pi\\) radians – they are periodic functions with period \\(2\\pi\\). The tangent function is defined via sine and cosine as: \\[\\begin{equation} \\tan(x)=\\frac{\\sin(x)}{\\cos(x)} \\tag{3.1} \\end{equation}\\] It’s graph looks rather different to sine and cosine. Figure 3.8: The function \\(\\tan(x)\\) (green curve). [Open graph in browser.] Like sine and cosine it is periodic, with period \\(\\pi\\). It is undefined at \\(x=\\frac{\\pi}{2}\\) (and repeatedly every \\(2\\pi\\)) since at this value \\(\\cos(\\frac{\\pi}{2})=0\\) so we have division by zero. Close to these points, where we are dividing by a very small value of \\(\\cos\\), it takes arbitrarily large positive and negative values and we have a vertical asymptote. Close to these points, the value of \\(\\cos(x)\\) in the denominator of (3.1) is very small, whilst the value of \\(\\sin(x)\\) is close to \\(1\\) or \\(-1\\), which results in very large positive or negative values of \\(\\tan(x)\\) and we have vertical asymptotes at these points. 3.6 Exponentials and logarithms An exponential function is one of the form \\[ f(x)=a^x \\] where the positive number \\(a\\) is called the . Figure 3.9: The function \\(f(x)=a^x\\) for varying positive values of \\(a\\). [Open graph in browser.] The term “exponential growth” corresponds to such functions; this is where the instantaneous rate of change of a quantity is proportional to its current magnitude and this is the characteristic property of exponential functions. More mathematically, the instantaneous rate of change is given by the derivative5, so we have \\[\\begin{equation} \\frac{d(a^x)}{dx}=ka^x \\tag{3.2} \\end{equation}\\] where \\(k\\) is the constant of proportionality6 (which we shall find in section ??). Exponential growth is a common occurance in physical systems, at least in the early stages of a process. For example, it can describe the early stages of population growth well, when there are no limits on resources and a constant birth rate; in reality, limits (on food, space, impact of disease etc.) mean that uncurbed exponential growth cannot take place forever. Any exponential function with any base \\(a&gt;1\\) will grow faster than any polynomial; although a polynomial can be larger for some small range of \\(x\\) values, eventually the exponential graph will “overtake” the polynomial. Figure 3.10: The functions \\(f(x)=a^x\\) for varying \\(a&gt;1\\) (red curve) and the monomial \\(g(x)=x^n\\) for varying power \\(n\\) (blue dashed curve); for any value of \\(a\\) and any value of \\(n\\), if you scroll up far enough you will see that the red curve overtakes the dashed blue curve. [Open graph in browser.] There is a special value of the base \\(a\\) for which the constant of proportionality is \\(k=1\\). This is known as Euler’s number and is denoted as \\(e\\). It is an irrational number (has an infinite, never repeating decimal expansion): \\[e=2.71828182846...\\] The function \\(e^x\\) is referred to as the natural exponential function or just the exponential function. It is also commonly denoted by \\(\\exp(x)\\). Since by definition \\(k=1\\) we have: \\[\\frac{de^x}{dx}=1e^x=e^x.\\] The inverse of an exponential function \\(f(x)=a^x\\) is called the logarithm to the base \\(a\\), denoted by \\(\\log_a(x)\\). Since it is the inverse, the exponential and logarithm “undo” the action of each other, so that \\[\\log_a(a^x)=x\\] and \\[a^{\\log_a(x)}=x\\]. For the base \\(a=e\\), we call the logarithm \\(\\log_e(x)\\) the natural logarithm and use the notation \\[\\ln(x)=\\log_e(x)\\] \\(\\ln(x)\\) is usually spoken as lun x or L.N. of x. Figure 3.11: The functions \\(f(x)=a^x\\) for varying \\(a&gt;1\\) (red curve) and the inverse \\(g(x)=\\log_a(x)\\) (blue curve) together with the line \\(y=x\\) to show the symmetry of the function and its inverse. [Open graph in browser.] In engineering applications it is also common to use base \\(2\\) and base \\(10\\). For base \\(10\\), the following notation is sometimes used \\[\\log(x)=\\log_{10}(x)\\] but we should always check what the author intends as sometimes \\(\\log(x)\\) is used to mean \\(\\ln(x)\\). We have the following rules for manipulating logarithms (these follow from the laws of exponents; try to derive them yourself): Theorem 3.2 (Rules of Logarithms) \\(\\log_a(xy)=\\log_a(x)+\\log_a(y)\\) \\(\\log_a(x^p)=p\\log_a(x)\\) \\(\\log_a(\\frac{x}{y})=\\log_a(x)-\\log_a(y)\\) \\(\\log_a(1)=0\\) \\(\\log_a(a)=1\\) We can also change between bases in logarithms with the following rule Theorem 3.3 (Change of Base Rule) Let \\(y=a^x\\), then \\(x=\\log_a(y)\\). Also, \\(\\log_b(y)=x\\log_b(a)\\), and so \\(x=\\dfrac{\\log_b(y)}{\\log_b(a)}\\). Therefore, \\[\\begin{equation*} \\log_a(y)=\\dfrac{\\log_b (y)}{\\log_b (a)}. \\end{equation*}\\] This allows us to calculate \\(\\log_a\\) of a number in terms of \\(\\log_b\\). An exponential function with base \\(a\\) can be re-written in any other base \\(b\\) with a constant factor in the argument as follows: \\[\\begin{align*} a^x&amp;=b^{\\log_b (a^x)} \\quad \\text{(note that $b^{\\log_b y}=y$)} \\\\ &amp;=b^{x\\log_b (a)}\\\\ &amp;=b^{kx},\\quad \\text{where }k=\\log_b(a). \\end{align*}\\] The “natural” choice of base for mathematical use is \\(b=e\\) and hence we usually only consider exponential functions in the form \\(e^{kx}\\) where \\(k=\\ln(a)\\). 3.6.1 Logarthmic plots When we have quatities that change over a large range of magnitudes, it can be more convenient to plot them on a “logarthmic scale” so that they do not take up so much space on the page. For example, the Ricther Magnitude Scale for measuring earthquakes is a logarthmic scale, which takes the logarithm to the base 10 of the amplitude of waves recorded by seismographs. The following is a “semi-log” plot, where the \\(y\\)-axis is the logarithmic scale and the \\(x\\)-axis is a (normal) linear scale. This means that every unit moved up along the \\(y\\)-axis actually represents a power of 10 increase in the amplitude of the seismograph waves. Figure 3.12: Magnitude of the August 2016 Central Italy earthquake (red dot) and aftershocks (which continued to occur after the period shown here). From Wikipedia - Phoenix7777. The logarithmic nature of the scale is sometimes made clearer by labelling the axis with an exponentially increasing scale and drawing in the minor gridlines. In figure 3.13 the major grid lines on the \\(y\\)-axis increase by a power of \\(10\\). The minor grid lines between \\(1\\) and \\(10\\) represent increments of \\(1\\) unit, then the minor grid lines between \\(10\\) and \\(10^2=100\\) represent increments of \\(10\\) units, and so on. Figure 3.13: Semi log graph paper (base 10). From wikimedia. 3.7 Hyperbolic functions The hyperbolic functions are defined in terms of the exponential function. They are named after the trigonometric functions due to possessing some similar properties, although their graphs are quite different. Hyperbolic sine, usually spoken as “shine” is defined as \\[\\sinh(x)=\\frac{e^x-e^{-x}}{2}\\] Hyperbolic cosine or “cosh” is defined as \\[\\cosh(x)=\\frac{e^x+e^{-x}}{2}\\] Hyperbolic tangent or “tanch” is defined as \\[{\\tanh(x)=\\dfrac{\\sinh(x)}{\\cosh(x)}=\\dfrac{e^x-e^{-x}}{e^x+e^{-x}}}\\] Figure 3.14: The functions \\(\\sinh(x)\\) (red curve), \\(\\cosh(x)\\) (blue curve) and \\(\\tanh(x)\\) (green curve). [Open graph in browser.] The usual trigonometric identities also hold for the corresponding hyperbolic functions, except that where ever there is a \\(\\sin^2\\) we replace it with a \\(-\\sinh^2\\). For example: \\[\\begin{align*} &amp;\\text{Trigonometric} &amp;\\text{Hyperbolic}\\\\ &amp;\\sin^2(x)+\\cos^2(x)=1 &amp;\\cosh^2(x)-\\sinh^2(x)=1 \\\\ &amp;\\sin(2x)=2\\sin(x)\\cos(x) &amp;\\sinh(2x)=2\\sinh(x)\\cosh(x) \\\\ &amp;\\cos(2x)=\\cos^2(x)-\\sin^2(x) &amp;\\cosh(2x)=\\cosh^2(x)+\\sinh^2(x) \\\\ &amp;1+\\tan^2(x)=\\sec^2(x) &amp;1-\\tanh^2(x)=\\text{sech}^2(x) \\end{align*}\\] As an example of where these functions arise in the real-world, the shape formed by a flexible cable or chain hanging from its ends under its own weight has the general form \\[\\begin{equation*} y=a\\cosh\\left(\\frac{x}{a}\\right) \\end{equation*}\\] and is known as a catenary curve. Figure 3.15: A chain hanging from points forms a catenary. From Wikipedia - Kamel15 See section ??.↩︎ The phrase “exponential growth” is sometimes used in a casual way to talk about anything that increases “quickly”, but mathematicians, scientists and engineers are more precise and would only use the term in its strict mathematical sense, as relating to an exponential function!↩︎ "],["trigonometry.html", "Chapter 4 Trigonometry 4.1 Pythagoras 4.2 Degrees and radians 4.3 Trigonometric ratios 4.4 Sine and cosine rules 4.5 Trigonometric waveforms 4.6 Trigonometric identities", " Chapter 4 Trigonometry 4.1 Pythagoras The most fundamental result in trigonometry is the Pythagorean Theorem: Theorem 4.1 (Pythagorean Theorem) In a right-angle triangle, the hypotenuse is the side opposite the right angle. In any right-angle triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides. With reference to figure 4.1, we have \\[c^2=a^2+b^2\\] Figure 4.1: A right-angled triangle with sides \\(a, b\\) and \\(c\\), with \\(c\\) being the hypotenuse. One particular application is finding the distance between two points in a Cartesian coordinate system. 4.2 Degrees and radians We can measure an angle in two different units: degrees or radians. In degrees, we split a full circle into 360 segments and then call the angle bewteen each segment 1 degree, or \\(1^\\circ\\). Of course a full circle is \\(360^\\circ\\). An alternative measure of angle is radians. In this case, we start from the fact that the circumference of a circle is given by \\[ C=2\\pi r \\] where \\(r\\) is the radius of the circle. If we had a circle with radius \\(r=1\\), then the circumference is \\(C=2\\pi\\). Now we consider this circle to be split into \\(2\\pi\\) segments7, with each segment being 1 radian, or \\(1 \\text{ rad}\\). A full circle is then \\(2\\pi \\text{ rad} = 6.283 \\text{ rad}\\) (to 3 sig. fig.). Sometimes it is more convenient to work in degrees, and other times it is more convenient to work in radians. We can easily convert between degrees and radians as follows. If we have an angle \\(D^\\circ\\) in degrees, then the corresponding angle in radians \\(R \\text{ rad}\\) is: \\[ R \\text{ rad}=\\frac{2\\pi}{360}\\times D^\\circ \\] and in the other direction \\[ D^\\circ=\\frac{360}{2\\pi}\\times R \\text{ rad}. \\] Where possible, we would state radians as multiples of \\(\\pi\\), so in particular: Degrees Radians \\(360^\\circ\\) \\(2\\pi\\text{ rad}\\) \\(270^\\circ\\) \\(\\frac{3}{2}\\pi\\text{ rad}\\) \\(180^\\circ\\) \\(\\pi\\text{ rad}\\) \\(120^\\circ\\) \\(\\frac{2}{3}\\pi\\text{ rad}\\) \\(90^\\circ\\) \\(\\frac{\\pi}{2}\\text{ rad}\\) \\(60^\\circ\\) \\(\\frac{\\pi}{3}\\text{ rad}\\) \\(45^\\circ\\) \\(\\frac{\\pi}{4}\\text{ rad}\\) \\(30^\\circ\\) \\(\\frac{\\pi}{6}\\text{ rad}\\) Note that a scientific calculator will have a button to switch between using degrees and radians: make sure your calculator is in the correct mode! 4.3 Trigonometric ratios Calculating angles and side lengths of right-angled triangles is of fundamental importance in science and engineering applications. Figure 4.2: A right-angled triangle. Recall that we can calculate relationships between angles and side lengths of a right-angled triangle (as described in figure 4.2) using sine, cosine and tangent: \\[\\begin{align*} \\sin(\\theta)&amp;=\\frac{Opp.}{Hyp.}\\\\ \\cos(\\theta)&amp;=\\frac{Adj.}{Hyp.}\\\\ \\tan(\\theta)&amp;=\\frac{Opp.}{Adj.} \\end{align*}\\] A useful way to remember these relationships is the mnemonic “SOH CAH TOA”: SOH - Sine equals Opposite over Hypotenuse CAH - Cosine equals Adjacent over Hypotenuse TOA - Tan equals Opposite over Adjacent These are most commonly used to determine angles by using the inverse functions: \\[\\begin{align*} \\theta&amp;=\\sin^{-1}(\\frac{Opp.}{Hyp.})\\\\ \\theta&amp;=\\cos^{-1}(\\frac{Adj.}{Hyp.})\\\\ \\theta&amp;=\\tan^{-1}(\\frac{Opp.}{Adj.}) \\end{align*}\\] WARNING: the notation suggests we are taking \\(\\sin\\) to the power \\(-1\\), so that \\(\\sin^{-1}=\\frac{1}{\\sin}\\), but this is NOT how the inverse is defined. Remember that the inverse “undoes” the action of the function. Scientific calculators will have buttons to apply these functions. The inverses are also sometimes called \\[\\begin{align*} \\arcsin&amp;=\\sin^{-1},\\\\ \\arccos&amp;=\\cos^{-1},\\\\ \\arctan&amp;=\\tan^{-1}. \\end{align*}\\] Example 4.1 (Applying inverse trig. functions) Calculate the angle \\(\\theta\\) in the following triangle. Figure 4.3: A right angled triangle with hypotenuse legth 14 units and adjacent length 8 units. Since we have the lengths of the hypotenuse and adjacent, we can use \\(\\cos^{-1}\\) to find \\(\\theta\\). Using a calculator we find \\[\\theta=\\cos^{-1}\\left(\\frac{8}{14}\\right)=55.15^\\circ \\text{(to 2 d.p.)}\\] We also have special names for the reciprocal trigonometric functions: \\[\\begin{align*} \\operatorname{cosec}(\\theta)&amp;=\\frac{1}{\\sin(\\theta)},\\\\ \\sec(\\theta)&amp;=\\frac{1}{\\cos(\\theta)},\\\\ \\cot(\\theta)&amp;=\\frac{1}{\\tan(\\theta)}. \\end{align*}\\] 4.4 Sine and cosine rules The Pythagorean Theorem and Trigonometric ratios above only apply to right-angle triangles. For general triangles, the sine and cosine rules give us useful relationsips between angles and side lengths. Figure 4.4: A general triangle with angles \\(A, B,\\) and \\(C\\) and the corresponding opposite sides as \\(a, b\\) and \\(c\\). Theorem 4.2 (Sine Rule) Label a triangle with angles \\(A, B,\\) and \\(C\\) and the corresponding opposite sides as \\(a, b\\) and \\(c\\) (as in figure 4.4). Then, \\[\\frac{a}{\\sin(A)}=\\frac{b}{\\sin(B)}=\\frac{c}{\\sin(C)}.\\] This is useful when we have: one side and any two angles, or two sides and an angle (but not the included angle). Theorem 4.3 (Cosine Rule) Label a triangle with angles \\(A, B,\\) and \\(C\\) and the corresponding opposite sides as \\(a, b\\) and \\(c\\) (as in figure 4.4). Then, \\[\\begin{align*} a^2&amp;= b^2 +c^2 -2bc\\cos(A)\\quad\\text{ or,}\\\\ b^2&amp;=a^2+c^2-2ac\\cos(B)\\quad\\text{ or,}\\\\ c^2&amp;=a^2+b^2-2ab\\cos(C). \\end{align*}\\] This is useful when we have: two sides and the included angle, or three sides. 4.5 Trigonometric waveforms The trigonometric functions often arise in applications in the context of waves. Whilst there are different forms of waves, trigonometric waves are arguably the most ubiquitous. In general, a function of the form \\[ f(x)=A\\sin(\\omega x + \\phi) \\] with parameters \\(A,\\omega\\) and \\(\\phi\\) is known as a sinusoidal function. Figure 4.5: A sinusoidal wave form plotted in radians. \\(A\\) is called the amplitude – this determines the range of values, from \\(-A\\) to \\(A\\); \\(\\omega\\) is the angular frequency – the rate of change of the sine function argument in units of radians per second or degrees per second; \\(\\phi\\) is the phase – this determines the offset of the sine function at \\(x=0\\) in units of radians or degrees. \\(T=\\frac{2\\pi}{\\omega}\\) is the length of one cycle e.g. measured from peak to peak, or trough to trough. If \\(x\\) represents time, then \\(T\\) is called the period. If \\(x\\) represents distance, then \\(T\\) is usually denoted by \\(\\lambda\\) and called the wavelength. Figure 4.6: The function \\(f(x)=A\\sin(\\omega x + \\phi)\\) (blue curve). [Open graph in browser.] A useful formula for combining \\(\\sin\\) and \\(\\cos\\) waves is: \\[a\\cos(\\theta)+b\\sin(\\theta)=R\\cos(\\theta-\\phi)\\] where: \\(R=\\sqrt{a^2+b^2}\\) \\(\\phi=\\tan^{-1}(b/a)\\) 4.6 Trigonometric identities Recall that there are a number of useful relationships between the trigonometric functions. Here are some key ones. 4.6.1 Pythagorean identities These follow from the Pythagorean theorem. \\[\\begin{gather*} \\sin^2(\\theta)+\\cos^2(\\theta)=1\\\\ 1+\\tan^2(\\theta)=\\sec^2(\\theta)\\\\ \\cot^2(\\theta)+1=\\operatorname{cosec}^2(\\theta) \\end{gather*}\\] 4.6.2 Compound angle formulae The following formulae allow us to manipulate sines, cosines and tangents of the sum or difference of two angles. \\[\\begin{align*} \\sin(\\theta+\\phi)&amp;=\\sin(\\theta)\\cos(\\phi)+\\cos(\\theta)\\sin(\\phi)\\\\ \\sin(\\theta-\\phi)&amp;=\\sin(\\theta)\\cos(\\phi)-\\cos(\\theta)\\sin(\\phi)\\\\ \\cos(\\theta+\\phi)&amp;=\\cos(\\theta)\\cos(\\phi)-\\sin(\\theta)\\sin(\\phi)\\\\ \\cos(\\theta-\\phi)&amp;=\\cos(\\theta)\\cos(\\phi)+\\sin(\\theta)\\sin(\\phi)\\\\ \\tan(\\theta+\\phi)&amp;=\\frac{\\tan(\\theta)+\\tan(\\phi)}{1-\\tan(\\theta)\\tan(\\phi)}\\\\ \\tan(\\theta-\\phi)&amp;=\\frac{\\tan(\\theta)-\\tan(\\phi)}{1+\\tan(\\theta)\\tan(\\phi)} \\end{align*}\\] 4.6.3 Double angle formulae Setting \\(\\phi=\\theta\\) in the compound angle formulae gives the following double angle formulae: \\[\\begin{align*} \\sin(2\\theta)&amp;=2\\sin(\\theta)\\cos(\\theta)\\\\ \\cos(2\\theta)&amp;=\\cos^2(\\theta)-\\sin^2(\\theta)\\\\ \\tan(2\\theta)&amp;=\\frac{2\\tan(\\theta)}{1-\\tan^2(\\theta)} \\end{align*}\\] 4.6.4 Product to sum formulae From the compound angle formulae we can derive the following. \\[\\begin{align*} \\sin(\\theta)\\cos(\\phi)&amp;=\\frac{1}{2}[\\sin(\\theta+\\phi)+\\sin(\\theta-\\phi)]\\\\ \\cos(\\theta)\\sin(\\phi)&amp;=\\frac{1}{2}[\\sin(\\theta+\\phi)-\\sin(\\theta-\\phi)]\\\\ \\cos(\\theta)\\cos(\\phi)&amp;=\\frac{1}{2}[\\cos(\\theta+\\phi)+\\cos(\\theta-\\phi)]\\\\ \\sin(\\theta)\\sin(\\phi)&amp;=-\\frac{1}{2}[\\cos(\\theta+\\phi)-\\cos(\\theta-\\phi)]\\\\ \\end{align*}\\] 4.6.5 Sum to product formulae In the other direction (also derived from the compound angle formulae) we have the following. \\[\\begin{align*} \\sin(\\theta)+\\sin(\\phi)=2\\sin\\left(\\frac{\\theta+\\phi}{2}\\right)\\cos\\left(\\frac{\\theta-\\phi}{2}\\right)\\\\ \\sin(\\theta)-\\sin(\\phi)=2\\cos\\left(\\frac{\\theta+\\phi}{2}\\right)\\sin\\left(\\frac{\\theta-\\phi}{2}\\right)\\\\ \\cos(\\theta)+\\cos(\\phi)=2\\cos\\left(\\frac{\\theta+\\phi}{2}\\right)\\cos\\left(\\frac{\\theta-\\phi}{2}\\right)\\\\ \\cos(\\theta)-\\cos(\\phi)=-2\\sin\\left(\\frac{\\theta+\\phi}{2}\\right)\\sin\\left(\\frac{\\theta-\\phi}{2}\\right) \\end{align*}\\] Note this is not an integer number of segments! It is approximately 6.283 segments.↩︎ "],["complex.html", "Chapter 5 Complex Numbers 5.1 \\(i\\) and complex numbers 5.2 Complex arithmetic 5.3 The argument, polar and exponential form for complex numbers 5.4 Roots of complex numbers 5.5 \\(e^{i\\theta}\\) and trigonometric identities", " Chapter 5 Complex Numbers We are familiar with integers (\\(\\dots, -5, -4, -3, -2, -1, 0, 1 \\dots\\)), denoted by \\(\\mathbb{Z}\\), and rational numbers (\\(1/2, 4/5\\), etc.), denoted by \\(\\mathbb{Q}\\). We also know about irrational numbers such as \\(e\\), \\(\\pi\\) (the area of a circle with radius equal to \\(1\\)) or \\(\\sqrt{2}\\) — one solution of the quadratic equation \\[ x^2 - 2 = 0. \\] The collection of natural numbers, integers, rational and irrational numbers make up what we call the real numbers, denoted by \\(\\mathbb{R}\\). But do we have “enough” numbers? The answer is no! We do not have to look hard for why we do not have enough numbers. The simple equation \\[ x^2 + 1 = 0 \\] does not have real solutions. The answer would be \\(\\pm \\sqrt{-1}\\), if that would make sense. More generally, the quadratic equation \\[ a x^2 + b x + c = 0, \\quad \\text{with}\\quad b^2 - 4 ac &lt; 0 \\] does not have real number solutions! 5.1 \\(i\\) and complex numbers Our fix is to define the imaginary number \\(i\\) by8 \\[ i = \\sqrt{-1},\\quad \\text{ so that }\\quad i^2 = -1. \\] With this definition in mind, consider the quadratic equation \\[ x^2 + 4 x + 13 = 0. \\] Completing the square gives \\[ (x+2)^2 + 9 = 0 \\quad \\implies \\quad x+2 = \\pm \\sqrt{-9} = \\pm 3 \\sqrt{-1} = \\pm 3 i \\quad \\implies \\quad x = -2 \\pm 3 i. \\] This leads us to Complex Numbers, denoted by \\(\\mathbb{C}\\), defined as the set of all numbers \\[ z = x + i y, \\quad \\text{where $x$ and $y$ are real numbers}. \\] This form is also called the Cartesian representation of a complex number. It has a real part \\(\\mathbf{Re}(z) = x\\) and an imaginary part \\(\\mathbf{Im}(z) = y\\). We can then represent a complex number graphically by plotting the real and imaginary parts as coordinates in a Cartesian coordinate system, as in figure 5.1, known as the complex plane. The \\(x\\)-axis is called the real axis and the \\(y\\)-axis is called the imaginary axis. Figure 5.1: The complex number \\(z = x + iy\\) in Cartesian coordinates. Note that any real number \\(x\\) can also be considered as a complex number, since it is a complex number that happens to have \\(0\\) imaginary part: \\(x = x +0i\\); it is a complex number that lies on the real axis. We know from experience that a quadratic equation can have at most 2 roots. Once we have accepted complex numbers as part of our everyday lives, we see that they complete the picture of roots of polynomials. Theorem 5.1 (Fundamental Theorem of Algebra) Every non-zero, complex, single variable, degree \\(n\\) polynomial, has exactly \\(n\\) roots. To clarify the above theorem, we are talking about a polynomial of the form \\[f(x)=a_0 + a_1 x + a_2 x^2 + \\dotsb + a_n x^n\\] where the variable \\(x\\) can be a complex number (so can also be a real number) and the coefficients \\(a_i\\) are complex numbers (so can also be real numbers) and at least one of these coefficients is not \\(0\\). Since we allow the single variable \\(x\\) to be a complex number, the roots may be complex numbers. Note that some of the \\(n\\) roots might be repeated roots, for example, according to the theorem the quadratic \\[x^2-4x+4=(x-2)(x-2)\\] must have two roots and in this case they happen to be repeated roots; we say the root is \\(x=2\\) with multiplicity 2. 5.2 Complex arithmetic We can add, subtract and multiply complex numbers \\(z = x+iy\\) and \\(w = u+iv\\), using usual algebraic rules for expanding brackets — just remembering that \\(i^2 = -1\\): \\[\\begin{align*} z + w &amp;= (x + iy) + (u + iv) = (x+u) + i (y+v),\\\\ z - w &amp;= (x + iy) - (u + iv) = (x-u) + i (y-v),\\\\ z \\times w &amp;= (x + iy) \\times (u + iv) = x u + i x v + i yu + i^2 y v = (xu - yv) + i (xv + yu). \\end{align*}\\] Example 5.1 (Complex Arithmetic) Let \\(z = 3 - 5i\\) (that is, \\(z = 3 + (-5)i\\)) and \\(w = 2 + i\\) then \\[ z + w = 5 - 4i, \\qquad z - w = 1 - 6i, \\] and \\[ z\\times w = (3 - 5i)\\times (2+i) = 6 + 3i - 10i - 5i^2 = 11 - 7i. \\] Thus \\(\\mathbf{Re}(z\\times w) = 11\\) and \\(\\mathbf{Im}(z\\times w) = -7\\). The complex conjugate of \\(z = x + iy\\) is the complex number \\[ \\overline{z} = x - iy. \\] This is the “mirror image” of \\(z\\) in the real axis. The modulus of \\(z = x + iy\\) is \\[ |z| = \\sqrt{x^2 + y^2} \\qquad \\text{(the non-negative square root).} \\] Geometrically, this is the distance of \\(z\\) (i.e. the point \\(P\\)) from the origin – See figure 5.1. Note that if \\(b = 0\\) (so that \\(z\\) is actually a real number), then this definition of \\(|z|\\) agrees with the usual modulus (absolute value) for real numbers. Example 5.2 (Complex conjugate) For \\(z = 2 - i\\), we have \\(\\overline{z} = 2 + i\\) and \\(|z| = \\sqrt{2^2 + (-1)^2} = \\sqrt{5}\\). Some properties of modulus and conjugate: For any complex numbers \\(z, w\\), \\(\\overline{\\overline{z}} = z\\); \\(\\overline{z \\pm w} = \\overline{z} + \\overline{w}\\); \\(\\overline{z\\times w} = \\overline{z}\\times \\overline{w}\\); \\(|z|\\) is a positive real number unless \\(z = 0\\) with \\(|0| = 0\\); \\(z\\times \\overline{z} = |z|^2\\); \\(|z\\times w| = |z|\\,|w|\\). What about division of complex numbers? This is made easy using the complex conjugate as follows: \\[\\frac{z}{w}=\\frac{z}{w}\\frac{\\overline{w}}{\\overline{w}}=\\frac{z\\times \\overline{w}}{|w|^2}.\\] Example 5.3 (Complex division) Let \\(z = 2 + 3i\\) and \\(w = 4 - 2i\\). \\[\\begin{align*} \\frac{z}{w}&amp;=\\frac{2 + 3i}{4 - 2i}\\frac{4+2i}{4+2i}\\\\ &amp;=\\frac{8+4i+12i+6i^2}{16+4}\\\\ &amp;=\\frac{2+16i}{20}\\\\ &amp;=\\frac{1}{10}+\\frac{4}{5}i \\end{align*}\\] A useful fact: if a polynomial has real coefficients and has a complex root \\(z\\), then its complex conjugate \\(\\overline{z}\\) is also a root; we say that complex roots appear in complex conjugate pairs in real polynomials. 5.3 The argument, polar and exponential form for complex numbers If we think of the complex number \\(z = x + iy\\) as a point \\(P = z\\) in the \\((x,y)\\)-plane, then it is natural to think also of representing \\(P\\) in polar coordinates \\((r,\\theta)\\) – see figure 5.2. Figure 5.2: The complex number \\(z = x + iy\\) in polar coordinates. Here, \\(r = |z| = \\sqrt{x^2 + y^2}\\) is the distance from the origin \\(O\\) and \\(\\theta\\) is the angle between the positive real axis and the line \\(OP\\). We call \\(\\theta\\) the argument of \\(z\\), denoted \\(\\theta = \\arg(z)\\). We have that \\[ P = (x,y) \\quad \\text{(in Cartesian coordinates)}\\quad \\text{or} \\quad P = (r,\\theta) \\quad \\text{(in Polar coordinates)} \\] with \\[\\begin{array}{rrclcrcl} &amp; r &amp;=&amp; \\sqrt{x^2 + y^2} &amp; \\quad\\text{and}\\quad &amp; \\tan(\\theta) &amp;=&amp; \\frac{y}{x},\\\\ \\text{or} \\qquad &amp; x &amp;=&amp; r \\cos(\\theta) &amp; \\quad\\text{and}\\quad &amp; y &amp;=&amp; r \\sin(\\theta). \\end{array}\\] Therefore, we can recover the Cartesian coordinates of \\(P\\) from the polar form \\((r, \\theta)\\) of a complex number \\(z\\) as \\[ z = r \\cos(\\theta) + i (r \\sin(\\theta)) = r (\\cos(\\theta) + i \\sin(\\theta)), \\] Theorem 5.2 (Euler's formula) For any real number \\(\\theta\\), \\[ e^{i\\theta}=\\cos (\\theta) + i \\sin(\\theta). \\] This can be derived from the power series of \\(e^\\theta, \\sin(\\theta)\\) and \\(\\cos(\\theta)\\). Eulers formula allows us to write any complex number \\(z\\) in yet another form, the exponential form of \\(z\\): \\[ z = r (\\cos (\\theta) + i \\sin (\\theta)) = re^{i\\theta}. \\] Note that since \\(\\sin\\) and \\(\\cos\\) are periodic with period \\(2\\pi\\), there are actually an infinite number of ways of expressing a complex number in polar or exponential form; for any integer value of \\(m\\) \\[re^{i(\\theta+2m\\pi)}=r(\\cos(\\theta+2m\\pi)+i\\sin(\\theta+2m\\pi))=r(\\cos(\\theta)+i\\sin(\\theta))=re^{i\\theta}=w\\] so we can add any multiple of \\(2m\\pi\\) to \\(\\theta\\) and get the same complex number. Graphically, this corresponds to going around a full circle \\(m\\) times in the complex plane and back to the same point. Figure 5.3: A complex number \\(w=re^{i\\theta}\\) plotted in the complex plane. Drag the sliders to vary \\(\\theta\\) and \\(r\\). [Open graph in browser.] The usual rules of exponents hold for complex exponentials, now using the rules of complex arithmetic. The exponential form is particularly nice when taking the product of two complex numbers. For \\(z = re^{i\\theta}\\) and \\(w = se^{i\\phi}\\), we have \\[\\begin{align*} u &amp;= (re^{i\\theta}) \\times (se^{i\\phi})\\\\ &amp;= rs e^{i(\\theta + \\phi)} \\\\ &amp;= R e^{i\\alpha}, \\end{align*}\\] where \\(R = rs\\) is the distance of \\(u\\) from the origin and \\(\\alpha = \\theta + \\phi\\) is the argument of \\(u\\). Caution: For \\(z = x + iy\\) (with \\(x \\neq 0\\)), if \\(\\theta = \\arg(z)\\) then we always have \\(\\tan (\\theta) = y/x\\). However, this does not mean that \\(\\theta = \\tan^{-1}(y/x)\\). The \\(\\tan^{-1}\\) (or \\(\\arctan\\)) function can only give values in the first and fourth quadrants (between \\(-\\pi/2\\) and \\(\\pi/2\\)), but the argument of a complex number could be in any of the four quadrants (between \\(-\\pi\\) and \\(\\pi\\)). Therefore, to find the argument of a complex number always draw a diagram. (A quick sketch is all that is needed.) 5.4 Roots of complex numbers We know that square roots of positive real numbers have two solutions, for example \\(\\sqrt{4}=\\pm 2\\). This agrees with Theorem (thm:FTA) as we could formulate this as a solution to the equation \\[x^2=4\\] which is the quadratic \\[x^2-4=0\\] and so must have two roots. More generally, the \\(n^\\text{th}\\) root of any number \\(w\\) can be formulated as the solutions to the polynomial equation \\[z^n-w=0.\\] Theorem (thm:FTA) implies that there are \\(n\\) solutions, allowing for these to be complex numbers. Furthermore, it turns out that these roots will be distinct (no repeated roots). The easiest way to find these roots is using the exponential form. We can write \\(w\\) as \\[w=re^{i\\theta}\\] then the first \\(n^\\text{th}\\) root is \\[w^{\\frac{1}{n}}=(re^{i\\theta})^\\frac{1}{n}=r^\\frac{1}{n}e^{i\\frac{\\theta}{n}}\\] but where are the remaining \\(n-1\\) roots? Well, since we can also write \\(w=re^{i(\\theta+2m\\pi)}\\), so \\[w^{\\frac{1}{n}}=r^{\\frac{1}{n}}e^{i(\\frac{\\theta}{n}+\\frac{2m}{n}\\pi)}\\] and we have a distinct solution for all values of \\(m=0,\\dotsc,n-1\\). Once we reach \\(m=n\\) we have gone full circle and are back to the original solution. To double check these are solutions, we can take them to the power \\(n\\) \\[\\begin{align*} (r^{\\frac{1}{n}}e^{i(\\frac{\\theta}{n}+\\frac{2m}{n}\\pi)})^n&amp;=re^{in(\\frac{\\theta}{n}+\\frac{2m}{n}\\pi)}\\\\ &amp;=re^{i(\\frac{n\\theta}{n}+\\frac{2mn}{n}\\pi)}\\\\ &amp;=re^{i(\\theta+2m\\pi)}\\\\ &amp;=re^{i\\theta}\\\\ &amp;=w. \\end{align*}\\] Geometrically, these \\(n\\) solutions are evenly spaced around a circle of radius \\(r^{\\frac{1}{n}}\\) in the complex plane, with an angle of \\(\\frac{2}{n}\\pi\\) between them. Example 5.4 (Complex roots) Find the cube roots of \\(z=2\\) and sketch them in the complex plane. We first write \\(z=2+0i\\) in exponential form \\[z=2e^{i0}\\] then the solutions are \\[\\begin{align*} w_1&amp;=\\sqrt[3]{2}e^{i0}=\\sqrt[3]{2}\\\\ w_2&amp;=\\sqrt[3]{2}e^{i(0+\\frac{2\\pi}{3})}=\\sqrt[3]{2}e^{i\\frac{2\\pi}{3}}\\\\ w_3&amp;=\\sqrt[3]{2}e^{i(0+\\frac{2(2)\\pi}{3})}=\\sqrt[3]{2}e^{i\\frac{4\\pi}{3}} \\end{align*}\\] Figure 5.4: The cube roots of \\(2\\) in the complex plane. Note they are evenly spaced around a circle of radius \\(\\sqrt[3]{2}\\). 5.5 \\(e^{i\\theta}\\) and trigonometric identities There are many double angle, and similar, formulas relating the various trig. functions. Remembering them can be quite a struggle. In fact we can recover them easily using complex numbers in polar form: \\[ e^{i \\theta} = \\cos (\\theta) + i \\sin (\\theta). \\] So for arbitrary \\(\\theta\\) and \\(\\phi\\) \\[ e^{i \\theta} e^{i \\phi} = \\begin{cases} (\\cos (\\theta) + i \\sin (\\theta)) \\times (\\cos (\\phi) + i \\sin (\\phi)) &amp; \\text{or} \\\\ e^{i (\\theta + \\phi)} = \\cos (\\theta + \\phi) + i \\sin (\\theta + \\phi). \\end{cases} \\] It follows that \\[\\begin{align*} (\\cos (\\theta) + i \\sin (\\theta)) \\times (\\cos (\\phi) + i \\sin (\\phi)) &amp;= \\cos (\\theta) \\cos (\\phi) - \\sin (\\theta) \\sin (\\phi) + i (\\cos (\\theta) \\sin (\\phi) + \\sin (\\theta) \\cos (\\phi))\\\\ &amp;= \\cos (\\theta + \\phi) + i \\sin (\\theta + \\phi) \\end{align*}\\] and equating real and imaginary parts on the left and right hand sides gives: \\[ \\cos (\\theta) \\cos (\\phi) - \\sin (\\theta) \\sin (\\phi) = \\cos (\\theta + \\phi) \\qquad \\text{and}\\qquad \\cos (\\theta) \\sin (\\phi) + \\sin (\\theta) \\cos (\\phi) = \\sin (\\theta + \\phi) \\] Engineers often use the notation \\(j\\) instead of \\(i\\).↩︎ "],["vectors.html", "Chapter 6 Vectors 6.1 Vector addition 6.2 Scalar multiplication 6.3 Vectors in Cartesian coordinates 6.4 Vector products", " Chapter 6 Vectors In physics, we distiguish between scalar quantities and vector quantities: Scalars are defined by a single numeric value. Examples include distance, speed, time, temperature, pressure, mass and energy. Vectors have associated both a numeric value and a direction. Examples include displacement, velocity, acceleration, force, and momentum. We depict vectors by arrows whose length is proportional to the magnitude of the quantity in question and whose direction is that of the action of the quantity. In symbols, we will use bold typeface to indicate a vector, such as \\(\\mathbf{v}\\), and we will denote a vector from point \\(A\\) to \\(B\\) by \\(\\overrightarrow{AB}\\). See figure 6.1. Figure 6.1: The vector \\(\\mathbf{v} = \\overrightarrow{AB}\\) is the vector from \\(A\\) to \\(B\\). Next we develop the mathematics of vectors. They are invaluable tools in science and engineering and in particular they can make light work of some otherwise difficult geometric calculations. 6.1 Vector addition Geometrically, to add two vectors together we can place the tail of one vector at the head of the other and take the vector that goes from the starting point to the finish point. Figure 6.2: Addition of two vectors \\(\\textbf{v}\\) and \\(\\textbf{w}\\) by aligning them tail to head. Example 6.1 (Vector addition) A swallow is flying at 40 km/h due north with an easterly wind blowing at a speed of 20 km/h. The swallow’s groundspeed can be found by adding the velocity vectors: \\[\\begin{align*} \\mathbf{v} &amp;= 40 \\text{ km/h North}\\\\ \\mathbf{w} &amp;= 20 \\text{ km/h West}\\\\ \\mathbf{v}+\\mathbf{w}&amp;=20\\sqrt{5} \\text{ km/h North by North West}. \\end{align*}\\] Alternatively, we can use the parallelogram rule (which amounts to the same thing): we draw a parallelogram with two of the non-parallel sides given by the vectors and take the diagonal as the resultant vector, as in figure 6.3. Figure 6.3: Parallelogram for addition of two vectors \\(\\textbf{v}\\) and \\(\\textbf{w}\\). 6.2 Scalar multiplication If we multiply a vector by a positive scalar, we change only the magnitude of the vector and keep its direction. For example, if the swallow in example 6.1 starts flying twice as fast, we have the velocity vector \\[2\\mathbf{v} = 80 \\text{ km/h North}.\\] If we multiply by a negative value, the vector now points in the opposite direction: \\[-2\\mathbf{v} = 80 \\text{ km/h South}.\\] Figure 6.4: Scalar multiplication of a vector \\(\\mathbf{v}\\). Finally, if we multiply by \\(0\\), we get the zero-vector \\(\\mathbf{0}\\), which has magnitude \\(0\\) and no direction: \\[0\\mathbf{v}=\\mathbf{0}\\]. 6.3 Vectors in Cartesian coordinates We mostly deal with vectors in a Cartesian coordinate system. We then define vectors of length 1 that point out from the origin along the coordinate axes. In three dimensions we label these as \\(\\mathbf{i}\\) along the \\(x\\)-axis, \\(\\mathbf{j}\\) along the \\(y\\)-axis and \\(\\mathbf{k}\\) along the \\(z\\)-axis. In two dimensions we simply drop the \\(\\mathbf{k}\\) vector. We can then write any vector \\(\\overrightarrow{OA}\\) from the origin \\(O\\) to a point \\(A\\) as some combination of the these vectors. For example, the vector in the plane from the origin to the point \\(A=(2,3)\\) is given by: \\[\\overrightarrow{OA}=2\\mathbf{i} + 3\\mathbf{j}.\\] We can reconcile this with our geometric understanding of addition of two vectors: we place the vector \\(2\\mathbf{i}\\) from the origin to the point \\((2,0)\\) and then place the tail of the vector \\(3\\mathbf{j}\\) at the head of \\(2\\mathbf{i}\\) to get to the point \\((2,3)\\); or, in terms of the parallelogram rule we have a rectangle with sides \\(2\\mathbf{i}\\) and \\(3\\mathbf{j}\\). See figure 6.5. Figure 6.5: The vector \\(2\\mathbf{i}+3\\mathbf{j}\\). Yet another way to write this vector is as a column vector \\[\\overrightarrow{OA}= \\begin{pmatrix} 2\\\\ 3 \\end{pmatrix}. \\] Such a representation is called a coordinate vector. More generally, the vector \\[\\mathbf{v}=x\\mathbf{i}+y\\mathbf{j}+z\\mathbf{k}\\] can be written as the column \\[\\begin{pmatrix} x\\\\ y\\\\ z \\end{pmatrix}. \\] Now addition and scalar multiplication can be carried out component-wise as follows. If we have \\[\\mathbf{v}=x\\mathbf{i}+y\\mathbf{j}+z\\mathbf{k}\\] and \\[\\mathbf{w}=x&#39;\\mathbf{i}+y&#39;\\mathbf{j}+z&#39;\\mathbf{k}\\] then \\[\\mathbf{v}+\\mathbf{w}=(x+x&#39;)\\mathbf{i}+(y+y&#39;)\\mathbf{j}+(z+z&#39;)\\mathbf{k}\\] and with \\(a\\) a scalar, \\[a\\mathbf{v}=ax\\mathbf{i}+ay\\mathbf{j}+az\\mathbf{k}.\\] Or, as column vectors with \\[ \\mathbf{v} = \\begin{pmatrix} x\\\\ y\\\\ z \\end{pmatrix}, \\qquad \\mathbf{w} = \\begin{pmatrix} x&#39;\\\\ y&#39;\\\\ z&#39; \\end{pmatrix} \\] we have \\[\\mathbf{v}+\\mathbf{w}= \\begin{pmatrix} x+x&#39;\\\\ y+y&#39;\\\\ z+z&#39; \\end{pmatrix}. \\] and \\[ a\\mathbf{v} = \\begin{pmatrix} ax\\\\ ay\\\\ az \\end{pmatrix}. \\] If points \\(A\\) and \\(B\\) have coordinates \\((a_1,a_2,a_3)\\) and \\((b_1,b_2,b_3)\\), respectively, then the vector from \\(A\\) to \\(B\\) is \\[\\overrightarrow{AB} = \\begin{pmatrix} b_1 - a_1 \\\\ b_2 - a_2\\\\ b_3 - a_3\\end{pmatrix}. \\] In 3-dimensional Cartesian coordinates, the length of the vector \\[\\mathbf{v} = \\begin{pmatrix} x\\\\ y\\\\ z\\end{pmatrix}\\] is written as \\(|\\mathbf{v}|\\) and defined as \\[|\\mathbf{v}| = \\sqrt{x^2 + y^2 + z^2}\\]. This is essentially a three-dimensional version of the Pythagorean theorem. A vector \\(\\mathbf{u}\\) is said to be a unit vector if it has unit length. That is, if \\(|\\mathbf{u}| = 1\\). For any non-zero vector \\(\\mathbf{v}\\), the vector \\[ \\mathbf{u}=\\frac{1}{|\\mathbf{v}|} \\mathbf{v} \\] is a unit vector in the same direction as \\(\\mathbf{v}\\). Example 6.2 Let \\[\\mathbf{v} = \\begin{pmatrix} 2\\\\ -3\\\\ 6\\end{pmatrix}\\] Then the length of \\(\\mathbf{v}\\) is \\(|\\mathbf{v}| = \\sqrt{2^2 + 3^2 + 6^2} = 7\\). So the unit vector \\(\\mathbf{u}\\) parallel to \\(\\mathbf{v}\\) is \\[ \\mathbf{u}=\\frac{1}{7}\\begin{pmatrix} 2\\\\ -3\\\\ 6\\end{pmatrix}. \\] 6.4 Vector products We have seen how to add vectors and how to multiply vectors with a scalar. In the following, we see how to multiply two vectors with each other. 6.4.1 Dot product The dot product (also called the scalar product or inner product) of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), denoted \\(\\mathbf{u}\\cdot\\mathbf{v}\\), is defined to be the quantity \\[\\begin{equation} \\mathbf{u}\\cdot\\mathbf{v} = |\\mathbf{u}||\\mathbf{v}|\\cos(\\theta) \\tag{6.1} \\end{equation}\\] where \\(\\theta\\) is the angle between the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). Note that the symmetry of the \\(\\cos\\) function means it does not matter which direction we measure the angle, i.e. clockwise or anticlockwise. Figure 6.6: The projection of \\(\\mathbf{v}\\) onto \\(\\mathbf{u}\\). An immediate application of the scalar product is the projection of one vector onto another. Consider Figure 6.6, which shows the projection of the vector \\(\\mathbf{v}\\) onto the vector \\(\\mathbf{u}\\). We drop a line from the tip of \\(\\mathbf{v}\\) running at right angles to the direction of the vector \\(\\mathbf{u}\\). This is akin to thinking of the shadow that \\(\\mathbf{v}\\) would cast on the vector \\(\\mathbf{u}\\) if the Sun is shining at right angles to \\(\\mathbf{u}\\). The length of the shadow is the projection of \\(\\mathbf{v}\\) onto \\(\\mathbf{u}\\), or \\[|\\mathbf{v}|\\cos(\\theta).\\] We have: \\[\\mathbf{u} \\cdot \\mathbf{v} &amp;= |\\mathbf{u}| \\times \\text{projection of $\\mathbf{v}$ onto $\\mathbf{u}$}\\] Since there was nothing special about choosing the projection onto \\(\\mathbf{v}\\) – we could have chosen the projection onto \\(\\mathbf{u}\\) – it is also true that: \\[\\mathbf{u} \\cdot \\mathbf{v}= |\\mathbf{v}| \\times \\text{projection of $\\mathbf{u}$ onto $\\mathbf{v}$}.\\] Example 6.3 (Work) An application of the dot product in physics is to the concept of work. We want to consider how much work is required to push a block over a certain distance \\(x\\). By calculation or measurement, we can determine the force \\(F\\) required to push the block. Then, by definition, the work done by the force is \\[W = Fx.\\] This is sufficient if the force acts in the same direction as the motion of the block. If, however, this is not the case (maybe the block is constrained to move on a rail), then it is the component of the force in the direction of motion that is important. Suppose \\(\\mathbf{F}\\) is the force vector and \\(\\mathbf{x}\\) is the displacement vector of the block. Let \\(\\theta\\) be the angle between the two vectors. Then the component of \\(\\mathbf{F}\\) in the \\(\\mathbf{x}\\) direction is precisely the projection of \\(\\mathbf{F}\\) onto \\(\\mathbf{x}\\) (as described in Figure 6.6). This is equal to \\[ |\\mathbf{F}|\\cos(\\theta). \\] So the total work done will be the product of this quantity with the magnitude of the displacement vector, that is \\[\\begin{align*} W &amp;= |\\mathbf{F}|\\cos(\\theta) \\times |\\mathbf{x}| \\\\ &amp;= |\\mathbf{F}||\\mathbf{x}|\\cos(\\theta) \\\\ &amp;= \\mathbf{F}\\cdot\\mathbf{x}, \\end{align*}\\] i.e. the dot product of \\(\\mathbf{F}\\) and \\(\\mathbf{x}\\). Note that if the dot product is \\(\\mathbf{u}\\cdot\\mathbf{v}=0\\) then either \\(\\cos(\\frac{\\pi}{2})=0\\) so that the two vectors are perpendicular, or one or both of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are the zero vector \\(\\mathbf{0}\\). When the dot product is zero, we say that the two vectors are orthogonal. Some other properties of the dot product that follow from the definition are: \\(\\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}\\); \\((a\\mathbf{u}) \\cdot \\mathbf{v} = \\mathbf{u} \\cdot (a\\mathbf{v})= a(\\mathbf{u} \\cdot \\mathbf{v})\\); \\(\\mathbf{u} \\cdot (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\cdot \\mathbf{v} + \\mathbf{u} \\cdot \\mathbf{w}\\); \\(\\mathbf{u}\\cdot\\mathbf{u} = |\\mathbf{u}|^2\\). Theorem 6.1 (Calculating the dot product) In Cartesian coordinates, the dot product of two vectors \\[\\mathbf{u} = x\\mathbf{i} + y\\mathbf{j} + z\\mathbf{k} \\qquad\\text{and}\\qquad \\mathbf{v} = a\\mathbf{i} + b\\mathbf{j} + c\\mathbf{k}\\] can be calulated as \\[\\begin{equation*} \\mathbf{u}\\cdot\\mathbf{v} = (x\\mathbf{i} + y\\mathbf{j} + z\\mathbf{k})\\cdot(a\\mathbf{i} + b\\mathbf{j} + c\\mathbf{k}) = xa + yb + zc. \\end{equation*}\\] To see this, consider the full product: \\[\\begin{align*} \\mathbf{u} \\cdot \\mathbf{v} &amp;= xa \\mathbf{i} \\cdot \\mathbf{i} + xb \\mathbf{i} \\cdot \\mathbf{j} + xc \\mathbf{i} \\cdot \\mathbf{k} \\\\ &amp;\\phantom{={}} + ya \\mathbf{j} \\cdot \\mathbf{i} + yb \\mathbf{j} \\cdot \\mathbf{j} + yc \\mathbf{j} \\cdot \\mathbf{k} \\\\ &amp;\\phantom{={}} + za \\mathbf{k} \\cdot \\mathbf{i} + zb \\mathbf{k} \\cdot \\mathbf{j} + zc \\mathbf{k} \\cdot \\mathbf{k} \\\\ &amp;= xa + yb + zc, \\end{align*}\\] then noting that the unit vectors \\(\\mathbf{i}\\), \\(\\mathbf{j}\\) and \\(\\mathbf{k}\\) have unit length and are perpendicular to one another, we first have by the property \\(\\mathbf{u}\\cdot\\mathbf{u}=|\\mathbf{u}|^2\\) that \\[ \\mathbf{i}\\cdot\\mathbf{i} = \\mathbf{j}\\cdot\\mathbf{j} =\\mathbf{k}\\cdot\\mathbf{k}=1, \\] and secondly, by orthogonality each of the following products are zero \\[\\begin{align*} \\mathbf{i} \\cdot \\mathbf{j}&amp;,\\; \\mathbf{i} \\cdot \\mathbf{k}\\\\ \\mathbf{j} \\cdot \\mathbf{i}&amp;,\\; \\mathbf{j} \\cdot \\mathbf{k}\\\\ \\mathbf{k} \\cdot \\mathbf{i}&amp;,\\; \\mathbf{k} \\cdot \\mathbf{j}. \\end{align*}\\] Example 6.4 (Geometry) Consider a triangle in three-dimensional space whose vertices are given by the points \\[ A = (2,4,-1),\\quad B = (0,3,1), \\quad C = (-3,1,5). \\] We compute the side lengths and angles for the triangle \\(ABC\\) above. First note that \\[ \\overrightarrow{AB} = \\begin{pmatrix} 0\\\\ 3\\\\ 1\\end{pmatrix} - \\begin{pmatrix} 2\\\\ 4\\\\ -1\\end{pmatrix} = \\begin{pmatrix} -2\\\\ -1\\\\ 2\\end{pmatrix} \\] and similarly \\[ \\overrightarrow{BC} = \\begin{pmatrix} -3\\\\ -2\\\\ 4\\end{pmatrix} \\quad\\text{and}\\quad \\overrightarrow{CA} = \\begin{pmatrix} 5\\\\ 3\\\\ -6\\end{pmatrix}. \\] From this, we can immediately calculate the length of the sides, namely \\[\\begin{align*} AB &amp;= |\\overrightarrow{AB}| = \\sqrt{(-2)^2 + (-1)^2 + 2^2} = 3, \\\\ BC &amp;= |\\overrightarrow{BC}| = \\sqrt{(-3)^2 + (-2)^2 + 4^2} = \\sqrt{29}, \\\\ CA &amp;= |\\overrightarrow{CA}| = \\sqrt{5^2 + 3^2 + (-6)^2} = \\sqrt{70}. \\end{align*}\\] The angles can then be computed using the dot product. For instance, the angle \\(\\theta_A\\) at the vertex \\(A\\) can be computed via \\[ \\cos(\\theta_A) = \\frac{\\overrightarrow{AB}\\cdot\\overrightarrow{AC}}{AB\\times AC} = -\\frac{\\overrightarrow{AB}\\cdot\\overrightarrow{CA}}{AB\\times CA}, \\] thus \\[ \\cos(\\theta_A) = -\\frac{-2\\times 5 - 1\\times 3 + 2\\times(-6)}{3\\sqrt{70}} = \\frac{25}{3\\sqrt{70}} = \\frac{25\\sqrt{70}}{3\\times 70} = \\frac{5\\sqrt{70}}{42}. \\] Similarly, we calculate \\[ \\cos(\\theta_B) = \\frac{\\overrightarrow{BA}\\cdot\\overrightarrow{BC}}{BA\\times BC} = -\\frac{16}{87}\\sqrt{29}. \\] and \\[ \\cos(\\theta_C) = \\frac{\\overrightarrow{CA}\\cdot\\overrightarrow{CB}}{CA\\times CB} = \\frac{9}{406}\\sqrt{29}\\sqrt{70}. \\] 6.4.2 Cross product Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in a three-dimensional space and let \\(\\theta\\) be the angle between the vectors. Note there are potentially two choices for the angle between two vectors: both \\(\\theta\\) and \\(2\\pi - \\theta\\). Unlike the case of the dot product, we need to be clear about our choice of angle \\(\\theta\\). We choose the angle \\(\\theta\\) such that \\(0 \\le \\theta &lt; \\pi\\). The cross product (or vector product) \\(\\mathbf{u}\\times\\mathbf{v}\\) of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in three-dimensional space is defined to be the (unique) vector of length \\[ |\\mathbf{u}||\\mathbf{v}|\\sin(\\theta) \\] that is orthogonal to both the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), such that \\(\\mathbf{u}, \\mathbf{v}\\) and \\(\\mathbf{u}\\times\\mathbf{v}\\) form a right-handed system. The last part of the definition needs some explanation. There are always two possible directions that are perpendicular to \\(\\mathbf{u}\\) and to \\(\\mathbf{v}\\), one being the opposite of the other. When we say that a set of three vectors \\(\\{\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\}\\) forms a right-handed system, we mean that if take our right hand and point our thumb in the direction of \\(\\mathbf{u}\\) and our index finger in the direction of \\(\\mathbf{v}\\), then our middle finger points (roughly) in the direction of \\(\\mathbf{w}\\). From Figure 6.7 we see that \\(\\{\\mathbf{u}, \\mathbf{v}, \\mathbf{u}\\times\\mathbf{v}\\}\\) form a right-handed system. Figure 6.7: The cross product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The cross product has the following properties: \\(\\mathbf{u}\\times\\mathbf{v} = -(\\mathbf{v} \\times \\mathbf{u})\\); \\(\\mathbf{u}\\times\\mathbf{v} = \\mathbf{0}\\) if and only if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are parallel, or \\(\\mathbf{u} = \\mathbf{0}\\) or \\(\\mathbf{v} = \\mathbf{0}\\); \\(|\\mathbf{u}\\times\\mathbf{v}| = |\\mathbf{u}||\\mathbf{v}|\\) if and only if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal; \\((a\\mathbf{u})\\times\\mathbf{v} = \\mathbf{u}\\times(a\\mathbf{v}) = a(\\mathbf{u}\\times\\mathbf{v})\\); \\(\\mathbf{u}\\times(\\mathbf{v}+\\mathbf{w}) = \\mathbf{u}\\times\\mathbf{v} + \\mathbf{u}\\times\\mathbf{w}\\). It follows that: \\[ \\begin{array}{rclrclrcl} \\mathbf{i}\\times\\mathbf{j} &amp;=&amp; \\mathbf{k}, \\qquad &amp; \\mathbf{j}\\times\\mathbf{k} &amp;=&amp; \\mathbf{i}, \\qquad &amp; \\mathbf{k}\\times\\mathbf{i} &amp;=&amp; \\mathbf{j},\\\\ \\mathbf{j}\\times\\mathbf{i} &amp;=&amp; -\\mathbf{k}, \\qquad &amp; \\mathbf{k}\\times\\mathbf{j} &amp;=&amp; -\\mathbf{i}, \\qquad &amp; \\mathbf{i}\\times\\mathbf{k} &amp;=&amp; -\\mathbf{j},\\\\ \\mathbf{i}\\times\\mathbf{i} &amp;=&amp; \\mathbf{0}, \\qquad &amp; \\mathbf{j}\\times\\mathbf{j} &amp;=&amp; \\mathbf{0}, \\qquad &amp; \\mathbf{k}\\times\\mathbf{k} &amp;=&amp; \\mathbf{0}. \\end{array} \\] Then we can compute the cross product of \\[ \\mathbf{u} = u_1\\mathbf{i} + u_2\\mathbf{j} + u_3\\mathbf{k} \\qquad\\text{and}\\qquad \\mathbf{v} = v_1\\mathbf{i} + v_2\\mathbf{j} + v_3\\mathbf{k}, \\] as \\[ \\mathbf{u} \\times \\mathbf{v} = (u_2v_3 - u_3v_2)\\mathbf{i} + (u_3v_1 - u_1v_3)\\mathbf{j} + (u_1v_2 - u_2v_1)\\mathbf{k}. \\] An easy way to remember this is to compute the determinant: \\[ \\mathbf{u}\\times\\mathbf{v} = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k}\\\\ u_1 &amp; u_2 &amp; u_3\\\\ v_1 &amp; v_2 &amp; v_3\\end{vmatrix}. \\] Example 6.5 (Magnetic fields) The vector product occurs in many equations in physics. For instance, consider a charged particle moving through a magnetic field. Let the charge on the particle be \\(q\\) Coulombs, let its velocity vector be \\(\\mathbf{v}\\) metres per second and let the magnetic field be \\(\\mathbf{B}\\) Teslas. Then the force \\(\\mathbf{F}\\) Newtons experienced by the particle is \\[ \\textbf{F} = q\\mathbf{v}\\times\\textbf{B}. \\] This formula expresses mathematically the fact that the force experienced by the particle is perpendicular to its direction of motion and to the magnetic field. Example 6.6 (Triangle area) What is the area of the triangle with vertices \\[ A = (2,4,-1), \\qquad B = (0,3,1), \\qquad C = (-3,1,5)? \\] Note that this is the same triangle that we considered in Example @ref{exm:dottriangle}, where we considered the lengths of the sides and the angles at the vertices. Figure 6.8: Computing the area of a triangle via the vector product. The area of a triangle is half its base times its height. In view of the diagram in Figure 6.8, the base is \\(|\\overrightarrow{AB}|\\) and the height is \\(|\\overrightarrow{AC}| \\sin(\\theta_A)\\), so the area of the triangle is \\[ \\frac{1}{2} |\\overrightarrow{AB}||\\overrightarrow{AC}| \\sin(\\theta_A). \\] This quantity can be computed in terms of the vector product as \\[ \\frac{1}{2} |\\overrightarrow{AB}\\times\\overrightarrow{AC}|. \\] Using the determinant method for computing the vector product, we obtain \\[\\begin{align*} \\overrightarrow{AB}\\times\\overrightarrow{AC} = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k}\\\\ -2 &amp; -1 &amp; 2\\\\ -5 &amp; -3 &amp; 6\\end{vmatrix} = (-6 + 6)\\mathbf{i} - (-12 + 10)\\mathbf{j} + (6 - 5)\\mathbf{k} = 2\\mathbf{j} + \\mathbf{k}. \\end{align*}\\] Thus, \\[ |\\overrightarrow{AB}\\times\\overrightarrow{AC}| = \\sqrt{2^2 + 1^2} = \\sqrt{5}, \\] and we deduce that the area of the triangle \\(ABC\\) is equal to \\(\\frac{\\sqrt{5}}{2}\\). "],["systems-of-linear-equations.html", "Chapter 7 Systems of Linear Equations 7.1 Lines and planes 7.2 Solving linear systems - Gaussian elimination 7.3 Echelon Form", " Chapter 7 Systems of Linear Equations So-called linear equations arise often in mathematical problems in science and engineering. In this chapter we will see a very useful method for solving simultaneous linear equations, known as Gaussian elimination. We’ll start by understanding the geometric nature of linear equations. 7.1 Lines and planes A linear equation is one of the form \\[a_1x_1+a_2x_2+\\dotsb+a_nx_n=b\\] where the \\(x_i\\)’s are variables, the \\(a_i\\)’s are coefficients, \\(b\\) is a constant and \\(n\\) is the number of variables in the equation. For \\(n=2\\), we have an equation of the form \\[a_1x_1+a_2x_2=b\\] which we know to be the equation of a line in the Cartesian plane (see section 3.1, where we used the notation \\(ax+by=c\\)). Suppose we have a pair of such equations. Then we could conveniently write them as \\[\\begin{align*} a_{11}x_1+a_{12}x_2&amp;=b_1\\\\ a_{21}x_1+a_{22}x_2&amp;=b_2 \\end{align*}\\] where now the first subscipt \\(i\\) on the \\(a_{ij}\\)’s and \\(b_i\\)’s denotes the equation it belongs to and the second subscipt \\(j\\) denotes the variable it belongs to. If we need to solve these equations to find the values of \\(x_1\\) and \\(x_2\\) that satisfy both equations simultaneously, then we call this a system of linear simultaneous equations. Figure 7.1: The two intersecting lines from example (exm:gaus1). Geometrically, a solution to the equations corresponds to those coordinates \\((x_1,x_2)\\) where the lines meet – these are values of \\(x_1\\) and \\(x_2\\) that satisfy both equations simultaneously – see figure (fig:sim-lines). We therefore have three different scenarios for the solutions of such equations. Most of the time, two lines in the plane will cross at exactly one point, hence we have just one solution (one pair of coordinates \\((x_1,x_2)\\)). It is possible that both lines are parallel to one another, with different axis intercepts. In this case, the lines never meet – there are no solutions. Finally, it is possible that the two equations actually describe the same line. In this case, any pair of coordinates \\((x_1,x_2)\\) that lies on the (common) line is a solution – there are infinitely many solutions. This idea extends to having more equations. For example, if we have a system of three equations in two variables: \\[\\begin{align*} a_{11}x_1+a_{12}x_2&amp;=b_1\\\\ a_{21}x_1+a_{22}x_2&amp;=b_2\\\\ a_{31}x_1+a_{32}x_2&amp;=b_3 \\end{align*}\\] Then it is possible that: All three lines will cross at exactly one point, hence we have just one solution. The three lines don’t cross at a common point – there are no solutions. This is the most probable scenario in this set-up: whilst any two of the lines might cross at a point, we need all three to cross at the same point in order for there to be a solution to the simultaneous equations. Finally, it is possible that the three equations actually describe the same line and again there are infinitely many solutions. When there are more equations than variables, the system is sometimes called overdetermined, since then we usually have scenario 2 above where trying to satsify the contraints impossed by the three equations is not possible. On the other hand, a system with less equations is sometimes called “underdetermined”, since in these cases we usually have an infinite number of solutions. For example, if we had a system with just one equation in two variables \\[\\begin{align*} a_{11}x_1+a_{12}x_2&amp;=b_1\\\\ \\end{align*}\\] then all points \\((x_1,x_2)\\) lying on this line are solutions. We can extend these ideas to higher dimensional systems, i.e. a higher number of variables. For example, if we have three equations in three variables \\[\\begin{align*} a_{11}x_1+a_{12}x_2+a_{13}x_3&amp;=b_1\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3&amp;=b_2\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3&amp;=b_3 \\end{align*}\\] then each equation describes a plane in 3-dimensional Cartesian coordinates. Now we have: A unique solution \\((x_1,x_2,x_3)\\) if all three planes cross at a single point – the most likely scenario. No solutions if none of the planes cross at a common point – think about all the ways this could happen, there are more ways than in two dimensions! Infinetely many solutions if all three planes intersect along a line or all are the same plane – again, think about the ways this could happen! You should also think about the meaning of overdetermined and underdetermined systems in 3-dimensions and how they look geometrically. We can of course extend these ideas to as many variables and equations as we wish: a linear equation in \\(n\\) variables represents a hyperplane in \\(n\\)-dimensional space, which is a subspace with dimension \\(n-1\\), but is impossible for us to directly visualise things in dimensions greater than 3! 7.2 Solving linear systems - Gaussian elimination There are a number of different approaches to solving linear equations. They all ultimately amount to the same thing, but some methods are more straighforward or computationally efficient depending on the situation. In example 2.4 we use rearrangement and substitution to solve \\[\\begin{align*} 3x_1+2x_2&amp;=5\\\\ x_1-4x_2&amp;=1. \\end{align*}\\] Now we will instead add and subtract multiples of each equation to reduce them to the solution. Example 7.1 Solve \\[\\begin{align*} 3x_1+2x_2&amp;=5\\\\ x_1-4x_2&amp;=1. \\end{align*}\\] First, lets swap the order of the equations so that the coefficient of \\(x_1\\) in the first equation is \\(1\\) \\[\\begin{align*} x_1-4x_2&amp;=1\\\\ 3x_1+2x_2&amp;=5. \\end{align*}\\] Next, we subtract 3 times the first equation from the second to eliminate \\(x_1\\) from the second equation. The second equation becomes \\[\\begin{align*} 3x_1+2x_2-3(x_1-4x_2)&amp;=5-3\\times 1\\\\ 14x_2&amp;=2 \\end{align*}\\] so we have the equivalent system \\[\\begin{align*} x_1-4x_2&amp;=1\\\\ 14x_2&amp;=2. \\end{align*}\\] Now divide the second equation by 14 \\[\\begin{align*} x_1-4x_2&amp;=1\\\\ x_2&amp;=\\frac{1}{7}. \\end{align*}\\] Add 4 times the second equation to the first to eliminate \\(x_2\\) from the first equation \\[\\begin{align*} x_1-4x_2+4x_2&amp;=1+\\frac{4}{7}\\\\ x_2&amp;=\\frac{1}{7}. \\end{align*}\\] and hence we have the solution \\[\\begin{align*} x_1&amp;=\\frac{11}{7}\\\\ x_2&amp;=\\frac{1}{7}. \\end{align*}\\] Note that in the above example, it is the coefficients \\(a_{ij}\\) and \\(b_i\\) that we are manipulating – the \\(x_j\\)’s just came along for the ride. So, let’s simplify things by just writing down the coefficients in the system of equations in a table as follows: \\[\\left(\\begin{array}{rr|r} 3&amp;2\\\\ 1&amp;-4 \\end{array} \\right). \\] This is called the matrix of coefficients. We also need to keep track of the right-hand side of the equation, so we add this as another column separated by a vertical line: \\[\\left(\\begin{array}{rr|r} 3&amp;2&amp;5\\\\ 1&amp;-4&amp;1 \\end{array} \\right). \\] We call this the augmented matrix. Now in solving the system of equations, we simply add/subtract multiples of rows of this augmented matrix. Labelling each row as \\(R_1\\) and \\(R_2\\) we perform the following operations: swap rows 1 and 2: \\(R_1\\leftrightarrow R_2\\) \\[\\left(\\begin{array}{rr|r} 1&amp;-4&amp;1\\\\ 3&amp;2&amp;5 \\end{array} \\right). \\] subtract 3 times row 1 from row 2: \\(R_2 \\to R_2-3R_1\\) \\[\\left(\\begin{array}{rr|r} 1&amp;-4&amp;1\\\\ 0&amp;14&amp;2 \\end{array} \\right). \\] divide row 2 by 14: \\(R_2\\to \\frac{1}{14}R_2\\) \\[\\left(\\begin{array}{rr|r} 1&amp;-4&amp;1\\\\ 0&amp;1&amp;\\frac{1}{7} \\end{array} \\right). \\] add 4 times the second row to the first: \\(R_1 \\to R_1+4 R_2\\) \\[\\left(\\begin{array}{rr|r} 1&amp;0&amp;\\frac{11}{7}\\\\ 0&amp;1&amp;\\frac{1}{7} \\end{array} \\right) \\] This represents \\[\\begin{align*} 1\\times x_1 + 0 \\times x_2 &amp;=x_1= \\frac{11}{7}\\\\ 0\\times x_1 + 1 \\times x_2 &amp;=x_2= \\frac{1}{7}\\\\ \\end{align*}\\] that is, we can just read off the solutions in the right-most column. This method is known as Gaussian9 elimination and provides a systematic way to solve simultaneous linear equations. Note that each step in the above process is reversible. For instance, the operation \\[R_1 \\to R_1 + 4R_2\\] can be reversed by performing \\[R_1 \\to R_1 - 4R_2.\\] This means that the final set of equations produced by Gaussian elimination is equivalent to the original set of equations and therefore has the same set of solutions. No matter the number of variables or equations, the aim is to reduce the coefficient matrix to a diagonal of \\(1\\)’s (or as close as possible, we’ll clarify this shortly) and \\(0\\)’s everywhere else, like so \\[\\left(\\begin{array}{rrrr|r} 1&amp;0&amp;\\dotsb&amp;0&amp;\\omega_1\\\\ 0&amp;1&amp;\\dotsb&amp;0&amp;\\omega_2\\\\ \\vdots&amp;&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;\\dotsb&amp;1&amp;\\omega_n \\end{array} \\right), \\] then read off the solutions as \\(x_1=\\omega_1, x_2=\\omega_2,\\dotsc, x_n=\\omega_n\\) from the right hand column. The following elementary row operations (EROs) comprise all operations required for Gaussian elimination: \\(R_i \\to R_i + \\alpha R_k\\), where \\(k \\neq i\\) and \\(\\alpha \\neq 0\\) is some scalar; \\(R_i \\to \\alpha R_i\\), where \\(\\alpha \\neq 0\\) is some scalar; \\(R_i \\leftrightarrow R_k\\), where \\(k\\neq i\\), denotes the swapping of the \\(i\\)-th and \\(k\\)-th rows. We now present a sketch of the full Gaussian elimination procedure using these EROs. First, note that we call the first non-zero entry (reading from left to right) in any row of a matrix the leading entry. A leading entry will always be used as a pivot, where a pivot is an element used to zero-out the other entries in the same column as the pivot. select the leading entry in the top row as a pivot; perform EROs to eliminate the entries below the pivot; move down to the next row and continue the process; having reached the bottom row, work back up again, that is: select the bottom right entry in the matrix of coefficients as the pivot; perform EROs to eliminate the entries above the pivot; move to the row above and continue the process; having reached the top row, stop and read off the solutions. Let’s have a look at another example applying these steps, this time in 3-dimensions. Example 7.2 Solve the system of equations \\[\\begin{align*} x+y+z&amp;=3,\\\\ 2x+3y+z&amp;=10, \\\\ 3x-y+2z&amp;=-2. \\end{align*}\\] We start by writing down the augmented matrix: \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 3 &amp; 1 &amp; 10 \\\\ 3 &amp; -1 &amp; 2 &amp; -2 \\end{array}\\right). \\] We select the top-left hand entry of the augmented matrix as a pivot. We eliminate the entries in the same column that lie below the pivot, that is \\[\\require{enclose} \\begin{array}{l} \\phantom{\\enclose{circle}{1}}\\quad \\\\ R_2 \\to R_2 - 2R_1\\colon \\quad\\\\ R_3 \\to R_3 - 3R_1\\colon \\quad \\end{array} \\left(\\begin{array}{crr|r} \\enclose{circle}{1} &amp; 1 &amp; 1 &amp; 3 \\\\ 0 &amp; 1 &amp; -1 &amp; 4 \\\\ 0 &amp; -4 &amp; -1 &amp; -11 \\end{array}\\right). \\] Then we select the first non-zero entry in the second row as our pivot and eliminate the entry below it, to obtain \\[ \\begin{array}{l} \\quad \\\\ \\phantom{\\enclose{circle}{1}} \\quad\\\\ R_3 \\to R_3 + 4R_2\\colon \\quad \\end{array} \\left(\\begin{array}{rcr|r} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 0 &amp; \\enclose{circle}{1} &amp; -1 &amp; 4 \\\\ 0 &amp; 0 &amp; -5 &amp; 5 \\end{array}\\right). \\] Next perform \\[ \\begin{array}{l} \\quad \\\\ \\quad\\\\ R_3 \\to -\\frac{1}{5}R_3\\colon \\quad \\end{array} \\left( \\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 3, \\\\ 0 &amp; 1 &amp; -1&amp; 4, \\\\ 0 &amp; 0 &amp; 1&amp; -1. \\end{array}\\right). \\] We continue, similarly to the steps above, by eliminating the entries above the pivot in the bottom row. \\[ \\begin{array}{l} R_1 \\to R_1 - R_3\\colon \\quad \\\\ R_2 \\to R_2 + R_3\\colon \\quad \\\\ \\phantom{\\enclose{circle}{1}}\\quad \\end{array} \\left(\\begin{array}{rrc|r} 1 &amp; 1 &amp; 0 &amp; 4 \\\\ 0 &amp; 1 &amp; 0 &amp; 3 \\\\ 0 &amp; 0 &amp; \\enclose{circle}{1} &amp;-1 \\end{array}\\right). \\] Finally, we eliminate the second column using the \\(1\\) in the second row as a pivot, that is \\[ \\begin{array}{l} R_1 \\to R_1 - R_2\\colon \\quad \\\\ \\phantom{\\enclose{circle}{1}}\\quad \\\\ \\quad \\end{array} \\left(\\begin{array}{rcr|r} 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; \\enclose{circle}{1} &amp; 0 &amp; 3 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right). \\] We read off the solution \\((x,y,z) = (1,3,-1)\\). 7.3 Echelon Form Recall that in general there may be one, none or infinitely many solutions to a system of linear equations. We now see what happens in Guassian elimination in these scenarios. When following the proceedure of Gaussian elimination we reduce the coefficient matrix to make the leading entry in each row equal to 1 and then clear all the values (make them zero) above and below this entry. This results in a coefficient matrix in what is known as reduced echelon form. This allows us to easily read off the solutions. Here is a formal definition of the reduced echelon form. Definition 7.1 (Reduced Echelon Form) A matrix is said to be in reduced echelon form (REF) if 1. all the rows consisting entirely of zeros lie below all non-zero rows (note, there need not be any zero or non-zero rows); 1. each leading entry in a row lies at least one place to the right of all leading entries in any previous rows; 1. every leading entry is \\(1\\); 1. each leading entry is the only non-zero entry in its column. The following are some examples of possible reduced echelon forms for \\(3\\times 3\\) coefficient matrices. \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; \\omega_1 \\\\ 0 &amp; 1 &amp; 0 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 1 &amp; \\omega_3 \\end{array}\\right),\\qquad \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 2 &amp; \\omega_1 \\\\ 0 &amp; 1 &amp; 0 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 0 &amp; \\omega_3 \\end{array}\\right),\\qquad \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; \\omega_1 \\\\ 0 &amp; 0 &amp; 1 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 0 &amp; \\omega_3 \\end{array}\\right). \\] The previous examples were all square coefficient matrices (the same number of equations as variables). It is also possible that we have rectangular coefficient matrices. Here are some possible echelon forms. \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; \\omega_1 \\\\ 0 &amp; 1 &amp; 0 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 1 &amp; \\omega_3 \\\\ 0 &amp; 0 &amp; 0 &amp; \\omega_4 \\end{array}\\right),\\qquad \\left(\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 2 &amp; -4&amp;\\omega_1 \\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\omega_3 \\end{array}\\right),\\qquad \\left(\\begin{array}{rrrr|r} 1 &amp; 3 &amp; 0 &amp; 0 &amp; \\omega_1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\omega_3 \\end{array}\\right). \\] Note that the reduced echelon form resuts in a “staircase” of \\(1\\)’s appearing either on or above the diagonal, with all entries below the staircase being zero. We can only take one step down any column, but might take multiple steps right along a row. So far, we have only seen examples like the first matrix above, where we have a diagonal of \\(1\\)’s. Since in this case we simply read off the solutions, it is easy to see that such forms result in a unique solution. We now give some examples and interpretations for other echelon forms. In the following \\(\\alpha_i\\) and \\(\\omega_i\\) can be any value. Example 7.3 (Zero coefficient row) \\[ \\left(\\begin{array}{rcr|r} 1 &amp; 0 &amp; \\alpha &amp; \\omega_1 \\\\ 0 &amp; 1 &amp; 0 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 0 &amp; \\omega_3 \\end{array}\\right),\\qquad \\] If \\(\\omega_3=0\\) the equations read \\[\\begin{align*} x_1+\\alpha x_3&amp;=\\omega_1\\\\ x_2&amp;=\\omega_2\\\\ 0&amp;=0 \\end{align*}\\] In effect, we only have two equations. There is no constraint on \\(x_3\\) and we call this a free variable. We can parameterise the solution by setting \\(x_3=\\beta\\) where \\(\\beta\\) can take any values – there are infinitely many solutions. So the solution is all points \\[(\\omega_1-\\alpha\\beta,\\omega_2,\\beta).\\] Geometrically, this is a line in 3-dimensional Cartesian coordinates which is drawn by varying \\(\\beta\\). On the other hand, if \\(\\omega_3\\neq 0\\), then in the bottom row we have the equation \\[0=\\omega_3\\] which is not possible. In this case, there are no solutions to the system of equations. We sometimes say that the set of equations is inconsistent. Example 7.4 (A rectangular reduced echelon form) \\[ \\left(\\begin{array}{rrrrr|r} 1 &amp; 0 &amp; \\alpha_1 &amp; 0 &amp; \\alpha_2 &amp; \\omega_1 \\\\ 0 &amp; 1 &amp; \\alpha_3 &amp; 0 &amp; \\alpha_4 &amp; \\omega_2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\alpha_5 &amp; \\omega_3 \\end{array}\\right). \\] In this case, we have the solutions \\[\\begin{align*} x_1 &amp; = \\omega_1 - \\alpha_1x_3 - \\alpha_2x_5\\\\ x_2 &amp; = \\omega_2 - \\alpha_3x_3 -\\alpha_4 x_5\\\\ x_4 &amp; = \\omega_3 - \\alpha_5x_5 \\end{align*}\\] where \\(x_3\\) and \\(x_5\\) are free variables, and hence we can set them as parameterise \\(x_3=\\beta\\), \\(x_4=\\gamma\\). More generally, any column that does not have a leading entry will correspond to a free variable. Since we have 2 parameters, this describes a plane in 5-dimensional space. The solution is all points \\[(\\omega_1 - \\alpha_1\\beta - \\alpha_2\\gamma, \\omega_2 - \\alpha_3\\beta -\\alpha_4\\gamma, \\beta, \\omega_3 - \\alpha_5\\gamma, \\gamma).\\] The method is named after Carl Friedrich Gauss (1777–1855) born in Braunschweig in the Holy Roman Empire, in what is now Germany. Gauss worked at the University of Gottingen.↩︎ "],["matrices.html", "Chapter 8 Matrices 8.1 Solving linear systems revisited 8.2 Determinants", " Chapter 8 Matrices A matrix \\(A\\) is a two-dimensional array of numbers. We say that \\(A\\) is an \\(m \\times n\\) matrix if it has \\(m\\) rows and \\(n\\) columns, for some natural numbers \\(m\\) and \\(n\\). We use the notation \\[ A = (a_{ij})_{m \\times n}, \\qquad \\text{or more simply} \\qquad A = (a_{ij}), \\] to indicate that the entry in row \\(i\\) and column \\(j\\) of \\(A\\) is \\(a_{ij}\\). We refer to \\(a_{ij}\\) as the \\(i\\)–\\(j\\) entry of the matrix \\(A\\). 8.1 Solving linear systems revisited In the previous chapter, given a set of \\(m\\) linear equations with \\(n\\) variables \\[\\begin{align*} a_{11}x_1+a_{12}x_2+\\dotsb+a_{1n}x_n&amp;=b_1\\\\ a_{21}x_1+a_{22}x_2+\\dotsb+a_{2n}x_n&amp;=b_2\\\\ \\vdots\\qquad&amp;=\\,\\vdots\\\\ a_{m1}x_1+a_{m2}x_2+\\dotsb+a_{mn}x_n&amp;=b_m \\end{align*}\\] we introduced the coefficient matrix \\[ A= \\begin{pmatrix} a_{11}&amp;a_{12}&amp;\\dotsb&amp;a_{1n}\\\\ a_{21}&amp;a_{22}&amp;\\dotsb&amp;a_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\dotsb&amp;\\vdots\\\\ a_{m1}&amp;a_{n2}&amp;\\dotsb&amp;a_{mn} \\end{pmatrix}. \\] We will now write the variables and r.h.s. as vectors of length \\(n\\) and \\(m\\) respectively \\[ \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\qquad\\text{and}\\qquad \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix}. \\] Next, we define the operation of multiplying a vector \\(\\mathbf{x}\\) of length \\(n\\) by an \\(m\\times n\\) matrix \\(A\\). The result is a vector of length \\(m\\) whose first entry is the sum of the element-wise products of the first row of \\(A\\) with the vector \\(\\mathbf{x}\\), the second entry is the sum of the element-wise products of the second row of \\(A\\) with the vector \\(\\mathbf{x}\\) and so on: \\[ A\\mathbf{x}=\\begin{pmatrix} a_{11}&amp;a_{12}&amp;\\dotsb&amp;a_{1n}\\\\ a_{21}&amp;a_{22}&amp;\\dotsb&amp;a_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\dotsb&amp;\\vdots\\\\ a_{n1}&amp;a_{n2}&amp;\\dotsb&amp;a_{nn} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} = \\begin{pmatrix} a_{11}x_1+a_{12}x_2+\\dotsb+a_{1n}x_n\\\\ a_{21}x_1+a_{22}x_2+\\dotsb+a_{2n}x_n\\\\ \\vdots\\\\ a_{m1}x_1+a_{m2}x_2+\\dotsb+a_{mn}x_n \\end{pmatrix} \\] Hence we can express the system of equations more compactly as \\[A\\mathbf{x}=\\mathbf{b}\\] where each row of the expanded equation corresponds to one of the linear equations. In the previous chapter we saw that solving such a system corresponds to finding the intersection points of the hyperplanes defined by these rows. We will call this the row picture of linear equations. However, there is another way to interpret \\(A\\mathbf{x}=\\mathbf{b}\\) in terms of column vectors. We must first introduce some new definitions. Definition 8.1 (Linear combinations) Consider a list of vectors \\[\\mathbf{v}_1, \\mathbf{v}_2,\\dotsc,\\mathbf{v}_n.\\] A linear combination of a list of vectors is a sum of the form \\[ \\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2 + \\dotsb + \\alpha_n\\mathbf{v}_n \\] for some scalar coefficients \\(\\alpha_i\\). A list of vectors is called linearly independent if we cannot write one of the vectors as a linear combination of the others, such as \\[ \\mathbf{v}_i=\\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2+\\dotsb+\\alpha_{i-1}\\mathbf{v}_{i-1}+\\alpha_{i+1}\\mathbf{v}_{i+1}+\\dotsb+\\alpha_n\\mathbf{v}_n \\] (none of the vectors depends on the others). The span of a list of vectors is all of the possible linear combinations. If we can reach any vector in the space by taking some linear combination then we say the vectors span the space. An intuitive way to think about linearly independent vectors is that they all point in “independent” directions. An intuitive way to think about the span is as all the points we can reach in the space by only walking in the directions of the list of vectors. Figure 8.1 demonstrates the concepts of linear combinations and span in 2-dimensions. Figure 8.1: Any vector \\(\\mathbf{u}\\) in the plane can be written as a linear combination of the vectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\), that is \\(\\mathbf{u}=\\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2\\). Drag the vector \\(\\mathbf{u}\\) to any point in the plane to see this decomposition. This means that \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) span the entire plane. You can also drag the vectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) and so long as these point in “independent directions” they still span the plane; if they lie along the same direction, then they only span a line.[Open plot in browser.] If we consider each column of the matrix \\(A\\) as a vector \\(\\mathbf{a}_i\\) of length \\(m\\), then the multiplication \\(A\\mathbf{x}\\) can also be written as \\[\\begin{align*} A\\mathbf{x}&amp;=x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\dotsb + x_n\\mathbf{a}_n\\\\ &amp;=x_1 \\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{pmatrix} + x_2 \\begin{pmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{pmatrix} + \\dotsb + x_n \\begin{pmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} a_{11}x_1+a_{12}x_2+\\dotsb+a_{1n}x_n\\\\ a_{21}x_1+a_{22}x_2+\\dotsb+a_{2n}x_n\\\\ \\vdots\\\\ a_{m1}x_1+a_{m2}x_2+\\dotsb+a_{mn}x_n \\end{pmatrix}. \\end{align*}\\] That is, \\(A\\mathbf{x}\\) is the linear combination of the columns \\(\\mathbf{a}_i\\) with coefficients \\(x_i\\). We will call this the column picture of matrix equations. As we shall come to see, this different point of view is very powerful in understanding the nature of linear equations. Finding a solution to \\(A\\mathbf{x}=\\mathbf{b}\\) can now be interpreted as answering the question: can we find a linear combination of the column vectors of \\(A\\) that sum to give the vector \\(\\mathbf{b}\\)? Or in other words: is the vector \\(\\mathbf{b}\\) in the span of the columns of \\(A\\)? The question of unique solutions or infinitely many solutions becomes: is there one way or infinitely many ways to take a linear combination of the columns of \\(A\\) to reach the vector \\(\\mathbf{b}\\)? Or in other words: are the columns of \\(A\\) linearly independent? These questions can once again be answered from inspecting the REF of the augmented matrix. In the statements below we shall refer to the coefficient matrix part of the REF as the l.h.s. and the final augmented column as the r.h.s. If there is a row of zeros in the l.h.s. but a non-zero value in the corresponding row of the r.h.s., then there are no solutions (\\(\\mathbf{b}\\) is not in the span of the columns of \\(A\\)). If all zero rows in the l.h.s. have a zero in the corresponding row of the r.h.s, together with every column in the l.h.s. containing a leading entry, then there is a unique solution (\\(\\mathbf{b}\\) is in the span of the columns of \\(A\\) together with the columns being linearly independent). If all zero rows in the l.h.s. have a zero in the corresponding row of the r.h.s, but not every column in the l.h.s. contains a leading entry, then there are infinitely many solutions (\\(\\mathbf{b}\\) is in the span of the columns of \\(A\\) but the columns are linearly dependent). A list of vectors that both span the space and are linearly independent are called a basis. A basis can be used as a coordinate system. We generally use the standard basis consisting of the unit vectors \\(\\mathbf{i}, \\mathbf{j}\\) and \\(\\mathbf{k}\\). The coordinates of a vector \\(\\mathbf{v}\\) in this basis are the values \\(\\alpha_i\\) in the linear combination \\(\\mathbf{v}=\\alpha_1\\mathbf{i} + \\alpha_2\\mathbf{j} + \\alpha_3\\mathbf{k}\\). If we had a different basis \\(\\mathbf{p}, \\mathbf{q}, \\mathbf{r}\\) then we could also express \\(\\mathbf{v}\\) as \\(\\mathbf{v}=\\beta_1\\mathbf{p}+\\beta_2\\mathbf{q}+\\beta_3\\mathbf{r}\\) and specify points using \\(\\beta_1,\\beta_2\\) and \\(\\beta_3\\) as coordinates. 8.2 Determinants When we have a square coefficient matrix \\(A\\), we can find out if there exists a unique solution to \\(A\\mathbf{x}=\\mathbf{b}\\) for any possible vector \\(\\mathbf{b}\\) by computing a scalar quantity associated to \\(A\\) known as the determinant. For a \\(2\\times 2\\) matrix \\[ A = \\begin{pmatrix} a &amp; b\\\\ c &amp; d\\end{pmatrix} \\] we define the determinant of \\(A\\), written as \\(\\det(A)\\) or \\(|A|\\), as \\[ \\det(A) = \\begin{vmatrix} a &amp; b\\\\ c &amp; d\\end{vmatrix} = ad - bc. \\] Example 8.1 (2x2 determinants) For instance, we have \\[\\begin{align*} \\begin{vmatrix} 1 &amp; 5\\\\ 6 &amp; 2\\end{vmatrix} &amp;= 1\\times 2 - 5\\times 6 = -28,\\\\ \\begin{vmatrix} 2 &amp; -5\\\\ 7 &amp; 3\\end{vmatrix} &amp;= 2\\times 3 - (-5\\times 7) = 41,\\\\ \\begin{vmatrix} a &amp; \\frac{b}{2}\\\\ \\frac{b}{2} &amp; c\\end{vmatrix} &amp; = ac - \\frac{b^2}{4} = -\\frac{1}{4}(b^2 - 4ac). \\end{align*}\\] Higher-order determinants are defined recursively in terms of lower-order determinants. For instance, for a \\(3\\times 3\\) matrix, we define \\[ \\begin{vmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i\\end{vmatrix} = a\\begin{vmatrix} e &amp; f\\\\ h &amp; i\\end{vmatrix} - b\\begin{vmatrix} d &amp; f\\\\ g &amp; i\\end{vmatrix} + c\\begin{vmatrix} d &amp; e\\\\ g &amp; h\\end{vmatrix}. \\] Note that the expression above contains three terms, each of which is the product of two factors: an entry of the matrix multiplied by a \\(2\\times 2\\) determinant. Each \\(2\\times 2\\) determinant is formed by crossing out the row and column containing the other factor. Also note the \\(-\\) sign in the second term. Example 8.2 (A 3x3 determinant) Compute the determinant of the matrix \\(A = \\left(\\begin{smallmatrix} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6\\\\ 7 &amp; 8 &amp; 9\\end{smallmatrix}\\right)\\). \\[\\begin{align*} \\det(A) = \\begin{vmatrix} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6\\\\ 7 &amp; 8 &amp; 9\\end{vmatrix} &amp;= 1\\begin{vmatrix} 5 &amp; 6\\\\ 8 &amp; 9\\end{vmatrix} - 2\\begin{vmatrix} 4 &amp; 6\\\\ 7 &amp; 9\\end{vmatrix} + 3\\begin{vmatrix} 4 &amp; 5\\\\ 7 &amp; 8\\end{vmatrix} \\\\ &amp;= 1\\left(5\\times 9-6\\times 8\\right) - 2\\left(4\\times 9 - 6\\times 7\\right) + 3(4\\times 8 - 5\\times 7)\\\\ &amp;= -3 - 2(-6)+3(-3)\\\\ &amp;=0 \\end{align*}\\] Theorem 8.1 (Unique solutions) Let \\(A\\) be a square matrix. There is a unique solution to \\(A\\mathbf{x}=\\mathbf{b}\\) for any vector \\(\\mathbf{b}\\) if and only if \\(\\det(A)\\neq 0\\). This gives us a useful test if we want to know if there are unique solutions for any vector \\(\\mathbf{b}\\). Note however that even if \\(\\det(A)=0\\) there could be solutions for a particular vector \\(\\mathbf{b}\\) – we would have to perform Guassian elimination to check. "],["linear-transformations.html", "Chapter 9 Linear Transformations 9.1 Matrix Algebra 9.2 Matrix inverse", " Chapter 9 Linear Transformations We have seen that we can formulate a system of linear equations as a matrix-vector equation \\[A\\mathbf{x}=\\mathbf{b}\\] and viewed this in two ways: as a compact way of expressing the system of linear equations: multiplying the vector \\(\\mathbf{x}\\) by the matrix \\(A\\), each row in the resulting vector equation is one of the linear equations as a vector equation with the l.h.s. being a linear combination of the columns of \\(A\\) We now introduce a third point of view: Multiplication by the matrix \\(A\\) acts as a function, taking the (vector) input \\(\\mathbf{x}\\) and returning the (vector) output \\(\\mathbf{b}\\). As a function, the matrix \\(A\\) represents a Linear Transformation. This is a type of function \\(f\\) whose inputs and outputs are vectors, with the following properties: \\(f(\\mathbf{x}_1+\\mathbf{x}_2)=f(\\mathbf{x}_1)+f(\\mathbf{x}_2)\\) for any two vectors \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) \\(f(\\lambda\\mathbf{x})=\\lambda f(\\mathbf{x})\\) for any scalar \\(\\lambda\\) and vector \\(\\mathbf{x}\\) Translating this to matrices: \\(A(\\mathbf{x}_1+\\mathbf{x}_2)=A\\mathbf{x}_1+A\\mathbf{x}_2\\) for any two vectors \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) \\(A(\\lambda\\mathbf{x})=\\lambda A\\mathbf{x}\\) for any scalar \\(\\lambda\\) and vector \\(\\mathbf{x}\\) Exercise: convice yourself that these properties hold by evaluating the left and right hand sides for a general \\(3\\times 3\\) matrix and confirming they are equal. Example 9.1 (Rotation about the origin) The following matrix rotates a vector by an angle \\(\\theta\\) anticlockwise about the origin. \\[R_\\theta=\\begin{pmatrix} \\cos(\\theta)&amp; -\\sin(\\theta)\\\\ \\sin(\\theta)&amp; \\cos(\\theta) \\end{pmatrix} \\] Figure 9.1: The action of \\(R_\\theta\\) on a vector \\(\\mathbf{v}\\) We can understand that this is a linear transformation geometrically, since: adding two vectors and then rotating is the same as rotating two vectors and then adding; taking a scalar multiple of a vector and then rotating is the same as rotating and then scaling. It turns out that all linear transformations can (in a certain sense) be decomposed as a rotation or reflection, followed by a coordinate scaling, followed by another rotation or reflection, as explained by the Singular Value Decomposition theorem. Of course not all functions have these special properties. Exercise: convince yourself that \\(f(x)=x^2\\) is not a linear transformation by finding values \\(x\\) and \\(y\\) (real numbers are 1-dimensional vectors) and a scalar \\(\\lambda\\) for which the above properties I. and II. do not hold. Now, if we were trying to solve an equation involving a function, say \\(f(x)=b\\), we would apply the inverse \\(f^{-1}\\) to both sides to get \\[\\begin{align*} f(x)&amp;=b\\\\ f^{-1}(f(x))&amp;=f^{-1}(b)\\\\ x&amp;=f^{-1}(b). \\end{align*}\\] Note this only works if \\(f^{-1}\\) exists – we know that not all functions have inverses10. We will soon show a condition for the existence of the inverse of a matrix \\(A\\), which we denote by \\(A^{-1}\\) and that is itself another matrix. We will then have another way to solve linear equations by applying the inverse matrix: \\[\\begin{align*} A\\mathbf{x}&amp;=\\mathbf{b}\\\\ A^{-1}(A\\mathbf{x})&amp;=A^{-1}\\mathbf{b}\\\\ \\mathbf{x}&amp;=A^{-1}\\mathbf{b}. \\end{align*}\\] First, we look at the algebra of matrices. 9.1 Matrix Algebra 9.1.1 Addition and Subtraction We can add and subtract two matrices, so long as the have the same dimensions \\(m\\times n\\). Then the sum and difference of two matrices \\(A\\) and \\(B\\) are \\[A+B=(a_{ij}+b_{ij}),\\qquad A-B=(a_{ij}-b_{ij}),\\] that is, we simply add or subtract the corresponding entries. Example 9.2 (Matrix Addition) Define the matrices \\(A\\) and \\(B\\) as \\[ A = \\begin{pmatrix} 2 &amp; 9 &amp; 6 \\\\ -1 &amp; 3 &amp; 5 \\end{pmatrix} \\qquad\\text{and}\\qquad B = \\begin{pmatrix} 0 &amp; 3 &amp; -1 \\\\ 2 &amp; 7 &amp; 8 \\end{pmatrix}. \\] Because the both matrices have dimensions \\(2 \\times 3\\), we may calculate \\[ A + B = \\begin{pmatrix} 2 &amp; 12 &amp; 5 \\\\ 1 &amp; 10 &amp; 13 \\end{pmatrix} \\qquad\\text{and}\\qquad A - B = \\begin{pmatrix} 2 &amp; 6 &amp; 7 \\\\ -3 &amp; -4 &amp; -3 \\end{pmatrix}. \\] Note, that matrix addition is commutative11, that is \\[ A + B = B + A. \\] 9.1.2 Multiplication by a scalar Let \\(A = (a_{ij})_{m\\times n}\\) be an \\(m\\times n\\) matrix and let \\(\\lambda\\) be a scalar. We define the matrix \\(\\lambda A\\) by \\[ (\\lambda A)_{ij} = \\lambda a_{ij}, \\] that is, we multiply every entry of \\(A\\) by \\(\\lambda\\). Example 9.3 (Scalar Multiplication) With \\(A\\) as in Example @ref(exm:mat_add), \\(\\lambda = 2\\) and \\(\\mu = -1\\), we have \\[ \\lambda A = 2A = \\begin{pmatrix} 4 &amp; 18 &amp; 12 \\\\ -2 &amp; 6 &amp; 10 \\end{pmatrix} \\qquad\\text{and}\\qquad \\mu A = -A = \\begin{pmatrix} -2 &amp;-9 &amp;-6 \\\\ 1 &amp; -3 &amp; -5\\end{pmatrix} \\] 9.1.3 Matrix multiplication We saw in section 8.1 how to multiply a vector by a matrix. This idea extends naturally to multiplying two matrices together as follows. Let \\(A = (a_{ij})_{m\\times l}\\) and \\(B = (b_{ij})_{l \\times n}\\). Then the matrix product \\(C = AB\\) is defined to be the matrix \\(AB = (c_{ij})_{m\\times n}\\) such that \\(c_{ij}\\) is the dot product of row \\(i\\) of \\(A\\) with column \\(j\\) of \\(B\\). That is, for all \\(i =1,\\dotsc, m\\) and all \\(j =1, \\dotsc, n\\), \\[ c_{ij} = a_{i1} b_{1j} + a_{i2} b_{2j} + \\dots + a_{il} b_{lj} \\] Note that the product \\(AB\\) is defined only if the number of columns of \\(A\\) is the equal to the number of rows of \\(B\\). The product matrix has the same number of rows as \\(A\\) and the same number of columns as \\(B\\). Example 9.4 (Matrix Multiplication) Let \\[ A = \\begin{pmatrix} 3 &amp; 2 &amp; -1\\\\ 0 &amp; 5 &amp; 4 \\end{pmatrix} \\qquad\\text{and}\\qquad B = \\begin{pmatrix} 1 &amp; 2 &amp; 4 &amp; 6\\\\ 5 &amp; 3 &amp; 0 &amp; 7\\\\ -1 &amp; -2 &amp; 1 &amp; 5 \\end{pmatrix}. \\] Then \\[ AB = \\begin{pmatrix} 3 &amp; 2 &amp; -1\\\\ 0 &amp; 5 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 2 &amp; 4 &amp; 6\\\\ 5 &amp; 3 &amp; 0 &amp; 7\\\\ -1 &amp; -2 &amp; 1 &amp; 5 \\end{pmatrix} = \\begin{pmatrix} 14 &amp; 14 &amp; 11 &amp; 27\\\\ 21 &amp; 7 &amp; 4 &amp; 55 \\end{pmatrix}. \\] Note that \\(A\\) is \\(2\\times3\\), that \\(B\\) is \\(3\\times 4\\) and that the product \\(AB\\) is a \\(2\\times 4\\) matrix. We have skipped the intermediate steps of multiplying all rows of \\(A\\) with all columns of \\(B\\), but to give an example, the \\(1\\)–\\(3\\) entry of \\(AB\\) is 11, which must be the dot product of the \\(1^\\text{st}\\) row of \\(A\\) and the \\(3^\\text{rd}\\) column of \\(B\\): \\[ \\begin{pmatrix} 3 &amp; 2 &amp; -1\\end{pmatrix} \\cdot \\begin{pmatrix} 4\\\\ 0\\\\ 1\\end{pmatrix} = 3 \\times 4 + 2 \\times 0 - 1\\times 1 = 11. \\] Unlike with ordinary (scalar) numbers, we do not normally have \\(AB = BA\\), that is Matrix multiplication is not commutative. Indeed, in the above example, \\(BA\\) is not even defined. The reason is that \\(B\\) has \\(4\\) columns, but \\(A\\) has \\(2\\) rows. 9.1.4 Functional interpretations Here we understand what the algebraic operations above mean when we are considering a matrix as a function. We let \\(A\\) and \\(B\\) be compatible matrices, \\(\\mathbf{x}\\) a vector and for comparison to other (non-linear) functions we take the examples \\(f(x)=x^2\\) and \\(g(x)=x^3\\). Addition/Subtraction \\[(A+B)\\mathbf{x}=A\\mathbf{x}+B\\mathbf{x}\\] This agrees with the usual definition of the sum of two functions, for example we have \\((f+g)(x)=f(x)+g(x)=x^2+x^3\\). Scalar multiplication \\[(\\lambda A)\\mathbf{x}=\\lambda (A \\mathbf{x})\\] Again this agrees with the usual definition for functions, for example \\((\\lambda f)(x)=\\lambda f(x)=\\lambda x^2\\). Matrix Multiplication \\[(AB)\\mathbf{x}=A(B\\mathbf{x})\\] Matrix multiplication corresponds to function composition, that is, first apply matrix \\(B\\) then apply matrix \\(A\\). In our example for non-linear functions, \\((f\\circ g)(x)=f(g(x))=f(x^3)=(x^3)^2=x^6\\) (where \\(\\circ\\) denotes function composition). When we have repeated multiplication of a matrix with itself \\(n\\) times we use the power notation: \\[A\\dotsb A=A^n\\] for example, \\(AA=A^2\\). 9.1.5 Two special matrices The \\(m\\times n\\) zero matrix \\(0_{m\\times n}\\) is defined as the \\(m\\times n\\) matrix which has only zero entries. Example 9.5 (Zero Matrix) For \\(m = 2\\) and \\(n = 3\\), we have \\[ 0_{m\\times n} = 0_{2\\times 3} = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] and (in contrast) \\[ 0_{n\\times m} = 0_{3\\times 2} = \\begin{pmatrix} 0 &amp; 0\\\\ 0 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}. \\] Note that for any vector \\(\\mathbf{x}\\) of length \\(n\\), we have \\(0_{m\\times n} \\mathbf{x}=\\mathbf{0}_m\\). The \\(n\\times n\\) identity matrix \\(I_n\\), is defined by \\(I_n = (\\delta_{ij})_{n\\times n}\\), where \\(\\delta_{ij}\\) is given as \\[\\delta_{ij} = \\begin{cases} 1 &amp; \\text{if $i = j$,} \\\\ 0 &amp; \\text{if $i \\neq j$.} \\end{cases} \\] The function (or sometimes referred to as a “symbol”) \\(\\delta\\) of the two variables \\(i\\) and \\(j\\) is called the Kronecker delta and pops up often in pure and applied mathematics. Example 9.6 (Identity Matrix) For \\(n = 3\\), we have \\[ I_3 = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}. \\] Note that for any vector \\(\\mathbf{x}\\) of length \\(n\\), we have \\[I_n\\mathbf{x}=\\mathbf{x}.\\] 9.1.6 Some further properties Assuming the matrices \\(A, B, C\\) all have compatible dimensions and with \\(\\lambda, \\mu\\) being scalars, we have: \\(\\lambda(A+B)=\\lambda A + \\lambda B\\) \\((\\lambda + \\mu)A=\\lambda A +\\mu A\\) \\(I_m A = A\\) and \\(A I_n=A\\) \\(0_{l\\times m}A=0_{l\\times n}\\) and \\(A0_{n\\times p}=0_{m\\times p}\\) \\((AB)C=A(BC)\\) \\(A(B+C)=AB+AC\\) \\((A+B)C=AC+BC\\) Exercise: pick some matrices and check that these hold. 9.2 Matrix inverse We are now ready to discuss the matrix inverse. By definition, the inverse \\(f^{-1}\\) of a function \\(f\\) is the function that satisfies \\[f^{-1}(f(x))=x\\quad\\text{and}\\quad f(f^{-1}(y))=y\\] for all input values \\(x\\) and output values \\(y\\) (the function and its inverse “reverse” the action of one another.) For a matrix, function composition is given by matrix mutiplication, so the definition of the inverse matrix \\(A^{-1}\\) of a matrix \\(A\\) requires \\[A^{-1}A\\mathbf{x}=\\mathbf{x}\\quad\\text{and}\\quad AA^{-1}\\mathbf{y}=\\mathbf{y}\\] for all input vectors \\(\\mathbf{x}\\) and output vectors \\(\\mathbf{y}\\). It turns out12 this will only work for square matrices, that is, a matrix \\(A\\) with same number of rows and columns: an \\(n\\times n\\) matrix. This immediately implies that \\(A^{-1}\\) is also an \\(n\\times n\\) matrix. Now another way to express the matrix inverse is via the statement: \\[A^{-1}A=I_n=AA^{-1}\\] since applying \\(A\\) and its inverse \\(A^{-1}\\) has the same effect as applying the identity matrix. Exercise: Show that the matrix \\[ B = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 2 &amp; 5 \\\\ 0 &amp; 1 &amp; 3 \\end{pmatrix} \\] is the inverse of the matrix \\[ A = \\begin{pmatrix} 1 &amp; 1 &amp; -2 \\\\ 0 &amp; 3 &amp; -5 \\\\ 0 &amp; -1 &amp; 2 \\end{pmatrix}. \\] by computing \\(AB\\) and \\(BA\\) and observing that \\(AB = BA = I_3\\). So \\(A\\) is invertible and \\(B=A^{-1}\\) is the inverse. It is important to note that not all (square) matrices are invertible. Example 9.7 (No inverse) Consider the matrix \\[ A = \\begin{pmatrix} 1 &amp; -1 \\\\ -1 &amp; 1 \\end{pmatrix}. \\] Assume that there exist \\(a,b,c,d\\), such that \\[ B = \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} \\] is an inverse of \\(A\\). For this, we require that \\[ \\begin{pmatrix} 1 &amp; -1 \\\\ -1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}. \\] That is, we require that \\[ \\begin{pmatrix} a - c &amp; b - d \\\\ -a + c &amp; -b + d \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}. \\] Comparing the entries in the left-most column of the matrices on the left- and right-hand sides of the above matrix equation, we deduce that, both \\[ a - c = 1 \\qquad\\text{and}\\qquad -a + c = 0, \\] which, adding both equations, gives \\(0 = 1\\), which is not true. Thus, assuming that \\(A\\) has an inverse leads to a contradiction, so there cannot be an inverse of \\(A\\) 9.2.1 Finding the inverse of a \\(2\\times 2\\) matrix We start by presenting the formula for the inverse of a \\(2\\times 2\\) matrix, which is relatively straightforward. Then we will discuss how to find the inverse of a general \\(n\\times n\\) matrix, which is more involved, in the next section. Theorem 9.1 (Inverse of a 2 x 2 matrix) Let \\(A\\) be the \\(2\\times 2\\) matrix \\[A=\\begin{pmatrix} a&amp;b\\\\ c&amp;d \\end{pmatrix}\\] with \\(\\det(A)=ad-bc\\neq 0\\). Then \\(A\\) is invertible, with inverse \\[A^{-1}=\\frac{1}{\\det(A)}\\begin{pmatrix} d&amp;-b\\\\ -c&amp;a \\end{pmatrix}.\\] Note that the formula only makes sense if \\(\\det(A)\\neq 0\\). It turns out that the condition \\(\\det(A)\\neq 0\\) is precisely the condition that must be satisfied for invertibility (for any size matrix). Theorem 9.2 (Existence of matrix inverse) An \\(n\\times n\\) matrix \\(A\\) is invertible if and only if \\(\\det(A)\\neq 0\\). If \\(A\\) is not invertible, then we say that \\(A\\) is singular. This is analagous to not being able to divide by zero in the one-dimensional case: we cannot solve \\(ax=b\\) if \\(a=0\\). Example 9.8 (2 x 2 inverse examples) Let \\[A = \\begin{pmatrix} 1 &amp; 5 \\\\ 2 &amp; 10 \\end{pmatrix}.\\] The determinant of \\(A\\) is \\(\\det(A) = 1\\times 10 - 5\\times 2 = 0\\). Thus \\(A\\) is singular. Let \\[M = \\begin{pmatrix} 7 &amp; 3 \\\\ 2 &amp; 3 \\end{pmatrix}.\\] We have \\(\\det(M) = 15\\), hence, \\(M\\) is invertible and its inverse is \\[M^{-1} = \\frac{1}{15} \\begin{pmatrix} 3 &amp; -3 \\\\ -2 &amp; 7 \\end{pmatrix}.\\] 9.2.2 Finding the inverse of an \\(n\\times n\\) matrix There does exist a formula that is a generalisation of that for \\(2\\times 2\\) matrices, based on Cramer’s Rule. This is important in the theory of matrices, but not very useful in practice for large matrices. Hence we present a different way of computing the inverse, via Guassian elimination. For the moment, denote \\(X=A^{-1}\\). We are looking for \\(X\\) such that \\(AX=I_n\\) and \\(XA=I_n\\). It turns out that if \\(X\\) satisfies either one of these equations then it is the unique inverse, so we shall just consider \\(AX=I_n\\). By the rules of matrix multiplication, the first column of \\(I_n\\) is the result of taking the dot product of the rows of \\(A\\) with the first column of \\(X\\). Denoting the \\(i^\\text{th}\\) row of \\(A\\) by \\(\\mathbf{a}_i\\) and the \\(j^\\text{th}\\) column of \\(X\\) by \\(\\mathbf{x}_j\\), we have \\[ A\\mathbf{x}_1= \\begin{pmatrix} \\mathbf{a}_1\\cdot \\mathbf{x}_1\\\\ \\mathbf{a}_2\\cdot \\mathbf{x}_1\\\\ \\vdots\\\\ \\mathbf{a}_n\\cdot \\mathbf{x}_1\\ \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0\\\\ \\vdots\\\\ 0 \\end{pmatrix}. \\] More generally, \\[ A\\mathbf{x}_j= \\begin{pmatrix} \\mathbf{a}_1\\cdot \\mathbf{x}_j\\\\ \\mathbf{a}_2\\cdot \\mathbf{x}_j\\\\ \\vdots\\\\ \\mathbf{a}_n\\cdot \\mathbf{x}_j\\ \\end{pmatrix} = \\begin{pmatrix} \\delta_{1j}\\\\ \\delta_{2j}\\\\ \\vdots\\\\ \\delta_{nj} \\end{pmatrix}. \\] These are just standard matrix–vector equations of the form “\\(A\\mathbf{x}=\\mathbf{b}\\)”, which we can solve using Guassian elimination to find the columns \\(\\mathbf{x}_j\\). Moreover, instead of doing this one column at a time, we can solve for all columns at once, as in the following example. Example 9.9 (Inverse via Gaussian elimination) Consider the matrix \\[ A = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 2 &amp; 5 \\\\ 0 &amp; 1 &amp; 3 \\end{pmatrix}. \\] We wish to find the inverse of \\(A\\). That is, we wish to find a \\(3\\times 3\\) matrix \\(X\\) such that \\(AX = I_3\\). Writing \\(\\mathbf{x}_1, \\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) for the columns of \\(X\\), this is equivalent to solving the \\(3\\) matrix–vector equations \\[ A\\mathbf{x}_1 = \\mathbf{e}_1, \\qquad A\\mathbf{x}_2 = \\mathbf{e}_2 \\qquad\\text{and}\\qquad A\\mathbf{x}_3 = \\mathbf{e}_3 \\] where \\[\\mathbf{e}_j= \\begin{pmatrix} \\delta_{1j}\\\\ \\delta_{2j}\\\\ \\delta_{3j}\\\\ \\end{pmatrix}. \\] Note \\(\\mathbf{e}_1=\\mathbf{i}, \\mathbf{e}_2=\\mathbf{j}, \\mathbf{e}_3=\\mathbf{k}\\), the newly introduced indexed notation \\(\\mathbf{e}_i\\) is just more convenient for extending to arbitrary dimensions. We could set this up for Gaussian elimination via the \\(3\\) augmented matrices \\[ \\left(A\\ |\\ \\mathbf{e}_1\\right), \\qquad \\left(A\\ |\\ \\mathbf{e}_2\\right) \\qquad\\text{and}\\qquad \\left(A\\ |\\ \\mathbf{e}_3\\right) \\] and then perform EROs to transform the above augmented matrices to the form \\[ \\left(I_3\\ | \\mathbf{x}_1\\right), \\qquad \\left(I_3\\ |\\ \\mathbf{x}_2\\right) \\qquad\\text{and}\\qquad \\left(I_3\\ |\\ \\mathbf{x}_3\\right), \\] allowing us to read off the solutions \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\). However, these will all require the same EROs, and so we can compute all three vectors \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) at the same time by starting with the following augmented matrix \\[ \\left(A\\ |\\ \\mathbf{e}_1 \\ \\mathbf{e}_2 \\ \\mathbf{e}_3\\right), \\] and then perform EROs to bring it into the reduced echelon form \\[ \\left(I_3\\ |\\ \\mathbf{x}_1\\ \\mathbf{x}_2\\ \\mathbf{x}_3\\right). \\] For the above matrix \\(A\\), the augmented matrix \\((A\\ |\\ I_3)\\) is given by \\[ \\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 5 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right). \\] First, we swap rows \\(2\\) and \\(3\\), which gives \\[ \\begin{array}{l} \\quad\\\\ \\quad\\\\ R_2 \\leftrightarrow R_3\\colon \\quad \\end{array} \\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 2 &amp; 5 &amp; 0 &amp; 1 &amp; 0 \\end{array}\\right). \\] As there are only zeros below the leading entry in row \\(1\\), we proceed to eliminate the entry under the leading entry in row \\(2\\), that is \\[ \\begin{array}{l} \\quad\\\\ \\quad\\\\ R_3 \\to R_3 - 2R_2\\colon\\quad \\end{array} \\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; -2 \\end{array}\\right). \\] Next, we eliminate the entries above the leading entry in row \\(3\\), which yields \\[ \\begin{array}{l} R_1 \\to R_1 + R_3\\colon\\quad \\\\ R_2 \\to R_2 + 3R_2\\colon\\quad \\\\ \\quad \\end{array} \\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -2 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 3 &amp; -5 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; -2 \\end{array}\\right). \\] Finally, we multiply row \\(3\\) by \\(-1\\) to arrive at \\[ \\begin{array}{l} \\quad \\\\ \\quad \\\\ R_3 \\to -R_3\\colon\\quad \\end{array} \\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -2 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 3 &amp; -5 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 2 \\end{array}\\right). \\] We read off the solutions \\[ \\mathbf{x}_1 = \\begin{pmatrix} 1\\\\ 0\\\\ 0\\end{pmatrix}, \\qquad \\mathbf{x}_2 = \\begin{pmatrix} 1\\\\ 3\\\\ -1\\end{pmatrix} \\qquad\\text{and}\\qquad \\mathbf{x}_3 = \\begin{pmatrix} -2\\\\ -5\\\\ 2\\end{pmatrix}. \\] This corresponds to \\[ X = \\begin{pmatrix} 1 &amp; 1 &amp; -2 \\\\ 0 &amp; 3 &amp; -5 \\\\ 0 &amp; -1 &amp; 2\\end{pmatrix}. \\] It can easily be checked that \\(AX = I_3 = XA\\), so \\(X\\) is the inverse of \\(A\\). Note that in the case of a singular matrix, this would be revealed in the above Gaussian elimination by yeilding an inconsistent set of equations. The advantage of having the matrix inverse is that we can now compute the solutions to \\(A\\mathbf{x}=\\mathbf{y}\\) for any vector \\(\\mathbf{y}\\) and we only had to perform the Guassian elimintation process once. Sometimes we make do with so called partial inverses as we have for the inverse trigonometric functions and the square root function.↩︎ The technical term for the fact that we can swap the order.↩︎ Due to the rank-nullity theorem.↩︎ "],["eigenvalues-and-eigenvectors.html", "Chapter 10 Eigenvalues and Eigenvectors 10.1 Matrix Diagonalisation 10.2 Finding Eigenvalues 10.3 Finding Eigenvectors 10.4 Powers of diagonalisable matrices", " Chapter 10 Eigenvalues and Eigenvectors Some linear transformations have “natural directions” associated with them. Example 10.1 Let \\[ A = \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\end{pmatrix}, \\] We can see that \\[ A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}. \\] Thus, it follows that \\[ A\\mathbf{u}_1 = 2 \\mathbf{u}_1, \\qquad \\text{where }\\quad\\mathbf{u}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}. \\] Moreover, we have \\[ A \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\2\\end{pmatrix}, \\] and so \\[ A\\mathbf{u}_2 = -\\mathbf{u}_2, \\qquad \\text{where }\\quad \\mathbf{u}_2 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}. \\] The above example shows that there are vectors associated to a matrix, which, when multiplied with the matrix from the left, are only scaled by a factor. This motivates the following definition. Definition 10.1 (Eigenvalues and Eigenvectors) Let \\(A\\) be an \\(n\\times n\\) matrix. Then a non-zero vector \\(\\mathbf{u}\\) is said to be an eigenvector of \\(A\\) if there exists a scalar \\(\\lambda\\) such that \\[ A\\mathbf{u} = \\lambda \\mathbf{u} \\] The scalar \\(\\lambda\\) is called the eigenvalue associated to \\(\\mathbf{u}\\). Example 10.2 In Example 10.1, we have that \\(\\mathbf{u}_1 = \\left(\\begin{smallmatrix}1\\\\1\\end{smallmatrix}\\right)\\) is an eigenvector associated with an eigenvalue \\(\\lambda_1 = 2\\), and that \\(\\mathbf{u}_2 = \\left(\\begin{smallmatrix}1\\\\-2\\end{smallmatrix}\\right)\\) is an eigenvector associated with an eigenvalue \\(\\lambda_2 = -1\\). We claim that any vector may be written as a linear combination of \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). That is, for all \\(\\mathbf{v}\\), there exist scalars \\(x_1, x_2\\) such that \\[ x_1 \\mathbf{u}_1 + x_2 \\mathbf{u}_2 = \\mathbf{v}. \\] Write \\(\\mathbf{x} = \\left(\\begin{smallmatrix}x_1\\\\ x_2\\end{smallmatrix}\\right)\\), then we may write the above equation as \\(B \\mathbf{x} = \\mathbf{v}\\), where \\(B = (\\mathbf{u}_1 \\ \\mathbf{u}_2)\\), i.e. the columns of the matrix \\(B\\) are \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). Note that \\[ \\det(B) = \\begin{vmatrix} 1 &amp; 1 \\\\ 1 &amp; -2 \\end{vmatrix} = -2 -1 = -3, \\] which is non-zero, hence the matrix is invertible, and so the equation \\(B \\mathbf{x} = \\mathbf{v}\\) has the (unique) solution \\(\\mathbf{x} = B^{-1} \\mathbf{v}\\). The matrix \\(B\\) being invertible is equivalent to saying that \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) are linearly independent. Having shown that any vector \\(\\mathbf{v}\\) may be written as a (unique) linear combination of \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\), we apply \\(A\\) to both sides of and deduce \\[\\begin{align*} A\\mathbf{v} &amp;= A(x_1 \\mathbf{u}_1 + x_2 \\mathbf{u}_2)\\\\ &amp;= x_1 A\\mathbf{u}_1 + x_2 A\\mathbf{u}_2\\\\ &amp;= \\lambda _1 x_1 \\mathbf{u}_1 + \\lambda_2 x_2 \\mathbf{u}_2, \\end{align*}\\] where we use the facts \\(A\\mathbf{u}_1 = \\lambda_1 \\mathbf{u}_1\\) and \\(A\\mathbf{u}_2 = \\lambda_2 \\mathbf{u}_2\\) for the last equality. Hence if we write a vector \\(\\mathbf{v}\\) in terms of the eigenvectors \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\), it is simple to calculate the result of multiplying by the matrix \\(A\\): we simply multiply these components by their eigenvalues. That is, we just strech/compress by a factor \\(\\lambda_i\\) in the \\(\\mathbf{u}_i\\) direction (and reverse direction if \\(\\lambda_i\\) is negative). 10.1 Matrix Diagonalisation Definition 10.2 (Diagonal Matrix) A square matrix is said to be a diagonal matrix if its non-diagonal entries are zero. That is, a matrix of the form \\[ D = \\begin{pmatrix} \\lambda_1 &amp; 0 &amp; 0 &amp; \\dotsb &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; 0 &amp;\\dotsb &amp; 0\\\\ \\vdots &amp; &amp;\\ddots &amp; &amp; \\vdots\\\\ 0&amp; &amp; \\dotsb &amp; &amp; \\lambda_n\\\\ \\end{pmatrix} \\] for any scalars \\(\\lambda_1,\\dotsc,\\lambda_n\\). Consider the diagonal matrix \\[ D = \\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix}, \\] Note that this acts very simply on a vector \\(\\mathbf{x}=\\left(\\begin{smallmatrix}x_1\\\\x_2\\end{smallmatrix}\\right)\\): \\[ D\\mathbf{x}= \\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix} \\begin{pmatrix}x_1\\\\x_2\\end{pmatrix} =\\begin{pmatrix} \\lambda_1 x_1\\\\ \\lambda_2 x_2 \\end{pmatrix} = \\begin{pmatrix} 2 x_1\\\\ -x_2 \\end{pmatrix} \\] it simply multiplies each element of the vector by a value on the diagonal. This is reminiscent of the action of \\(A\\) on We’ll now look at an example to understand the meaning of the term “diagonalising a matrix”. Example 10.3 Suppose that we write a vector \\(\\mathbf{v} = x_1 \\mathbf{u}_1 + x_2 \\mathbf{u}_2\\) in terms of the eigenvectors \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) from example (exm:eigen2). We say that \\(\\mathbf{v}\\) has the coordinate vector \\[\\mathbf{x}_{\\mathcal{P}}=\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\\] with respect to the vectors13 \\(\\mathcal{P}=[\\mathbf{u}_1,\\mathbf{u}_2]\\) – these vectors take the place of the usual vectors \\(\\mathcal{E}=[\\mathbf{e}_1,\\mathbf{e}_2]\\) (although note that the vectors in \\(\\mathcal{P}\\) are not unit vectors here). We may write \\(\\mathbf{v}\\) as \\[ \\mathbf{v} = P \\mathbf{x}_\\mathcal{P}, \\] where \\(P = (\\mathbf{u}_1\\; \\mathbf{u}_2)\\) is the matrix whose columns are the vectors \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). Consider an exemplar vector \\[ \\mathbf{v} = 5 \\mathbf{u}_1 - 3 \\mathbf{u}_2= 2 \\mathbf{e}_1 + 11 \\mathbf{e}_2 \\] Then \\(\\mathbf{v}\\) has co-ordinate vector \\(\\mathbf{x}_\\mathcal{P}=(5,-3)\\) and \\[ \\mathbf{v} = P \\begin{pmatrix} 5 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -2 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 11\\end{pmatrix} \\] We have seen that \\(A\\) acts very simply in these coordinates: \\[ A\\mathbf{v}=\\lambda _1 x_1 \\mathbf{u}_1 + \\lambda_2 x_2 \\mathbf{u}_2 \\] and writing this result as a coordinate vector with respect to \\(\\mathcal{P}\\) we would have \\[ (A\\mathbf{v})_\\mathcal{P}= \\begin{pmatrix} \\lambda_1 x_1\\\\ \\lambda_2 x_2 \\end{pmatrix} \\] We can see that \\(A\\) acts like a diagonal matrix in the coordinates \\(\\mathcal{P}\\). If we defined \\[D=\\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix}\\] then note that \\(D\\) acts on the coordinate vector as \\[D\\mathbf{x}_\\mathcal{P}= \\begin{pmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{pmatrix} \\begin{pmatrix} x_1\\\\ x_2 \\end{pmatrix} =\\begin{pmatrix} \\lambda_1 x_1\\\\ \\lambda_2 x_2 \\end{pmatrix} \\] Since the matrix \\(P\\) converts between coordinates \\(\\mathcal{P}\\) and standard coordinates \\(\\mathcal{E}\\), that is \\(\\mathbf{v}=P\\mathbf{x}_\\mathcal{P}\\), we can see that \\(P^{-1}\\) converts in the other direction \\(\\mathbf{x}_\\mathcal{P}=P^{-1}\\mathbf{v}\\). We can use this to write \\(A\\) as \\[A=PDP^{-1}.\\] This decomposes \\(A\\mathbf{v}\\) into three operations \\(PDP^{-1}\\mathbf{v}\\): Apply \\(P^{-1}\\) to convert \\(\\mathbf{v}\\) to \\(P^{-1}\\mathbf{v}=\\mathbf{x}_\\mathcal{P}\\); Now apply the diagonal matrix \\(D\\) to \\(\\mathbf{x}_\\mathcal{P}\\), which simply multiplies each component by the eigenvalue \\(\\lambda_i\\); Finally, convert back to standard coordinates by applying \\(P\\) to give the result \\(A\\mathbf{v}\\). We call the matrix \\(D\\) the diagonalisation of the matrix \\(A\\). Note that \\(D\\) can be obtained by applying \\(P^{-1}\\) to the left and \\(P\\) to the right of the above equation to obtain: \\[P^{-1}AP=P^{-1}PDP^{-1}P=D.\\] (using \\(P^{-1}P=I\\)). Theorem 10.1 (Diagonalisation) Let \\(A\\) be an \\(n\\times n\\) matrix with \\(n\\) eigenvectors \\(\\mathbf{u}_1, \\dots, \\mathbf{u}_n\\) with corresponding eigenvalues \\(\\lambda_1, \\dotsc, \\lambda_n\\). If the matrix \\(P=(\\mathbf{u}_1 \\dotsb \\mathbf{u}_n)\\) is invertible (in other words, the eigenvectors are linearly independent) then \\[ D = P^{-1}A P, \\] where \\(D\\) is the diagonal matrix whose \\(i\\)-th diagonal entry is \\(\\lambda_i\\). We say that \\(A\\) is diagonalisable. It is worth noting here that it is not always possible to diagonalise a matrix. We shall see an example later. 10.2 Finding Eigenvalues Suppose \\(\\mathbf{u}\\) is an eigenvector of a matrix \\(A\\) with associated eigenvalue \\(\\lambda\\). This means that \\(\\mathbf{u}\\) is a non-zero vector such that \\(A\\mathbf{u} = \\lambda \\mathbf{u}\\). Equally, \\[\\begin{array}{rrcl} &amp; \\lambda \\mathbf{u} - A \\mathbf{u} &amp;=&amp; \\mathbf{0} \\\\ \\Longleftrightarrow\\qquad &amp; \\lambda I_n \\mathbf{u} - A\\mathbf{u} &amp;=&amp; \\mathbf{0} \\\\ \\Longleftrightarrow\\qquad &amp; (\\lambda I_n-A) \\mathbf{u} &amp;=&amp; \\mathbf{0}. \\end{array}\\] Setting \\(B_{\\lambda} = \\lambda I_n - A\\), we have that \\(\\lambda\\) is an eigenvalue of \\(A\\) if, and only if, \\(B_{\\lambda}\\mathbf{u} = \\mathbf{0}\\) has a non-zero solution \\(\\mathbf{u}\\). Recall that for a general square matrix \\(M\\), the matrix–vector system \\(M \\mathbf{v} = \\mathbf{b}\\) has a unique solution for \\(\\mathbf{v}\\) if, and only if, \\(\\det(M) \\neq 0\\). Note the system \\(B_{\\lambda} \\mathbf{v} = \\mathbf{0}\\) always has at least one solution, namely \\(\\mathbf{v} = \\mathbf{0}\\), so the system has a non-zero solution if, and only if, it has more than one solution, which is equivalent to \\(\\det(B_{\\lambda}) = 0\\). We use this fact to find eigenvalues, as shown in the following example. Example 10.4 (Finding Eigenvalues) Let \\[ A = \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\end{pmatrix}. \\] Then the matrix \\(B_{\\lambda} = \\lambda I_{2} - A\\) is given as \\[ B_{\\lambda} = \\begin{pmatrix} \\lambda - 1 &amp; -1 \\\\ -2 &amp; \\lambda \\end{pmatrix}. \\] Then we have \\[\\begin{align*} \\det(B_{\\lambda}) &amp;= \\lambda(\\lambda-1) - 2 \\\\ &amp;= \\lambda^2 - \\lambda - 2 \\\\ &amp;= (\\lambda - 2)(\\lambda + 1). \\end{align*}\\] Thus, \\(\\lambda\\) is an eigenvalue of \\(A\\) if and only if \\[ 0 = (\\lambda - 2)(\\lambda + 1), \\] and equivalently if and only if \\(\\lambda = 2\\) or \\(\\lambda = -1\\). Hence, the eigenvalues of \\(A\\) are \\[ \\lambda_1 = 2 \\qquad\\text{and}\\qquad \\lambda_2 = -1. \\] Note, that in the above example \\(\\det(B_\\lambda)\\) is a polynomial of degree 2 in the variable \\(\\lambda\\). We can generalise this observation. Definition 10.3 (Characteristic Polynomial) Let \\(A\\) be an \\(n\\times n\\) matrix. Then the function \\[ p_A(\\lambda)=\\det(\\lambda I_n - A) \\] is a polynomial of degree \\(n\\) in the variable \\(\\lambda\\). We call this polynomial \\(p_A\\) the characteristic polynomial of \\(A\\). So in general, we can find the eigenvalues of a square matrix \\(A\\) by finding the roots of the characteristic polynomial \\(p_A\\), which is obtained from computing the determinant \\(\\det(\\lambda I_n - A)\\). 10.3 Finding Eigenvectors Example 10.5 (Finding Eigenvectors) Let again \\[ A = \\begin{pmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\end{pmatrix}. \\] We know from above that \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = -1\\) are the eigenvalues of \\(A\\). We first find an eigenvector corresponding to \\(\\lambda_1\\). Any non-zero solution \\(\\mathbf{v} = \\left(\\begin{smallmatrix}v_1\\\\v_2\\end{smallmatrix}\\right)\\) of the equation \\(B_2 \\mathbf{v} = \\mathbf{0}\\) is such an eigenvector. For \\(\\lambda = \\lambda_1 = 2\\), we have \\[ B_2 = \\begin{pmatrix} 1 &amp; -1 \\\\ -2 &amp; 2 \\end{pmatrix}. \\] We solve \\(B_2 \\mathbf{v} = \\mathbf{0}\\) via Gaussian elimination. The augmented matrix for \\(B_2 \\mathbf{v} = \\mathbf{0}\\) is given by \\[ \\left(\\begin{array}{rr|r} 1 &amp; -1 &amp; 0 \\\\ -2 &amp; 2 &amp; 0 \\end{array}\\right). \\] We perform the ERO \\(R_2 \\to R_2 + 2R_1\\) to obtain \\[ \\left(\\begin{array}{rr|r} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] From this, we see that \\(v_2\\) is arbitrary, say \\(v_2 = \\alpha\\), and \\(v_1 = v_2 = \\alpha\\). So, the eigenvectors corresponding to the eigenvalue \\(\\lambda_1 = 2\\) are given by the set \\[ \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\] for any non-zero value of \\(\\alpha\\). By a similar process, we find an eigenvector corresponding to \\(\\lambda_2 = -1\\). Any non-zero solution \\(\\mathbf{v} = \\left(\\begin{smallmatrix}v_1\\\\v_2\\end{smallmatrix}\\right)\\) of the equation \\(B_{-1} \\mathbf{v} = \\mathbf{0}\\) is such an eigenvector. We have \\[ B_{-1} = \\begin{pmatrix} -2 &amp; -1 \\\\ -2 &amp; -1\\end{pmatrix}, \\] and again solve \\(B_{-1} \\mathbf{v} = \\mathbf{0}\\) via Gaussian elimination to give the eigenvectors corresponding to \\(\\lambda_2 = -1\\) as \\[ \\beta \\begin{pmatrix} 1 \\\\ -2\\end{pmatrix} \\] for any non-zero value of \\(\\beta\\). Note that there are infinitely many eigenvectors corresponding to an eigenvalue, because if a non-zero vector \\(\\mathbf{v}\\) satisfies \\(B_{\\lambda} \\mathbf{v} = \\mathbf{0}\\), then any multiple of \\(\\mathbf{v}\\), i.e. \\(\\mu\\mathbf{v}\\) for any \\(\\mu \\neq 0\\) satisfies the equation, too, as we have \\(B_{\\lambda} (\\mu\\mathbf{v}) = \\mu (B_{\\lambda}{\\mathbf{v}}) = \\mu\\mathbf{0} = \\mathbf{0}\\). In order to obtain specific eigenvectors, we simply need to set \\(\\alpha\\) and \\(\\beta\\) to some non-zero value, for which we usually simply take \\(\\alpha=1\\) and \\(\\beta=1\\). Hence two suitable eigenvectors of \\(A\\) are \\[ \\mathbf{u}_1=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\quad\\text{ and }\\quad \\mathbf{u}_2=\\begin{pmatrix} 1 \\\\ -2\\end{pmatrix} \\] With the following example, we demonstrate how to find the eigenvalues and eigenvectors for a \\(3\\times 3\\) matrix. Example 10.6 Let \\(A\\) be given by \\[ A = \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 2 &amp; 3 \\end{pmatrix}. \\] Then \\(A\\) has characteristic polynomial \\[\\begin{align*} p_A(\\lambda) &amp;= \\det(\\lambda I_3 - A) \\\\ &amp;= \\begin{vmatrix} \\lambda - 1 &amp; 0 &amp; 1 \\\\ -1 &amp; \\lambda - 2 &amp; -1 \\\\ -2 &amp; -2 &amp; \\lambda - 3 \\end{vmatrix}. \\end{align*}\\] We have: \\[\\begin{align*} p_A(\\lambda) &amp;= (\\lambda - 1) \\begin{vmatrix} \\lambda -2 &amp; -1 \\\\ -2 &amp; \\lambda -3 \\end{vmatrix} + \\begin{vmatrix} -1 &amp; \\lambda -2 \\\\ -2 &amp; -2 \\end{vmatrix}\\\\ &amp;= (\\lambda - 1)\\big( (\\lambda - 2)(\\lambda - 3) - 2 \\big) + (2 + 2(\\lambda - 2)) \\\\ &amp;= (\\lambda - 1)(\\lambda^2 - 5 \\lambda + 4) + 2(\\lambda - 1) \\\\ &amp;= (\\lambda - 1)(\\lambda^2 - 5 \\lambda + 6) \\\\ &amp;= (\\lambda - 1)(\\lambda - 2)(\\lambda - 3). \\end{align*}\\] Therefore, we find that \\(A\\) has eigenvalues \\(\\lambda_1 = 1\\), \\(\\lambda_2 = 2\\) and \\(\\lambda_3 = 3\\). It remains to find the eigenvectors of \\(A\\). Recall that, for each eigenvalue \\(\\lambda\\), this is equivalent to finding all non-zero solutions \\(\\mathbf{v} = (v_1,v_2,v_3)\\) to the equation \\(B_{\\lambda} \\mathbf{v} = \\mathbf{0}\\), where \\(B_{\\lambda} = \\lambda I_3 - A\\). Here, we have \\[ B_{\\lambda} = \\begin{pmatrix} \\lambda - 1 &amp; 0 &amp; 1 \\\\ -1 &amp; \\lambda - 2 &amp; -1 \\\\ -2 &amp; -2 &amp; \\lambda - 3 \\end{pmatrix}. \\] We find the eigenvectors for the eigenvalue \\(\\lambda_1 = 1\\). For \\(\\lambda = \\lambda_1 = 1\\), we have \\[ B_{1} = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ -1 &amp; -1 &amp; -1 \\\\ -2 &amp; -2 &amp; -2 \\end{pmatrix}, \\] which is (automatically) the augmented matrix representing \\(B_{1} \\mathbf{v} = \\mathbf{0}\\) where we omit the solution vector. We perform a Gaussian elimination. First, we apply EROs \\(R_3 \\to R_3 - 2 R_2\\) and \\(R_2 \\to -R_2\\) to obtain \\[ \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] and then we apply \\(R_1 \\leftrightarrow R_2\\) to have \\[ \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] and finally \\(R_1 \\to R_1 - R_2\\) to arrive at \\[ \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\] From this we see that \\(v_2\\) can be chosen arbitrary, say \\(v_2 = \\alpha\\). Then, in view of \\(v_1 = -v_2\\) and \\(v_3 = 0\\), we have the set of eigenvectors corresponding to the eigenvalue \\(\\lambda_1 = 1\\) given as \\[ \\alpha \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\] For instance, taking \\(\\alpha = 1\\), gives a specific eigenvector \\[ \\mathbf{u}_1 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}. \\] We find the eigenvectors for the eigenvalue \\(\\lambda_2 = 2\\). For \\(\\lambda = \\lambda_2 = 2\\), we have \\[ B_{2} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; -1 \\\\ -2 &amp; -2 &amp; -1 \\end{pmatrix}. \\] As above, we use Gaussian elimination to find non-zero solutions \\(\\mathbf{v}\\) to the equation \\(B_{2} \\mathbf{v} = \\mathbf{0}\\). This gives the set of eigenvectors \\[ \\beta \\begin{pmatrix} -2 \\\\ 1 \\\\ 2 \\end{pmatrix} \\] For instance, taking \\(\\beta = 1\\), we find the specific eigenvector \\[ \\mathbf{u}_2 = \\begin{pmatrix} -2 \\\\ 1 \\\\ 2 \\end{pmatrix}. \\] We find the eigenvectors for the eigenvalue \\(\\lambda_3 = 3\\). For \\(\\lambda = \\lambda_3 = 4\\), we have \\[ B_{3} = \\begin{pmatrix} 2 &amp; 0 &amp; 1 \\\\ -1 &amp; 1 &amp; -1 \\\\ -2 &amp; -2 &amp; 0 \\end{pmatrix}. \\] As above, we use Gaussian elimination to find non-zero solutions \\(\\mathbf{v}\\) to the equation \\(B_{3} \\mathbf{v} = \\mathbf{0}\\). This gives the set of eigenvectors \\[ \\gamma \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix}. \\] For instance, taking \\(\\gamma = -1\\), we find the specific eigenvector \\[ \\mathbf{u}_3 = (1,-1,-2). \\] It also makes sense to consider vectors and matrices with complex number entries. These arise in applications such as analysis of electronic circuits and in quantum mechanics. We won’t consider such situations here, but we will now see that even if we start with vectors and matrices with real entries, complex number eigenvalues and eigenvectors with complex entries can still arise. We can immediately see that this could be possible, since the eigenvalues are calculated as roots of the characteristic polynomial and we know that real polynomials can have complex roots. Example 10.7 (Complex Eigenvalues and Eigenvectors) Consider the matrix \\[ A= \\begin{pmatrix} 0&amp;-1\\\\ 1&amp;0 \\end{pmatrix} \\] The matrix \\(A\\) has the effect of rotating vectors in the plane anticlockwise by \\(\\pi/2\\) (it is matrix \\(R_{\\pi/2}\\) from example (exm:rotmat)), so we can see geometrically that it can’t have any real eigenvectors, since these would correspond to fixed directions in the plane that are scaled by an eigenvalue. The characteristic polynomial is \\[p_A(\\lambda)=\\lambda^2+1\\] which has solutions \\(\\lambda_1=i\\) and \\(\\lambda_2=-i\\). We can also find the eigenvectors in the usual way by solving \\((A-\\lambda I)\\mathbf{v}=0\\). For \\(\\lambda_1\\) \\[ (A-iI)= \\begin{pmatrix} -i&amp;-1\\\\ 1&amp;-i \\end{pmatrix} \\] and performing \\(R_1\\to iR_1\\) \\[ \\begin{pmatrix} 1&amp;-i\\\\ 1&amp;-i \\end{pmatrix} \\] we find that the corresponding eigenvectors are \\(\\alpha\\left(\\begin{smallmatrix}1\\\\-i\\end{smallmatrix}\\right)\\) and taking \\(\\alpha=1\\), \\[ \\mathbf{u}_1= \\begin{pmatrix} 1\\\\ -i \\end{pmatrix} \\] Similarly, we find an eigenvector for \\(\\lambda_2\\): \\[ \\mathbf{u}_2= \\begin{pmatrix} 1\\\\ i \\end{pmatrix} \\] Let \\[ P=\\begin{pmatrix} 1&amp;1\\\\ -i&amp;i \\end{pmatrix} \\] then \\[ P^{-1}= \\frac{1}{2i} \\begin{pmatrix} i&amp;-1\\\\ i&amp;1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1&amp;i\\\\ 1&amp;-i \\end{pmatrix} \\] and letting \\[ D= \\begin{pmatrix} i&amp;0\\\\ 0&amp;-i \\end{pmatrix} \\] we may write \\[ A=PDP^{-1}. \\] Although \\(P^{-1}\\), \\(D\\) and \\(P\\) all contain complex numbers, their product leaves only real numbers and we get back to the real matrix \\(A\\) (check that \\(PDP^{-1}\\) does indeed equal \\(A\\)). Complex roots of real polynomials always come in complex conjugate pairs and so we will have complex conjugate eigenvalue pairs and corresponding complex conjugate eigenvector pairs. To see this, if \\(p_B(\\lambda)=\\lambda^n+a_{n-1}\\lambda^{n-1}+\\dotsb+a_1\\lambda+a_0\\) is a (real) characteristic polynomial of a real \\(n\\times n\\) matrix \\(B\\) and \\(\\lambda\\) is a complex root, then taking the complex conjugate \\[\\begin{align*} 0=p_B(\\lambda)=\\overline{p_B(\\lambda)}&amp;=\\overline{\\lambda^n}+\\overline{a_{n-1}\\lambda^{n-1}}+\\dotsb+\\overline{a_1\\lambda}+\\overline{a_0}\\\\ &amp;=\\overline{\\lambda}^n+a_{n-1}\\overline{\\lambda}^{n-1}+\\dotsb+a_1\\overline{\\lambda}+{a_0} \\end{align*}\\] (using that the coefficients \\(a_j\\) are real) shows that \\(\\overline{\\lambda}\\) is also a root. Let \\(\\mathbf{u}\\) be an eigenvector corresponding to \\(\\lambda\\). Then, \\[ B\\overline{\\mathbf{u}}=\\overline{B}\\overline{\\mathbf{u}}=\\overline{B\\mathbf{u}}=\\overline{\\lambda \\mathbf{u}}=\\overline{\\lambda}\\overline{\\mathbf{u}} \\] (using that \\(B\\) is real in the first step) i.e. the vector \\(\\overline{\\mathbf{u}}\\) is an eigenvector with eigenvalue \\(\\overline{\\lambda}\\). As we mentioned earlier, not all matrices are diagonalisable. Here is an example. Example 10.8 (A non-diagonalisable matrix) Consider the matrix \\[ A=\\begin{pmatrix} 3&amp;1\\\\ 0&amp;3 \\end{pmatrix}. \\] The characteristic polynomial of \\(A\\) is \\[ p_A(x)=\\det(\\lambda I-A)= \\begin{vmatrix} \\lambda-3&amp;-1\\\\ 0&amp;\\lambda-3 \\end{vmatrix} =(\\lambda-3)^2 \\] so the only eigenvalue of \\(A\\) is \\(3\\). Now \\[ A-3I= \\begin{pmatrix} 0&amp;1\\\\ 0&amp;0 \\end{pmatrix} \\] so the eigenvectors are \\[ \\alpha \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} \\] So we do not have 2 linearly independent eigenvectors since all eigenvectors are multiples of \\(\\mathbf{u}=\\left(\\begin{smallmatrix}1\\\\0\\end{smallmatrix}\\right)\\). Therefore, we cannot diagonalise this matrix. We briefly mention that there are other useful matrix decompositions to diagonalisation that always work. The generalisation of diagonalisation is the Jordan Normal Form, which is an important theoretical tool. Another key technique is the Singular Value Decomposition which is important in numerical applications of matrices and in data science (see Principal Component Analysis). 10.4 Powers of diagonalisable matrices One of the most important applications of diagaonalisation is in computing powers of matrices. If a matrix \\(A\\) is diagonalisable, we can write it as \\[A=PDP^{-1}\\] Now if we want to cmopute \\(A^2\\), we have \\[A^2=PDP^{-1}PDP^{-1}=PD^2P^{-1}\\] where we used \\(P^{-1}P=I\\). Now, more generally \\[ A^n=PDP^{-1}PDP^{-1}\\dotsb PDP^{-1}=PD^nP^{-1} \\] for any natural number \\(n\\). By the rules of matrix multiplication, the \\(n\\)-th power of the diagonal matrix \\[ D= \\begin{pmatrix} \\lambda_1&amp;&amp;0\\\\ &amp;\\ddots&amp;\\\\ 0&amp;&amp;\\lambda_m \\end{pmatrix} \\] is simply \\[ D^n = \\begin{pmatrix} \\lambda_1^n&amp;&amp;0\\\\ &amp;\\ddots&amp;\\\\ 0&amp;&amp;\\lambda_m^n \\end{pmatrix}. \\] This gives us an efficient way to compute \\(A^n\\). Example 10.9 (Powers of a diagonalisable matrix) Consider the matrix \\[ A= \\begin{pmatrix} 8&amp;-6\\\\ 3&amp;-1 \\end{pmatrix}. \\] We find \\[A=PDP^{-1}= \\begin{pmatrix} 1&amp;2\\\\ 1&amp;1 \\end{pmatrix} \\begin{pmatrix} 2&amp;0\\\\ 0&amp;5 \\end{pmatrix} \\begin{pmatrix} -1&amp; 2\\\\ 1&amp; -1 \\end{pmatrix}.\\] Now using \\[ A^n=PD^nP^{-1} \\] we have \\[ A^n= \\begin{pmatrix} 1&amp;2\\\\ 1&amp;1 \\end{pmatrix} \\begin{pmatrix} 2^n&amp;0\\\\ 0&amp;5^n \\end{pmatrix} \\begin{pmatrix} -1&amp; 2\\\\ 1&amp; -1 \\end{pmatrix} = \\begin{pmatrix} -2^n+2\\times 5^n&amp; 2^{n+1}-2\\times 5^n\\\\ -2^{n}+5^n&amp; 2^{n+1}-5^n \\end{pmatrix}. \\] We now have a nice expression that works for any \\(n\\) and this is much easier than trying to calculate \\(A^n\\) directly. Technically, \\(\\mathcal{P}\\) is what is known as a basis – a list of vectors that are linearly independent and span the space and hence can act as a coordinate system.↩︎ "],["differentiation.html", "Chapter 11 Differentiation 11.1 Concept 11.2 Standard derivatives 11.3 Rules 11.4 Higher order derivatives 11.5 Further Techniques", " Chapter 11 Differentiation 11.1 Concept Differentiation is the mathematical tool to answer questions about rates of change. The derivative of a real function \\(f\\) at a point \\((x,f(x))\\) is the slope of the tangent line at that point. Figure 11.1: The tangent line to a graph. To find this gradient, we start by approximating the tangent line by taking a secant line through the point \\((x,f(x))\\) and a point \\((x+h,f(x+h))\\) with \\(h\\) a small number. Figure 11.2: A secant line through \\((x,f(x))\\) and a point \\((x+h,f(x+h))\\). Now if we take \\(h\\to 0\\), that is \\(h\\) becomes vanishingly small, the second point approaches \\((x,f(x))\\). Then the gradient of the secant line converges to the gradient of the tangent line. The gradient of the secant line is: \\[ \\frac{\\Delta y}{\\Delta x}=\\frac{f(x+h)-f(x)}{h}. \\] The mathematical term for taking \\(h\\to 0\\) is taking the limit as \\(h\\) tends to \\(0\\), written \\[ \\lim\\limits_{h\\to 0}\\frac{f(x+h)-f(x)}{h}. \\] The idea here is to consider what happens to this mathematical expression as \\(h\\) becomes infinitesimally small. We call this gradient of the tangent line the derivative of \\(f\\) at \\(x\\). We use the following notations for the derivative: \\[f&#39;(x)\\quad\\text{ or }\\quad \\frac{df}{dx} \\] Here we present some examples of finding derivatives from first principles, i.e. by using the limit definition of the derivative. Example 11.1 (Derivatives from first principles) The first two examples are applied to straight lines, whose tangents are the same as the lines themselves, hence we should get the same gradient from the derivative. Consider a constant function \\(f(x)=c\\) (the graph of which is a straight horizontal line). We know that the gradient of this line is zero. Let’s try applying the definition. \\[\\begin{align*} f&#39;(x)&amp;=\\lim\\limits_{h\\to 0}\\frac{f(x+h)-f(x)}{h}\\\\ &amp;=\\lim\\limits_{h\\to 0}\\frac{c-c}{h}\\quad\\text{applying the definition of $f$}\\\\ &amp;=\\lim\\limits_{h\\to 0}\\frac{0}{h}\\\\ &amp;=\\lim\\limits_{h\\to 0}0\\quad\\text{note this expression is now independent of the value of $h$, hence...}\\\\ &amp;=0 \\end{align*}\\] in agreement with our geometical understanding. Consider a straight line of the form \\(f(x)=mx+c\\). Using the definition of the derivative \\[\\begin{align*} f&#39;(x)&amp;=\\lim\\limits_{h\\to 0}\\frac{f(x+h)-f(x)}{h}\\\\ &amp;=\\lim\\limits_{h\\to 0}\\frac{[m(x+h)+c]-[mx+c]}{h}\\quad\\text{applying the definition of $f$}\\\\ &amp;=\\lim\\limits_{h\\to 0}\\frac{mh}{h}\\\\ &amp;=\\lim\\limits_{h\\to 0}m\\quad\\text{note this expression is now independent of the value of $h$, hence...}\\\\ &amp;=m. \\end{align*}\\] again in agreement with our geometical understanding. Consider \\(f(x)=x^2\\). \\[\\begin{align*} f&#39;(x)&amp;=\\lim\\limits_{h\\to 0}\\frac{f(x+h)-f(x)}{h}\\\\ &amp;=\\lim\\limits_{h\\to 0}\\frac{(x+h)^2-x^2}{h}\\quad\\text{applying the definition of $f$}\\\\ &amp;=\\lim\\limits_{h\\to 0}\\frac{x^2+2xh+h^2-x^2}{h}\\quad\\text{expanding brackets}\\\\ &amp;=\\lim\\limits_{h\\to 0}2x+h\\quad\\text{by simplifying}\\\\ &amp;=2x\\quad\\text{since $h$ becomes vanishingly small.} \\end{align*}\\] It would be hard work if we always had to apply the definition to find the derivatives of functions. Thankfully, others have already done the hard work and we have some rules for derivatives of standard functions, together with some rules for derivatives of combinations of functions (including sums, products and compositions). This allows us to compute the derivatives of a wide variety of expressions we might encounter. 11.2 Standard derivatives \\[\\begin{align*} &amp;\\frac{d}{dx}x^a=ax^{a-1}\\qquad \\text{for any $a\\in\\mathbb{R}$}\\\\ &amp;\\\\ &amp;\\frac{d}{dx}\\ln(x)=\\frac{1}{x}\\\\ &amp;\\frac{d}{dx}e^{x}=e^{x}\\\\ &amp;\\\\ &amp;\\frac{d}{dx}\\sin(x)=\\cos(x)\\\\ &amp;\\frac{d}{dx}\\cos(x)=-\\sin(x)\\\\ &amp;\\frac{d}{dx}\\tan(x)=\\sec^2(x)\\\\ &amp;\\\\ &amp;\\frac{d}{dx}\\sinh(x)=\\cosh(x)\\\\ &amp;\\frac{d}{dx}\\cosh(x)=\\sinh(x) \\end{align*}\\] 11.3 Rules Sum Rule \\[ (f+g)&#39;(x)=f&#39;(x)+g&#39;(x) \\] Example 11.2 Find the derivative of \\[h(x)=x^3+\\sin(x).\\] Let \\(f(x)=x^3\\), \\(g(x)=\\sin(x)\\). Then \\[h(x)=(f+g)(x)=x^3+\\sin(x)\\] and \\[h&#39;(x)=(f+g)&#39;(x)=f&#39;(x)+g&#39;(x)=3x^2+\\cos(x).\\] Product Rule \\[ (f\\cdot g)&#39;(x)=f&#39;(x)\\cdot g(x)+f(x)\\cdot g&#39;(x) \\] Example 11.3 Find the derivative of \\[h(x)=x^3\\sin(x)\\] Let \\(f(x)=x^3\\), \\(g(x)=\\sin(x)\\). Then \\[h(x)=(f\\cdot g)(x)=x^3\\sin(x)\\] and \\[h&#39;(x)=(f\\cdot g)&#39;(x)=f&#39;(x)g(x)+f(x)g&#39;(x)=3x^2\\sin(x)+x^3\\cos(x).\\] In particular (exercise), for any constant \\(a\\) we have: \\((af(x))&#39;=af&#39;(x)\\). Quotient Rule \\[ \\left(\\dfrac{f}{g}\\right)&#39;(x)=\\frac{g(x)\\cdot f&#39;(x)-f(x)\\cdot g&#39;(x)}{[g(x)]^2} \\] Example 11.4 Find the derivative of \\[h(x)=\\frac{x^3}{\\sin(x)}.\\] Let \\(f(x)=x^3\\), \\(g(x)=\\sin(x)\\). Then \\[h(x)=(\\frac{f}{g})(x)=\\frac{x^3}{\\sin(x)}\\] and \\[h&#39;(x)=(\\frac{f}{g})&#39;(x)=\\frac{g(x)\\cdot f&#39;(x)-f(x)\\cdot g&#39;(x)}{[g(x)]^2}=\\frac{3x^2\\sin(x)-x^3\\cos(x)}{\\sin^2(x)}.\\] Chain Rule \\[ \\left(f\\circ g\\right)&#39;(x)=f&#39;(g(x))\\cdot g&#39;(x) \\] (where \\(\\left(f\\circ g\\right)(x)=f(g(x))\\) denotes function composition). Example 11.5 Find the derivative of \\[h(x)=\\sin(x^3)\\] Let \\(f(x)=\\sin(x)\\), \\(g(x)=x^3\\). Then \\[h(x)=(f\\circ g)(x)=\\sin(x^3)\\] and \\[h&#39;(x)=(f\\circ g)&#39;(x)=f&#39;(g(x))\\cdot g&#39;(x)=\\cos(x^3)(3x^2)=3x^2\\cos(x^3).\\] The key to using these rules is in splitting your function \\(h(x)\\) into the right combination of two or more simpler functions (\\(f(x)\\) and \\(g(x)\\) in the above examples) to make it easy to apply the rules. 11.4 Higher order derivatives The derivative of a function \\(f\\) is also a function, which we denote \\(f&#39;\\). We can also take the derivative of the function \\(f&#39;\\), yielding the function \\((f&#39;)&#39;\\), which is usually written as \\(f&#39;&#39;\\), and is called the second derivative of \\(f\\). For example, with \\(f(x)=x^3\\) we have \\(f&#39;(x)=3x^2\\) and \\(f&#39;&#39;(x)=6x\\). There is no reason to stop at the second derivative, and we can define \\(f&#39;&#39;&#39;=(f&#39;&#39;)&#39;\\), \\(f&#39;&#39;&#39;&#39;=(f&#39;&#39;&#39;)&#39;\\) etc. After a few derivatives of \\(f\\) it is more convenient to use the notation: \\[\\begin{align*} f^{(0)}&amp;=f\\\\ f^{(1)}&amp;=f&#39;\\\\ f^{(2)}&amp;=(f&#39;)&#39;=f&#39;&#39;\\\\ f^{(3)}&amp;=(f&#39;&#39;)&#39;=f&#39;&#39;&#39;\\\\ &amp;\\phantom{..}\\vdots\\\\ f^{(n+1)}&amp;=(f^{(n)})&#39; \\end{align*}\\] The functions \\(f^{(n)}\\) for \\(n\\geq 2\\) are called higher-order derivatives. Derivatives may also be denoted using Leibniz notation: \\[ \\frac{df(x)}{dx} \\] This notation ties in with Leibniz’s intuitive notion of the derivative as the quotient of the “infinitely small difference \\(df(x)=f(x+dx)-f(x)\\)”, by the “infinitely small number \\(dx\\)”; but it is important to note that the derivative is defined as the limit, and the symbols \\(df(x)\\) and \\(dx\\) have no meaning of their own, and cannot be manipulated separately. Higher order derivatives are denoted \\[ \\frac{d^2f(x)}{dx^2},\\frac{d^3f(x)}{dx^3},\\dotsc,\\frac{d^nf(x)}{dx^n},\\dotsc \\] Leibniz’s Formula allows us to easily compute higher order derivatives of products. The \\(n^\\text{th}\\) derivative of the product \\((f\\cdot g)(x)\\) is \\[ (f\\cdot g)^{(n)}(x)=\\sum_{k=0}^{n}\\binom{n}{k}f^{(k)}(x)\\cdot g^{(n-k)}(x). \\] Recall that the binomial coefficient is given by: \\[ \\binom{n}{k}=\\frac{n!}{k!(n-k)!}. \\] 11.5 Further Techniques 11.5.1 Implicit Differentiation Curves in the plane are often described by the set of all coordinates \\((x,y)\\) satisfying an equation of the form \\({F(x,y)=0}\\) for some function \\(F\\) of two variables. For example, the unit circle is described by the equation \\(F(x,y)=x^2+y^2-1=0\\). As with the circle, we often cannot describe such a curve in the form of a graph of a function \\(y=f(x)\\) (since it is multivalued for each \\(x\\)). How do we differentiate such a curve? Example 11.6 The unit circle \\(x^2+y^2=1\\) has a well defined slope at every point except \\((-1,0)\\) and \\((1,0)\\) where there are vertical tangents. For the circle we can quite easily find functions that describe the differentiable parts of the curve: \\[\\begin{align*} f_1(x)&amp;=\\sqrt{1-x^2} &amp;-1&lt; x &lt; 1\\\\ f_2(x)&amp;=-\\sqrt{1-x^2} &amp;-1&lt; x &lt; 1 \\end{align*}\\] which describe the upper and lower halves of the circle, respectively. Figure 11.3: Functions describing the upper an lower parts of the circle Then to obtain the derivative at any point on the circle (except \\((-1,0),(1,0)\\)), we can calculate the derivative of the relevant function \\(f_1\\) or \\(f_2\\): \\[ f_1&#39;(x)=\\dfrac{-x}{(1-x^2)^\\frac{1}{2}},\\qquad f_2&#39;(x)=\\dfrac{x}{(1-x^2)^\\frac{1}{2}}. \\] For the circle we are able to write down differentiable functions with explicit formulae. In general, however, it will not be possible, or at least not easy, to find such functions. A function \\(f\\) defined by the fact that it satisfies an equation \\(F(x,f(x))=0\\) is called an implicit function; it is defined implicitly, rather than by an explicit formula. In general, the coordinates where \\(F(x,y)=0\\) may not be describable by the graph of a differentiable function. Even where it is, there may be more than one function defined by the relation, as was the case with the circle. Furthermore, it may not be possible to write down \\(f\\) as a nice formula as we did for the circle. If we assume that a function \\(f\\) is a differentiable solution to the equation, then we can derive a formula for \\(f&#39;\\) by the process of implicit differentiation. (We will only deal with simple cases here anyway and so need not be concerned with the technichal details, but in general we would need to turn to the Implicit Function Theorem to guarantee that all technichal assumptions are valid.) The process has two main steps: Differentiate both sides of the equation with respect to \\(x\\), treating \\(y\\) as a differentiable function of \\(y(x)\\) and using the Chain Rule. Solve for \\(y&#39;(x)\\), giving a formula involving \\(x\\) and \\(y\\) that yields the derivative at any point \\((x,y)\\) on the curve \\(F(x,y)=0\\). Example 11.7 (Implicit differentiation) Consider the unit circle \\(x^2+y^2=1\\). We treat \\(y=y(x)\\) as a differentiable function. \\[\\begin{align*} \\frac{d}{dx}(x^2+y^2)&amp;=\\frac{d}{dx}(1)\\\\ \\frac{d}{dx}(x^2)+\\frac{d}{dx}(y^2)&amp;=0\\\\ 2x+2y\\frac{dy}{dx}&amp;=0\\\\ \\frac{dy}{dx}&amp;=-\\frac{x}{y} \\end{align*}\\] Where in the third line we have used the chain rule, by setting \\(y=y(x)\\) so that \\[ \\frac{d}{dx}(y(x)^2)=2y(x)\\dfrac{d}{dx}(y(x)). \\] Note that we have “infinite gradients” if \\(y=0\\), as we had already stated. Note that this agrees with our previous analysis: if \\((a,b)\\) lies on the upper half of the circle then the slope at this point is given by \\[ f_1&#39;(a)=\\dfrac{-a}{(1-a^2)^\\frac{1}{2}} \\] or using the above implicit derivative and the formula for \\(f_1\\) \\[ \\frac{dy}{dx}=-\\frac{a}{b}=-\\frac{a}{f_1(a)}=-\\frac{a}{\\sqrt{1-a^2}}. \\] and similarly for a point on the lower half of the circle. 11.5.2 Logarithmic Differentiation By the chain rule, the derivative of \\(\\ln\\circ f\\) is \\(\\dfrac{f&#39;}{f}\\), and this will often be easier to compute than \\(f&#39;\\) because logarithms turn products into sums, and powers into products. The derivative for \\(f\\) can then be recovered by multiplying through by \\(f\\). This process is known as logarithmic differentiation. Example 11.8 (Logarithmic differentiation) Find the derivative of \\[ y=x^x. \\] First take the natural logarithm: \\[ \\ln(y)=x\\ln(x), \\] then differentiate implicitly: \\[\\begin{align*} \\frac{d}{dx}(\\ln(y))&amp;=\\frac{d}{dx}(x\\ln(x))\\\\ \\frac{1}{y}\\frac{dy}{dx}&amp;=\\frac{dx}{dx}\\ln(x)+x\\frac{d}{dx}\\ln(x)\\\\ \\frac{1}{y}\\frac{dy}{dx}&amp;=\\ln(x)+x\\frac{1}{x}\\\\ \\frac{dy}{dx}&amp;=y(\\ln(x)+1)=x^x(\\ln(x)+1). \\end{align*}\\] Warning! The derivative of \\(x^x\\) is \\(x\\cdot x^{x-1}\\). We can’t treat variable exponents like constant exponents! 11.5.3 Parametric Differentiation Curves in the plane may also be described parametrically. Given two functions \\(u(t)\\) and \\(v(t)\\), the curve \\[ \\{(x,y): x=u(t), y=v(t) \\] is said to be represented parametrically by \\(u\\) and \\(v\\), and the pair of functions \\(u\\) and \\(v\\) are called a parametric representation of the curve. We can think of such a curve as being the path traced out by a particle moving in the plane; at time \\(t\\) the particle is at the position with coordinates \\((u(t),v(t))\\). A parametric curve is a function from the real numbers into the plane, given by \\(F(t)=(u(t),y(t))\\). Such curves will generally not be the graph of a function (due to being multivalued for a given \\(x\\)-coordinate). Figure 11.4: The parametric curve given by \\(x(\\theta)=4\\sin(\\theta)+2\\sin(3\\theta)\\) and \\(y(\\theta)=4\\cos(\\theta)+2\\cos(3\\theta)\\). How do we differentiate such a curve? Suppose that \\({x=u(t)}\\), \\({y=v(t)}\\) is a parametric representation of a curve, and that \\(u&#39;\\neq 0\\) on some interval. Then on this interval the curve lies on the graph of a function \\(f\\), and at the point \\(x=u(t)\\) \\[ f&#39;(x)=\\frac{v&#39;(t)}{u&#39;(t)} \\] To see this, we have \\({v(t)=f(x(t))}\\), so differentiating with respect to \\(t\\) and using the chain rule on the right hand side, \\[ v&#39;(t)=f&#39;(x)u&#39;(t) \\] and since \\(u&#39;(t)\\neq 0\\) \\[ f&#39;(x)=\\frac{v&#39;(t)}{u&#39;(t)}. \\] In Leibniz notation this is often written as: \\[ \\frac{dy}{dx}=\\frac{\\dfrac{dy}{dt}}{\\dfrac{dx}{dt}} \\] and one can imagine “cancelling the \\(dt\\)’s”. Example 11.9 (Parametric differentiation) An ellipse \\(\\dfrac{x^2}{a^2}+\\dfrac{y^2}{b^2}=1\\) can be represented parametrically by \\({x=a\\cos(\\theta), y=b\\sin(\\theta)}\\). The derivative at a point on the ellipse corresponding to parameter value \\(\\theta\\) is: \\[\\begin{align*} \\frac{dy}{dx}&amp;=\\frac{dy/d\\theta}{dx/d\\theta}\\\\ &amp;=-\\frac{b\\cos(\\theta)}{a\\sin(\\theta)}\\\\ &amp;=-\\frac{b}{a}\\cot(\\theta). \\end{align*}\\] Second order parametric derivatives. Suppose that \\(x=u(t)\\), \\(y=v(t)\\), then \\[ \\frac{d^2y}{dx^2}=\\frac{d}{dx}\\left(\\frac{dy}{dx}\\right)=\\frac{d}{dx}\\left(\\frac{\\dfrac{dy}{dt}}{\\dfrac{dx}{dt}}\\right) \\] and we can compute this using the chain rule: \\[ \\frac{d^2y}{dx^2}=\\frac{d}{dt}\\left(\\frac{dy}{dx}\\right)\\frac{dt}{dx}. \\] Alternatively, we could use the quotient rule, but this is more complicated: \\[ \\frac{d^2y}{dx^2}=\\frac{\\frac{dx}{dt}\\frac{d^2y}{dt^2}-\\frac{dy}{dt}\\frac{d^2x}{dt^2}}{\\left( \\frac{dx}{dt}\\right)^3}. \\] "],["applications-of-differentiation.html", "Chapter 12 Applications of Differentiation 12.1 Maxima and Minima 12.2 Points of Inflection 12.3 Higher Order Derivative Test 12.4 Graph Sketching 12.5 Optimisation", " Chapter 12 Applications of Differentiation 12.1 Maxima and Minima “…nothing at all takes place in the universe in which some rule of maximum or minimum does not appear…” Leonhard Euler We quite often need to find maximum and minimum points of a function in relation to optimisation or due to some physical principle, such as the minimisation of potential energy in physics. We distinguish between local maxima/minima and global maxima/minima. A global maximum is a “peak” in the graph of a function that is higher than or equal to any other value of the function, whilst a local maximum is a “peak” that is higher than or equal to any other value in the immediate region around the peak; we have similar definitions for a global and local minimum. If we are looking at maxima/minima where there are no points (globally/locally) that are equal in value, we add the word strict maximum/minimum. Differentiation can help us to find local maxima/minima. We first mention some basic relationships between functions and their derivatives. These are all intuitively obvious (their mathematical proofs follow from the Mean Value Theorem). Fermat’s Theorem. If \\(x=a\\) is a local maximum or local minimum point for \\(f(x)\\), then \\(f&#39;(a)=0\\). The converse to this theorem is not true, e.g. for \\(f(x)=x^3\\), we have \\(f&#39;(0)=0\\), but \\(a=0\\) is clearly not a local maximum or minimum for \\(f\\). We use the term critical point or stationary point for a point \\(a\\) at which the derivative \\(f&#39;(a)=0\\). Constant Function. If \\(f&#39;(x)=0\\) for all \\(x\\) then \\(f\\) is constant (that is, \\(f(x)=c\\) for some constant \\(c\\)). Constant Difference. If \\(f\\) and \\(g\\) are such that \\(f&#39;(x)=g&#39;(x)\\) for all \\(x\\), then there is some constant \\(c\\) such that \\(f=g+c\\) (geometrically this says that \\(f\\) is a vertical translation of \\(g\\)). Functions with Positive or Negative Derivatives. If \\(f&#39;(x)&gt;0\\) for all \\(x\\), then \\(f\\) is strictly increasing. If \\(f&#39;(x)&lt;0\\) for all \\(x\\), then \\(f\\) is strictly decreasing. The converse is not true: consider again \\(f(x)=x^3\\) which is strictly increasing but has \\(f&#39;(0)=0\\). Note that in order to have a local maximum or minimum at a point \\(x=a\\) it is a requirement that \\(f&#39;(a)=0\\), but this is not a sufficient condition to gaurantee we have a maximum or minimum at \\(a\\). We can determine if a critical point is a local maximum or minimum using the so-called second derivative test. Theorem 12.1 (Second Derivative Test for Maxima and Minima) Suppose \\(f&#39;(a)=0\\) (i.e. \\(a\\) is a critical point of \\(f\\)). If \\(f&#39;&#39;(a)&gt;0\\), then \\(f\\) has a strict local minimum at \\(a\\), If \\(f&#39;&#39;(a)&lt;0\\), then \\(f\\) has a strict local maximum at \\(a\\). To understand this theorem, note that if \\(f&#39;&#39;(a)&gt;0\\) then this says that \\(f&#39;(x)\\) is strictly increasing near \\(a\\) and we also know that \\(f&#39;(x)\\) passes through \\(0\\) at \\(x=a\\). This means \\(f&#39;(a-\\delta)&lt;0\\) and \\(f&#39;(a+\\delta)&gt;0\\) for an arbitrary small number \\(\\delta&gt;0\\); that is, we have a negative gradient to the left of \\(a\\) and a positive gradient to the right of \\(a\\), hence \\(a\\) must be a local minimum of the function. A similar argument holds for \\(f&#39;&#39;(a)&lt;0\\) and \\(a\\) being a local maximum. Warning! The function \\(g(x)=x^4\\) has a strict global (and hence also local) minimum at \\(0\\), where \\(g&#39;(0)=0\\) but we have \\(g&#39;&#39;(0)=0\\), hence the converse to the Second Derivative Test is not true. 12.2 Points of Inflection An inflection point \\(x=a\\) is a point where the tangent line to a graph of a function \\(f\\) at \\((a,f(a))\\) crosses the graph at \\((a,f(a))\\). This corresponds to a point where the graph changes from being convex to concave (or vice versa). Figure 12.1: Inflection points It is necessary that the second derivative has different signs to the left and right of \\(a\\) for it to be an inflection point, and hence \\(f&#39;&#39;(a)=0\\). But note that \\(f&#39;&#39;(a)=0\\) alone is not sufficient for \\(a\\) to be an inflection point (for example, \\(f(x)=x^4\\) at \\(x=0\\) has derivative \\(f&#39;&#39;(0)=0\\), but this is a minimum). Note that we do not need \\(f&#39;(a)=0\\), so a point of inflection is not necessarily a critical point. Having a nonzero third order derivative at a point \\(a\\) where \\(f&#39;&#39;(a)=0\\) ensures that the second derivative has different signs to the left and right of \\(a\\). Theorem 12.2 (Third Derivative Test for Inflection Points) If \\(f&#39;&#39;(a)=0\\) and \\(f&#39;&#39;&#39;(a)\\neq 0\\) then \\(a\\) is a point of inflection Warning! The function \\(g(x)=x^5\\) has a point of inflection at \\(0\\), but with \\(g&#39;&#39;(0)=0\\) and \\(g&#39;&#39;&#39;(0)=0\\). This demonstrates that the above criteria on the third derivative is sufficient but not necessary. 12.3 Higher Order Derivative Test We can generalise the above derivative tests to determine whether critical points are maxima, minima or inflection points based on higher order derivatives. Theorem 12.3 (Higher Order Derivative Test) Let \\(f\\) be a sufficiently differentiable function on an interval containing the point \\(a\\). If with \\(n&gt;1\\) we have \\(f^{(i)}(a)=0\\) for \\(i=1,\\dotsc,{n-1}\\) but \\(f^{(n)}\\neq 0\\), then If \\(n\\) is even and \\(f^{(n)}&lt;0\\), then \\(a\\) is a strict local maximum point If \\(n\\) is even and \\(f^{(n)}&gt;0\\), then \\(a\\) is a strict local minimum point If \\(n\\) is odd and \\(f^{(n)}&lt;0\\), then \\(a\\) is a strictly decreasing point and an inflection point If \\(n\\) is odd and \\(f^{(n)}&gt;0\\), then \\(a\\) is a strictly decreasing point and an inflection point Note that the test can fail if the higher order derivatives at \\(a\\) cease to exist before they become non-zero, or if the derivatives of \\(f\\) at \\(a\\) of all orders are zero i.e. \\(f^{(i)}(a)=0\\) for all \\(i\\). 12.4 Graph Sketching We now have a lot of tools to help us understand the nature of a function and to sketch graphs. We can analyse a function \\(f\\) by looking at: the values \\(x\\) where \\(f(x)=0\\), i.e. where the graph crosses the \\(x\\)-axis the value of \\(f(0)\\), i.e. where the graph crosses the \\(y\\)-axis the critical points of \\(f\\) (possibly local maxima/minima, inflection points) and the value of \\(f\\) at these critical points the sign of \\(f&#39;\\) the location and nature of any vertical asymptotes if the function is periodic 12.5 Optimisation Optimisation is the process of finding the “best” inputs, that maximise or minimise some objective function. For suitable objective functions we can use our derivative tests to find these inputs. Here we shall look at an example and give a general method for solving such problems. Example 12.1 (A fencing problem) A rectangular storage compound needs to be built on a building site. An existing wall is to be used as one side of the compound and the other 3 walls will be built with fencing. There is 100m of fencing available. What is the maximum storage area that can be built and what are the lengths of the sides? Let the side opposite the wall have length \\(y\\) and the other two sides length \\(x\\). The length of the fencing will be the perimeter of these three sides, hence \\[2x+y=100 m\\] We want to maximise the area, hence our objective function is \\[A=x\\times y.\\] We use the first equation to find \\(y\\) in terms of \\(x\\): \\[y=100-2x\\] then \\[A=x(100-2x)=100x-2x^2.\\] To maximise, we need to find the solution for \\(\\frac{dA}{dx}=0\\): \\[\\frac{dA}{dx}=100-4x=0\\quad\\implies\\quad x=25 m\\] We check this is a maximum at \\(x=25\\). We have \\[\\frac{d^2A}{dx^2}=-4\\] which is indeed negative, hence we have a maximum at \\(x=25\\). Substituting back into our equations for \\(y\\) and \\(A\\) we have: \\[\\begin{align*} x&amp;=25m\\\\ y&amp;=50m\\\\ A&amp;=1250 m^2. \\end{align*}\\] Methodology Obtain an expression for the quantity that must be maximised or minimised; Ensure that the expression is in terms of just one unknown variable; Get the resulting expression into a form that can be differentiated; Differentiate the function and set equal to zero; Solve for the unknown variable, i.e. find the critical points; Find the second derivative of the function and check which critical points are maxima/minima; Substite the points back into the objective function (in terms of the one variable) to find the values at these points – select the largest/smallest. "],["sequences.html", "Chapter 13 Sequences 13.1 Limits of Sequences", " Chapter 13 Sequences A sequence is an ordered list of objects (usually numbers, but also vectors, matrices or any other mathematical object). It may be finite or infinite. Some sequences of numbers: A finite sequence: \\(2, 4, 6, 8\\) Square numbers: \\(1, 4, 9, 16,\\dotsc\\) Fibonacci: \\(0, 1, 1, 2, 3, 5, 8, 13, 21, 34,\\dotsc\\) What comes next?: \\(1, 11, 21, 1211, 111221, 312211,\\dotsc\\) (the “look-and-say’’ sequence) We write \\[\\begin{align*} (a_n)&amp;_{n=1}^{\\infty}\\quad\\text{or simply}\\quad(a_n)\\quad\\text{for the infinite sequence of numbers }a_1,a_2,a_3,\\dotsc\\\\ &amp;\\big\\uparrow_\\text{read as &#39;&#39;from $n$ equals $1$ to infinity&#39;&#39;} \\end{align*}\\] We say that the number \\(a_1\\) is the first term of the sequence, \\(a_2\\) the second term etc. and \\(a_n\\) is a general term of the sequence – the \\(n^\\text{th}\\) term. Note that a sequence of real numbers can also be viewed as a function from the natural numbers to the real numbers with \\(f(n)=a_n\\). Two ways of specifying the terms of a sequence are: 1. By a formula for each term depending on \\(n\\), e.g. \\[ a_n=n^2-1\\quad\\text{corresponding to the sequence}\\quad 0,3,8,15,24,\\dotsc \\] By a recursive forumla, i.e. a formula that depends on previous terms in the sequence, e.g. the Fibonacci sequence can be defined by \\(F_1=0,F_2=1\\) together with \\[ F_n=F_{n-1}+F_{n-2}\\qquad\\text{for }n&gt;2 \\] Two commonly encountered forms of sequences are arithmetic sequences and geometric sequences. An arithmetic sequence is given by a formula of the form \\[a_n=a+(n-1)d\\] where \\(a\\) is the first term and \\(d\\) is the common difference. For example, \\[4,7,10,13,\\dotsc\\] is given by the formula \\[a_n=4+(n-1)3.\\] A geometric sequence is given by a formula of the form \\[a_n=ar^{n-1}\\] where \\(a\\) is the first term and \\(r\\) is the common ratio. For example, \\[2, 1, \\frac{1}{4}, \\frac{1}{8},\\dotsc\\] is given by the formula \\[a_n=2\\times \\left(\\frac{1}{2}\\right)^{n-1}.\\] 13.1 Limits of Sequences Consider the sequence defined by the following formula: \\[ a_n=\\frac{n}{n+1}. \\] The first three terms are \\[ a_1=\\frac{1}{2},\\quad a_2=\\frac{2}{3},\\quad a_3=\\frac{3}{4}. \\] The terms corresponding to \\(n=100,101,102\\) are \\[ a_{100}=\\frac{100}{101},\\quad a_{101}=\\frac{101}{102},\\quad a_{102}=\\frac{102}{103} \\] which are close to \\(1\\), for example \\(\\frac{100}{101}\\) is different to \\(1\\) by just \\(\\frac{1}{101}\\). We can see that as \\(n\\) grows, \\(a_n\\) becomes closer and closer to the value \\(1\\); we say that the sequence \\(\\{a_n\\}_{n=1}^\\infty\\) has limit \\(1\\). Intuitively a sequence has a limit \\(L\\) if its terms get closer and closer to \\(L\\). More mathematically we say that a sequence \\((a_n)\\) converges to a limit \\(L\\) if \\(a_n\\) is “close to” \\(L\\) “for all large positive integers \\(n\\)”. We give a formal mathematical definition below. The following examples introduce some more terminology. The harmonic sequence \\[ 1, \\frac{1}{2}, \\frac{1}{3},\\dotsc \\] defined by the formula \\[ a_n=\\dfrac{1}{n} \\] is continually decreasing and converges to zero. The Fibonacci sequence does not converge to a limit and keeps growing without bound \\[ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34,\\dotsc \\] If a sequence does not converge we say that it diverges. Because this sequence continues to grow we say it diverges to infinity. The sequence \\[ a_n=(-1)^{n+1} \\] has terms \\(1,-1,1,-1,1,\\dotsc\\) and does not converge to a single value; because it does not converge it diverges, but because it does not simply grow to \\(+\\infty\\) or \\(-\\infty\\) we say that it . The formal definition of a limit for a sequence is: Definition 13.1 (definition name) Let \\((a_n)_{n=1}^{\\infty}\\) be a sequence of real numbers. We say that the sequence has limit \\(L\\) if for every \\(\\varepsilon&gt;0\\) there exists a positive integer \\(N\\) such that if \\(n\\geq N\\) then \\[ \\left|a_n-L\\right|&lt;\\varepsilon. \\] This definition works as follows: by choosing a small value for \\(\\varepsilon\\) we set how close the terms of the sequence need to be to \\(L\\) and the definition then says we must be able to find an integer \\(N\\) such that all terms of the sequence \\(a_n\\) for \\(n\\geq N\\) (i.e. the values \\(a_N,a_{N+1},a_{N+2},\\dotsc\\)) are within \\(\\varepsilon\\) of \\(L\\); we then need to be able to do this for any value of \\(\\varepsilon\\) – this is how we formally state that the terms must be getting closer to \\(L\\). (The Greek lower-case letter epsilon \\(\\varepsilon\\) is commonly used to denote a “small” number in mathematics.) If the sequence \\((a_n)_{n=1}^{\\infty}\\) has limit \\(L\\), we write \\[ \\lim\\limits_{n\\to\\infty}a_n=L \\] read as “the limit of \\(a_n\\) as \\(n\\) approaches infinity is \\(L\\)”, or alternatively \\[ a_n\\to L\\text{ as }n\\to\\infty \\] read as “\\(a_n\\) tends to the limit \\(L\\) as \\(n\\) tends to infinity”. Note that for any arithmetic sequence \\(a_n=a+(n-1)d\\) we have: \\(d&gt;0\\): divergence to \\(\\infty\\); \\(d=0\\): convergence to \\(a\\) (all terms are \\(a\\)); \\(d&lt;0\\): divergence to \\(-\\infty\\). For geometric sequences \\(a_n=ar^n\\) with \\(a\\neq 0\\) we have: \\(r&gt;1\\): divergence to \\(\\infty\\) if \\(a&gt;0\\), or \\(-\\infty\\) if \\(a&lt;0\\); \\(r=1\\): convergence to \\(a\\) (the constant sequence with all terms \\(a\\)); \\(-1 &lt; r &lt; 1\\): convergence to \\(0\\) (the constant sequence with all terms \\(0\\) in the case \\(r=0\\)); \\(r\\leq-1\\): an oscillating sequence. "],["series.html", "Chapter 14 Series 14.1 Sigma Notation 14.2 Infinite Series 14.3 Power Series 14.4 Taylor Series", " Chapter 14 Series A series is the sum of the terms of a sequence. 14.1 Sigma Notation Given a sequence \\(a_1,a_2,a_3,\\dotsc\\) the sum of the first \\(n\\) terms is written \\[ \\sum_{k=1}^{n}a_k=a_1+a_2+\\dotsb+a_n \\] (read as “the sum of \\(a_k\\) from \\(k=1\\) to \\(n\\)”). Such a sum is called a series. Note that the summation index \\(k\\) is a dummy index — we can happily replace it with another label. We can also shift the starting value of the index, that is, we can reindex the series; this may be useful to give a simpler expression for the series. For example, we can equivalently write the above series as \\[\\begin{align*} \\sum_{j=1}^{n}a_j,&amp;&amp;\\sum_{k=0}^{n-1}a_{k+1},&amp;&amp;\\sum_{p=100}^{n+99}a_{p-99}, \\end{align*}\\] since these all sum the same values from the sequence. We are quite often interested in summing the terms of a given sequence. The sum of the first \\(n\\) terms of an arithmetic sequence is: \\[ \\sum_{k=1}^{n}a_k=\\frac{n}{2}\\left(2a+(n-1)d\\right) \\] and the sum of the first \\(n\\) terms is: \\[ \\sum_{k=1}^{n}a_k=\\frac{a(1-r^n)}{1-r} \\] for \\(r\\neq1\\), and \\[ \\sum_{k=1}^{n}a_k=na \\] for \\(r=1\\). 14.2 Infinite Series Consider the sequence defined by \\(a_n=\\dfrac{1}{2^n}\\). What is the sum of all the terms of this sequence? That is, the sum \\[ \\frac{1}{2}+\\frac{1}{2^2}+\\frac{1}{2^3}+\\frac{1}{2^4}+\\dotsb=\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}+\\frac{1}{16}+\\dotsb \\] We can view the terms of the sequence geometrically as fractions of a square with total area one. Figure 14.1: The series \\(\\sum_{n=1}^\\infty\\dfrac{1}{2^n}\\) can be viewed as summing up parts of a square with total area one. We are effectively summing up parts of the square, whereby we keep adding half of what is left. Since after an “infinite number” of terms we will have included all parts of the square, it looks like \\[ \\frac{1}{2}+\\frac{1}{2^2}+\\frac{1}{2^3}+\\frac{1}{2^4}+\\dotsb=1. \\] This example is a geometric series, which is a sum of a geometric sequence. Here we have first term \\(a=\\frac{1}{2}\\) and common ratio \\(r=\\frac{1}{2}\\). The sum of the first \\(n\\) terms of a geometric series is (for \\(r\\neq1\\)): \\[ S_n=\\sum_{k=1}^{n}ar^{k-1}=\\frac{a(1-r^n)}{1-r} \\] so we have \\[ S_n=\\frac{\\dfrac{1}{2}\\left(1-\\dfrac{1}{2^n}\\right)}{1-\\dfrac{1}{2}}=\\left(1-\\dfrac{1}{2^n}\\right). \\] The sum of the first \\(n\\) terms of a series is called the \\(n^\\text{th}\\) partial sum of the series. We define the sum of an infinite series to be the limit of the partial sums. In this example we have \\[ \\lim\\limits_{n\\to\\infty}S_n=\\lim\\limits_{n\\to\\infty}\\left(1-\\dfrac{1}{2^n}\\right)=1. \\] Infinite series can be convergent (sum to a finite value) or divergent (to \\(+\\infty\\) or \\(-\\infty\\), or oscillate). Example 14.1 (Some infinite series) Consider the sum of the first \\(n\\) natural numbers: \\[ T_n=\\sum_{k=1}^{n}k=1+2+3+\\dotsb +n \\] This is an , i.e. the sum of an arithmetic sequence. The partial sum can be written as the general formula \\[ T_n=\\frac{n(n+1)}{2}. \\] We have \\(\\displaystyle\\sum_{k=1}^\\infty k=\\lim\\limits_{n\\to\\infty} T_n=\\infty\\), and hence this infinite series diverges to infinity. Note that all arithmetic series diverge to \\(\\pm\\infty\\) (why?). Let \\(a_n=(-1)^n\\) and consider the infinite series \\(\\sum_{n=1}^{\\infty}a_n\\). This is a geometric series with \\(a=-1\\) and \\(r=-1\\). We have the partial sums \\[\\begin{align*} S_1&amp;=-1\\\\ S_2&amp;=-1+1=0\\\\ S_3&amp;=-1+1-1=-1 \\end{align*}\\] with general formula \\[ S_n= \\begin{cases} -1&amp;\\text{if $n$ is odd,}\\\\ 0&amp;\\text{if $n$ is even.} \\end{cases} \\] Hence \\(\\lim\\limits_{n\\to\\infty}S_n\\) does not exist (it oscillates) and the infinite series diverges. We have seen an example of a convergent geometric series \\(\\sum_{n=1}^\\infty\\frac{1}{2^n}=1\\), a divergent geometric series \\(\\sum_{n=1}^{\\infty}(-1)^n\\) which oscilates$. We can also have divergent series that diverge to \\(\\pm\\infty\\). We’ll now look at the general behaviour of a geometric series \\(\\sum_{k=1}^{\\infty}ar^{k-1}\\). Recall that the partial sums are given by \\[ S_n=\\sum_{k=1}^{n}ar^{k-1}=\\frac{a(1-r^n)}{1-r} \\] and note that the only term that depends on \\(n\\) is \\(r^n\\). If \\(-1&lt; r &lt;1\\), then \\(r^n\\to 0\\) as \\(n\\to infty\\) and the sum will be convergent with \\[ S=\\lim\\limits_{n\\to\\infty}S_n=\\frac{a}{1-r} \\] If \\(r=1\\) then the partial sums become \\(S_n=\\sum_{k=1}^{n}a=na\\), which clearly diverges to infinity. If \\(r&gt;1\\), then \\(r^n\\)$ and hence \\(S_n\\to\\infty\\). If \\(r=-1\\), then we have a similar situation to the previous example with \\(S_n\\) alternating between \\(+a\\) and \\(-a\\). The series oscillates (a type of divergence). If \\(r&lt;-1\\) then we alternately add and subtract numbers which are getting bigger and bigger in absolute value, so the series oscillates between increasingly larger positive and negative values. The \\[ \\sum_{n=1}^{\\infty}\\frac{1}{n} \\] diverges to infinity, that is, \\(\\lim\\limits_{n\\to\\infty}\\sum_{k=1}^{n}\\frac{1}{k}=\\infty\\). To see this we collect together some terms \\[ \\sum_{n=1}^{\\infty}\\frac{1}{n}=1+\\frac{1}{2}+\\left( \\frac{1}{3}+\\frac{1}{4}\\right) +\\left( \\frac{1}{5}+\\frac{1}{6}+\\frac{1}{7}+\\frac{1}{8}\\right) +\\dotsb \\] and by comparing the terms in brackets with those in the following series, we can see that the harmonic series must be greater than this series \\[\\begin{align*} &amp;1+\\frac{1}{2}+\\left( \\frac{1}{4}+\\frac{1}{4}\\right) +\\left( \\frac{1}{8}+\\frac{1}{8}+\\frac{1}{8}+\\frac{1}{8}\\right) +\\dotsb\\\\ &amp;=1+\\frac{1}{2}+\\left( \\frac{1}{2}\\right) +\\left(\\frac{1}{2}\\right) +\\dotsb \\end{align*}\\] Since the second series clearly diverges to infinity, so must the harmonic series. It actually diverges incredibly slowly — here are some of the partial sums: \\[ S_{10}\\approx 2.93,\\qquad S_{20}\\approx 3.40,\\qquad S_{1000}\\approx 7.49,\\qquad S_{100000}\\approx 12.09 \\] This example shows in particular that even if the terms are shrinking towards zero the series need not converge — the speed at which they shrink is also important. On the other hand, if the terms in a series don’t shrink to zero then the series will obviously diverge! Interestingly a series of the form \\[ \\sum_{n=1}^{\\infty}\\frac{1}{n^p} \\] where \\(p\\) is a real constant diverges if \\(p\\leq 1\\) and converges if \\(p&gt;1\\), so the terms \\(\\frac{1}{n}\\) in the harmonic series are decreasing only just too slowly to give a convergent series. In particular it can be shown that \\[ \\sum_{n=1}^{\\infty}\\frac{1}{n^2}=\\frac{\\pi}{6} \\] This curious result was found by Leonhard Euler in 1734: see The Basel Problem. It is not easy to solve with the techniques considered in these notes and is best tackled with more powerful mathematical tools, like Fourier Series. 14.3 Power Series Let \\(t\\) be a fixed real number. A power series is a series of the form \\[ \\sum_{n=0}^{\\infty}a_n(x-t)^n \\] where \\((a_n)_{n=0}^{\\infty}\\) is a sequence and \\(x\\) is a real number. Power series can be thought of as “polynomials with infinite degree”, but technically they are not polynomials, which must have finite degree. Consider the power series \\[ \\sum_{n=0}^{\\infty}\\frac{x^n}{n+1}=1+\\frac{x}{2}+\\frac{x^2}{3}+\\frac{x^3}{4}+\\dotsb \\] For \\(x=-1\\) this gives the alternating harmonic series, which converges. For \\(x=1\\) this gives the harmonic series, which diverges. So convergence of a power series can depend on the value of \\(x\\). The binomial series is another example of a power series \\[ (1+x)^\\alpha=1+\\alpha x+\\frac{\\alpha(\\alpha-1)}{2!}x^2+\\frac{\\alpha(\\alpha-1)(\\alpha-2)}{3!}x^3+\\dotsb \\] which converges for any real value of \\(\\alpha\\) as long as \\(|x|&lt;1\\). 14.4 Taylor Series 14.4.1 Linearisation It is often useful to approximate a complicated function by something that is easier to work with. The simplest way to do this at a differentiable point \\(a\\) of a function \\(f\\) is to approximate the values of the function \\(f\\) near \\(a\\) by the tangent line \\[ y=f(a)+f&#39;(a)(x-a). \\] In this context, we call the tangent line the of \\(f\\) at \\(a\\), and write it as the function \\[ L_a(x)=f(a)+f&#39;(a)(x-a). \\] At values of \\(x\\) sufficiently close to \\(a\\), \\(L_a(x)\\) gives a good approximation to \\(f(x)\\). We can see this should be true since the derivative is obtained as a limit of the slope of secant lines passing through nearby points and since this limit converges, if we zoom in on the graph of \\(f\\) at \\((a,f(a))\\) it should look more and more like a straight line, i.e. it is approximately linear. Figure 14.2: Zooming in on the tangent line at \\((a,f(a))\\). More mathematically we have \\[ \\lim\\limits_{x\\to a}\\frac{f(x)-L_a(x)}{(x-a)}=\\lim\\limits_{x\\to a}\\frac{f(x)-f(a)}{x-a}-f&#39;(a)=0 \\] where the second equality uses the definition of the derivative. This says that as \\(x\\) approaches \\(a\\) not only does the difference \\(f(x)-L_a(x)\\) become small, but it becomes small faster than \\(x-a\\). Whether or not the linearisation is a good enough approximation depends on the application, and in particular how wide a range of \\(x\\) values we are interested in near the point. Next we look at a better (but necessarily more complicated) way of obtaining a local approximation to a differentiable function by using polynomials. 14.4.2 Taylor Polynomials Consider a general polynomial function \\[ p(x)=a_0+a_1 x+\\dotsb +a_n x^n. \\] The coefficients \\(a_k\\) can be expressed in terms of the value of \\(p\\) and its derivatives at \\(0\\) as follows \\[\\begin{align*} p(x)&amp;=a_0+a_1 x+\\dotsb +a_n x^n &amp; p(0)&amp;=a_0\\\\ p&#39;(x)&amp;=a_1+2a_2x+\\dotsb +n a_n x^{n-1}&amp; p&#39;(0)&amp;=a_1\\\\ p&#39;&#39;(x)&amp;=2a_2+6a_3 x+\\dotsb +n(n-1)a_n x^{n-2}&amp;p&#39;&#39;(0)&amp;=2a_2 \\end{align*}\\] and in general \\[ p^{(k)}(0)=k!a_k\\qquad\\text{or}\\qquad a_k=\\frac{p^{(k)}(0)}{k!} \\] for \\(0\\leq k \\leq n\\), with the definitions \\(p^{(0)}=p\\) and \\(0!=1\\). We could perform a similar analysis (exercise) for a polynomial in \\((x-a)\\) \\[ p(x)=a_0+a_1(x-a)+\\dotsb +a_n (x-a)^n \\] and arrive at \\[ a_k=\\frac{p^{(k)}(a)}{k!}. \\] Suppose that \\(f\\) is any function such that the derivatives \\(f^{(1)}(a),\\dotsc, f^{(n)}(a)\\) all exist. Then letting \\[ a_k=\\frac{f^{(k)}(a)}{k!} \\] we define the Taylor Polynomial of degree \\(n\\) for \\(f\\) at \\(a\\) as \\[ P_{n,a}(x)=a_0+a_1(x-a)+\\dotsb +a_n (x-a)^n. \\] The Taylor Polynomial \\(P_{n,a}\\) has been defined so that it has the same \\(k^\\text{th}\\) derivative as \\(f\\) at \\(a\\): \\[ P_{n,a}^{(k)}(a)=f^{(k)}(a)\\qquad\\text{for all }0\\leq k\\leq n \\] (Exercise: check this) and in fact is clearly the unique polynomial with degree \\(\\leq n\\) with this property. Also note that the first degree Taylor Polynomial \\(P_{1,a}\\) is the same as the linearisation \\(L_a\\). We now want to show that a Taylor Polynomial can provide a good approximation to a function and in particular that the higher the degree of the polynomial the better the approximation. Example 14.2 (Taylor Polynomial) We find the \\(n^\\text{th}\\) degree Taylor Polynomial of \\(\\sin\\) at \\(0\\). The first few derivatives at \\(0\\) are: \\[\\begin{align*} \\sin(0)&amp;=0\\\\ \\sin^{(1)}(0)&amp;=\\cos(0)=1\\\\ \\sin^{(2)}(0)&amp;=-\\sin(0)=0\\\\ \\sin^{(3)}(0)&amp;=-\\cos(0)=-1\\\\ \\sin^{(4)}(0)&amp;=\\sin(0)=0 \\end{align*}\\] and we can see that higher order derivatives will cycle through \\(0,1,0,-1\\). The coefficients \\(a_k=\\dfrac{\\sin^{(k)}(a)}{k!}\\) are therefore: \\(0,1,0,-\\frac{1}{3!},0,\\frac{1}{5!},\\dotsc\\), so there are no even powers of \\(x\\). We can list just odd numbers by considering \\(2n+1\\) for any natural number \\(n\\). Then the Taylor Polynomial \\(P_{2n+1,0}\\) of degree \\({2n+1}\\) for \\(\\sin\\) at \\(0\\) is: \\[ P_{2n+1,0}(x)=x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\frac{x^7}{7!}+\\dotsc+(-1)^n\\frac{x^{2n+1}}{(2n+1)!} \\] By plotting the first few Taylor polynomials of \\(\\sin\\) at \\(0\\), it appears that adding more terms provides a better approximation of \\(\\sin\\) Figure 14.3: First few Taylor polynomials of \\(\\sin(x)\\) at \\(0\\) As another example, consider the \\(n^\\text{th}\\) degree Taylor Polynomial of \\(e^x\\) at \\(0\\): \\[ P_{n,0}=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+\\dotsb+\\frac{x^n}{n!} \\] If we plot \\(P_{n,0}\\) for the first few \\(n\\) it does indeed appear that \\(P_{n,0}(x)\\) is closer to \\(e^x\\) at zero for higher values of \\(n\\). We will now make this idea more precise. Theorem 14.1 (theorem name) Let \\(P_{n,a}\\) be the Taylor polynomial of degree \\(n\\) of a function \\(f\\) at a point \\(a\\). Then \\[ \\lim\\limits_{x\\to a}\\frac{f(x)-P_{n,a}(x)}{(x-a)^n}=0, \\] that is, \\(P_{n,a}\\) is equal to \\(f\\) up to order \\(n\\) at \\(a\\). Moreover, \\(P_{n,a}\\) the unique polynomial of degree \\(n\\) with this property. This says that \\(P_{n,a}\\) is the unique degree \\(n\\) polynomial that best approximates \\(f\\) at \\(a\\), in the sense that as \\(x\\to a\\) the difference between \\(f\\) and \\(P_{n,a}\\) tends to zero relatively quickly (faster than \\((x-a)^n\\), which goes to zero faster for increasing values of \\(n\\)). So, the higher the degree of the Taylor Polynomial the faster the values converge to the original function at the point \\(a\\). 14.4.3 Taylor Series So far we have considered approximating functions by Taylor polynomials, that is, for a function \\(f\\) we defined the \\(n^\\text{th}\\) degree Taylor polynomial \\[ P_{n,a}(x)=\\sum_{k=0}^{n}\\frac{f^{(k)}(a)}{k!}(x-a)^k. \\] We saw that taking higher degree Taylor polynomials can give better approximations of a function. Now, what if we let \\(n\\to\\infty\\)? That is, if we consider the power series \\[ \\sum_{k=0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k=\\lim\\limits_{n\\to\\infty}P_{n,a}(x). \\] We might expect that in the limit we in fact get \\[ \\displaystyle\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k=f(x). \\] Is this the case? The short answer is: not always, but it does for “everyday” functions at least on some interval around \\(a\\) (such as trigonometric functions, hyperbolic functions, roots, exponentials and logarithms). We call a power series of the form \\[ \\sum_{k=0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] the Taylor series for \\(f\\) at \\(a\\). When a Taylor series is formulated at the point \\(a=0\\), it is often referred to as a Maclaurin series. A Taylor series is a type of power series. In the previous section we saw that a power series might only converge for certain values of \\(x\\). Generally, the Taylor series of a functino will either converge to the function for all values of \\(x\\) or within some interval. Here we list some commonly used Taylor series derived at \\(a=0\\), which you should familiarise yourself with. The Validity column states the interval around \\(0\\) for which the Taylor series is equal to the original function. Series Validity \\(\\frac{1}{1-x}=1+x+x^2+x^3+\\dotsb=\\sum_{n=0}^{\\infty}x^n\\) \\(-1&lt; x &lt; 1\\) \\(e^x=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+\\dotsb = \\sum_{n=0}^{\\infty}\\frac{x^n}{n!}\\) \\(-\\infty\\leq x \\leq \\infty\\) \\(\\cos(x)=1-\\frac{x^2}{2!}+\\frac{x^4}{4!}-\\dotsb= \\sum_{n=0}^{\\infty}(-1)^n\\frac{x^{2n}}{(2n)!}\\) \\(-\\infty\\leq x \\leq \\infty\\) \\(\\sin(x)=x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\dotsb=\\sum_{n=0}^{\\infty}(-1)^n\\frac{x^{2n+1}}{(2n+1)!}\\) \\(-\\infty\\leq x \\leq \\infty\\) \\(\\ln(x+1)=x-\\frac{x^2}{2!}+\\frac{x^3}{3!}-\\dotsb= \\sum_{n=1}^{\\infty}(-1)^{n+1}\\frac{x^{n}}{n!}\\) \\(-1&lt; x \\leq 1\\) \\(\\tan^{-1}(x)=x-\\frac{x^3}{3}+\\frac{x^5}{5}-\\dotsb= \\sum_{n=0}^{\\infty}(-1)^n\\frac{x^{2n+1}}{2n+1}\\) \\(-1\\leq x \\leq 1\\) "],["integration.html", "Chapter 15 Integration 15.1 Indefinite Integrals 15.2 Definite Integrals 15.3 Integration by Substitution", " Chapter 15 Integration Integration is (almost) the reverse process of differentiation. Geometrically, differentiation corresponds to finding the gradient of a tangent line, whilst integration corresponds to finding the area under a curve. The precise relationship between differentation and integration will be given later by the Fundamental Theorem of Calculus. 15.1 Indefinite Integrals Consider the following function \\[F(x)=x^2.\\] Its derivative is \\[\\frac{dF(x)}{dx}=2x.\\] If integration is the reverse process of differentation, then we might expect the integral of \\(f(x)=2x\\) to be \\(F(x)=x^2\\). However, note that the derivative of \\[G(x)=x^2+3\\] is also \\[\\frac{dG(x)}{dx}=2x=f(x).\\] Hence, it would be impossible to know if \\(F\\) or \\(G\\) was the original function. In fact, any function of the form \\(2x+C\\) where \\(C\\) is a constant would yield the same derivative. Therefore, the best we can do is to say the integral of \\(f(x)=2x\\) has the form \\(x^2+C\\). More generally, the graphs of all functions of the form \\(F(x)+C\\) for some particular function \\(F\\) and various constants \\(C\\) are just vertical translations of one another, so we can see they would indeed have the same gradient at any point \\(x\\). If \\(\\frac{dF(x)}{dx}=f(x)\\) we say that “\\(F(x)+C\\)” with \\(C\\) an arbitrary constant is the indefinite integral of \\(f(x)\\). The operation of taking the indefinite integral of a function \\(f(x)\\) is written \\[\\int f(x) dx\\] which is read as “the integral of \\(f(x)\\) with respect to \\(x\\)”, where \\(\\int\\) denotes taking the integral and \\(dx\\) specifies that we are integrating with respect to the variable \\(x\\) (we need to specify this as some functions might have more than one variable). In this context the function \\(f(x)\\) being integrated is called the integrand. If we know a function \\(F(x)\\) whose derivative is \\(f(x)\\) then we can immediately find the indefinite integral as \\(F(x)+C\\). A function \\(F\\) such that \\(\\frac{dF(x)}{dx}=f(x)\\) is called an anti-derivative of \\(f\\). 15.1.1 Standard Integrals Some basic integrals obtained from differentiating well known functions \\[\\begin{eqnarray} &amp;&amp; \\frac{d }{d x} \\sin(x) = \\cos(x) \\; \\to \\; \\int \\cos(x) d x = \\sin(x) +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\cos(x) = -\\sin(x) \\; \\to \\; \\int \\sin(x) d x = -\\cos(x) +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\tan(x) = \\frac{1}{\\cos^2(x)} \\; \\to \\; \\int \\frac{1}{\\cos^2(x)} d x = \\tan(x) +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\cot(x) = - \\frac{1}{\\sin^2(x)} \\; \\to \\; \\int \\frac{1}{\\sin^2(x)} d x = - \\cot(x) +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\frac{x^{n+1}}{n+1} = x^n \\; \\to \\; \\int x^n \\; dx = \\frac{x^{n+1}}{n+1} +C \\; \\; \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\sin^{-1}(x) = \\frac{1}{\\sqrt{1-x^2} } \\; \\to \\; \\int \\frac{1}{\\sqrt{1-x^2} } d x = \\sin^{-1}(x) +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\tan^{-1}(x) = \\frac{1}{1+x^2} \\; \\to \\; \\int \\frac{1}{1+x^2 } d x = \\tan^{-1}(x) +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} e^x = e^x \\; \\to \\; \\int e^x = e^x +C\\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\ln |x| = \\frac{1 }{x} \\; \\to \\; \\int \\frac{1}{x} d x = \\ln |x| +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} a^x = a^x \\ln a \\; \\to \\; \\int a^x d x = \\frac{a^x}{\\ln a} +C \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\sinh(x) = \\cosh(x) \\; \\to \\; \\int \\cosh(x) d x = \\sinh(x) +C, \\nonumber \\\\ &amp;&amp; \\frac{d }{d x} \\cosh(x) = \\sinh(x) \\; \\to \\; \\int \\sinh(x) d x = \\cosh(x) +C, \\nonumber \\\\ \\end{eqnarray}\\] 15.2 Definite Integrals Integration is defined by finding the area under the curve of a function. This is achieved by taking successively more accurate approximations, in a similar way to finding the gradient of a tangent line in differentation. To find the area under a curve \\(f\\) between points \\(x=a\\) and \\(x=b\\), we can draw rectangles of some “small” width and whose height is the value of the function at some point between the edges of the rectangle. Some of these rectangles will over-estimate the area under their part of the curve and others will under-estimate the area. This error is reduced by taking thinner and thinner rectangles (and hence using a greater number of rectangles). In the limit that the width tends to \\(0\\), we obtain the area under the curve \\(f\\) between \\(a\\) and \\(b\\). When the upper limit \\(a\\) and lower limit \\(b\\) are specified like this, we have the definite integral of \\(f\\). Note that if a curve crosses the \\(x\\)-axis, then the “area under the curve” obtained from the definite integral is the area above the \\(x\\)-axis minus the area below the \\(x\\)-axis. The following is a formal definition of the integral, which is included for completeness, but can be ignored for now. Definition 15.1 (Reimann Integral) Consider a partition \\(P_n\\) of the interval \\(a\\leq x\\leq b\\) into \\(n\\) equal width subintervals \\[x_i\\leq x\\leq x_j\\] for \\(i=0,\\dotsc,n\\) with \\(x_0=a\\), \\(x_i=a+i\\Delta x\\), \\(x_n=b\\) and width \\(\\Delta x=(b-a)/n\\). For each subinterval choose a point \\(c_i\\) such that \\(x_i &lt; c_i &lt; x_j\\). Then the integral is approximated by \\[\\sum_{i=1}^{n-1}f(c_i)\\Delta x.\\] If the following limit exists independently of the choice of the points \\(c_i\\) \\[\\lim_\\limits{n\\to\\infty}\\sum_{i=1}^{n-1}f(c_i)\\Delta x\\] then we call this the integral of \\(f\\) between \\(a\\) and \\(b\\), and denote it by \\[\\int_{a}^{b}f(x)dx.\\] Theorem 15.1 (Some Properties of Definite Integrals) \\[\\begin{eqnarray} &amp;&amp;\\int_a^b [f(x)+g(x)] dx = \\int_a^b f(x) dx + \\int_a^b g(x) dx \\nonumber \\\\ &amp;&amp;\\int_a^b \\alpha f(x) dx = \\alpha \\int_a^b f(x) dx dx \\nonumber \\\\ &amp;&amp; \\int_a^b f(x) dx = -\\int_b^a f(x) dx \\nonumber \\\\ &amp;&amp;\\int_a^a f(x) dx = 0 \\nonumber \\\\ &amp;&amp; \\int_a^b f(x) dx = \\int_a^c f(x) dx + \\int_c^b f(x) dx, \\; \\text{ for any $c$ such that $a&lt;c&lt;b$} \\nonumber \\\\ &amp;&amp; \\int_a^b f(x) dx \\ge 0,\\; \\mbox{ if } \\; f(x) &gt;0, \\; a \\leq x \\leq b \\nonumber \\\\ &amp;&amp; \\int_{-a}^a f(x) dx =2 \\int_{0}^a f(x) dx ,\\; \\text{ if $f(x)= f(-x)$ (e.g. $\\cos(x)$, $x^2$)} \\nonumber \\\\ &amp;&amp; \\int_{-a}^a f(x) dx = 0 ,\\; \\text{ if $f(x)= -f(x)$ (e.g. $\\sin(x)$, $x^3$)} \\nonumber \\end{eqnarray}\\] The key theorem of calculus that links differentiation and integration is known as the Fundamental Theorem of Calculus. Theorem 15.2 (Fundamental Theorem of Calculus (FTC)) Let the function \\(f\\) be continuous on the interval from \\(a\\) to \\(b\\). Then: The function \\(g\\) defined by \\[g(x)=\\int_a^xf(x)dx\\] is continuous on the interval \\(a\\le x \\le b\\) and differentiable on the interval \\(a&lt; x &lt; b\\) with \\[\\frac{dg(x)}{dx}=f(x).\\] The definite integral of \\(f\\) from \\(a\\) to \\(b\\) is given by \\[\\int_a^bf(x)dx=F(b)-F(a)\\] where \\(F\\) is any anti-derivative of \\(f\\). To find the definite integral we can make use of part 2 of the FTC 15.2 together with the properties in 15.1. Example 15.1 (Definite Integrals) Evaluate the definite integral \\[ \\int_0^3 f(x) dx = \\int_0^3 (x^3- 6x) dx, \\] First observe that: \\[ \\frac{d}{d x} \\left( \\frac{1}{4}x^4-3 x^2 \\right)=x^3- 6x=f(x). \\] So an anti-derivative of \\(f(x)\\) is \\(F(x)=\\frac{1}{4}x^4-3 x^2\\). The fundamental Theorem of Calculus then gives \\[ \\begin{array}{ll} \\int_0^3 f(x) dx =F(3) -F(0) \\\\ \\\\ &amp; = \\frac{1}{4} 3^4-3\\,3^2 =\\frac{3}{4} 27- 27= -\\frac{27}{4} \\end{array} \\] Alternatively, using the properties of the integral and our list of standard integrals \\[\\begin{align*} \\int_0^3 (x^3- 6x) dx&amp;=\\int_0^3 x^3 dx -6\\int_0^3 x dx\\\\ &amp;=[\\frac{x^4}{4}]_0^3-6[\\frac{x^2}{2}]_0^3\\\\ &amp;=[\\frac{x^4}{4}-3x^2]_0^3\\\\ &amp;=(\\frac{81}{4}-27)-(0-0)\\\\ &amp;=\\frac{-27}{4} \\end{align*}\\] Evaluate \\[ \\int_0^\\frac{\\pi}{2} \\cos(x) dx. \\] We have: \\[\\begin{align*} \\int_0^\\frac{\\pi}{2} \\cos(x) dx&amp;=[\\sin(x)]_0^\\frac{\\pi}{2}\\\\ &amp;=\\sin\\left(\\frac{\\pi}{2}\\right)-\\sin(0)\\\\ &amp;=1 \\end{align*}\\] 15.3 Integration by Substitution Theorem 15.3 (Integration by Substitution) If $ u=u(x)$ is a differentiable function and \\(f\\) is continuous, then \\[ \\int f( u(x) ) \\frac{du(x)}{dx} dx = \\int f( u ) du \\quad \\text{indefinite integral} \\] \\[ \\int_a^b f( u(x) ) \\frac{du(x)}{dx} dx = \\int_{u(a)}^{u(b)} f(u) du \\quad \\text{definite integral}. \\] Here \\(d u\\) acts as if it is a differential: \\[ du= \\frac{du(x)}{dx} dx, \\;\\; d (c+x)=dx; \\;\\; d (x^2)=2x dx, \\;\\; d (\\sin x)= \\cos x d x, ..., \\] Example 15.2 (Integration by Substitution) Evaluate the indefinite integral \\[ \\int (3x+2)^4 dx \\] Introduce \\(u=3x+2\\), then \\(du= \\frac{du}{dx}\\; dx = 3\\; dx\\), so \\(dx=\\frac{1}{3} du\\). Now we can write \\[\\begin{align*} \\int (3x+2)^4 dx&amp;= \\int u^4 \\frac{du}{3}\\\\ &amp;=\\frac{1}{3}\\frac{u^5}{5}+C\\\\ &amp;=\\frac{1}{15}(3x+2)^5+C. \\end{align*}\\] Evaluate the indefinite integral \\[\\int \\cos(5x) dx\\] Introduce \\(u=5x\\), then \\(du = \\frac{du}{dx}\\; dx = 5\\; dx\\), so \\(dx = \\frac{1}{5} du\\). Now we can write \\[\\begin{align*} \\int \\cos(5x) dx&amp;=\\int \\cos(u)\\frac{du}{5}\\\\ &amp;=\\frac{1}{5}\\sin(u)+C\\\\ &amp;=\\frac{1}{5}\\sin(5x)+C \\end{align*}\\] Evaluate the indefinite integral \\[ \\int 2 x \\sqrt{1+x^2} dx. \\] Introduce \\(u=1+x^2\\), then \\(d u = \\frac{du}{dx}\\; d x=2x \\; dx\\) and we can write \\[\\begin{align*} \\int \\sqrt{(1+x^2)} (2x dx)&amp;=\\int {u}^{1/2} du &amp;=\\frac{2}{3} u^{3/2}+C\\\\ &amp;= \\frac{2}{3} (1+x^2)^{3/2}+C. \\end{align*}\\] Evaluate the indefinite integral \\[ \\int x^3 \\cos (2+x^4) dx. \\] Introduce \\(u=2+x^4\\), then \\(d u =\\frac{du}{dx} \\; d x=4x^3 \\; dx\\) and we can write \\begin{align*} (2+x^4) (4x^3 dx) &amp;= (u) du &amp;= (u) +C \\ &amp;= (2+x^4) +C. $$ Evaluate the definite integral \\[\\int_1^2 \\frac{-3}{2x} \\; dx.\\] Introduce \\(u=2x\\), then \\(du=\\frac{du}{dx}\\; dx = 2\\; dx\\) and we have (remembering to change the limits \\(a=1\\) and \\(b=2\\) to \\(u(a)=2\\) and \\(u(b)=4\\)) \\[\\begin{align*} \\int_1^2 \\frac{-3}{2x} \\; dx&amp;=\\frac{-3}{2}\\int_2^4\\frac{1}{u}\\; du\\\\ &amp;= [-\\frac{3}{2}\\ln(u)]_2^4\\\\ &amp;=\\frac{-3}{2}(\\ln(4)-\\ln(2))\\\\ &amp;=\\frac{-3}{2}\\ln(4/2)\\\\ &amp;=\\frac{-3}{2}\\ln(2)\\\\ &amp;=-1.04 \\text{to 2 d.p.} \\end{align*}\\] Or alternatively, rather than changing the limits we can substitute back in for \\(u\\) and use the original limits: \\[ [-\\frac{3}{2}\\ln(2x)]_1^2=-\\frac{3}{2}(\\ln(4)-\\ln(2))=\\frac{-3}{2}\\ln(2)=-1.04 \\text{to 2 d.p.} \\] "],["further-integration-techniques.html", "Further Integration Techniques 15.4 Substitution 15.5 Integration by Parts", " Further Integration Techniques 15.4 Substitution 15.5 Integration by Parts "],["blank5.html", "blank5", " blank5 "],["notation.html", "Notation", " Notation A summary of notation used in these notes. “\\(A \\implies B\\)” is read as “statement \\(A\\) implies statement \\(B\\)”. \\(\\approx\\) means approximately equal to. We often use Greek letters for mathematical symbols. Capital letter Small letter Name \\(A\\) \\(\\alpha\\) Alpha \\(B\\) \\(\\beta\\) Beta \\(\\Gamma\\) \\(\\gamma\\) Gamma \\(\\Delta\\) \\(\\delta\\) Delta \\(E\\) \\(\\epsilon\\), \\(\\varepsilon\\) Epsilon \\(Z\\) \\(\\zeta\\) Zeta \\(H\\) \\(\\eta\\) Eta \\(\\Theta\\) \\(\\theta\\), \\(\\vartheta\\) Theta \\(I\\) \\(\\iota\\) Iota \\(K\\) \\(\\kappa\\) Kappa \\(\\Lambda\\) \\(\\lambda\\) Lambda \\(M\\) \\(\\mu\\) Mu \\(N\\) \\(\\nu\\) Nu \\(\\Xi\\) \\(\\xi\\) Xi \\(O\\) \\(\\omicron\\) Omicron \\(\\Pi\\) \\(\\pi\\) Pi \\(R\\) \\(\\rho\\), \\(\\varrho\\) Rho \\(\\Sigma\\) \\(\\sigma\\) Sigma \\(T\\) \\(\\tau\\) Tau \\(\\Upsilon\\) \\(\\upsilon\\) Upsilon \\(\\Phi\\) \\(\\phi\\), \\(\\varphi\\) Phi \\(X\\) \\(\\chi\\) Chi \\(\\Psi\\) \\(\\psi\\) Psi \\(\\Omega\\) \\(\\omega\\) Omega "],["references.html", "References", " References "],["exercise-set-1.html", "Exercise Set 1", " Exercise Set 1 These exercises cover the topics of Algebra, Equations and Inequalities. Simplify each of the following (Hint: use the rules of exponents where needed). \\(x = 3pq+5pr=2qr+qp-6rp\\) \\(y = 5l^2mn+2nl^2m-3mln^2+l^2nm+4n^2ml-nm^2\\) \\(z = \\frac{(s^\\frac{1}{3})^\\frac{3}{4}\\times (t^\\frac{1}{4})^{-1}}{(t^\\frac{1}{2}\\times (s^{-\\frac{1}{4}})^{-1})}\\) Expand the brackets in each of the following and simplify the expression. \\(-4x(2x-y)(3x+2y)\\) \\((a-2b)(2a-3b)(3a-4b)\\) \\(-\\{-2[x-3(y-4)]-5(z+6)\\}\\) \\((v^3-v^2-2)(1-3v+2v^2)\\) Simplify each of the following. \\(\\frac{p}{q^3}\\div\\frac{p^3}{q}\\) \\(\\frac{a^2b}{2c}\\times\\frac{ac^2}{2b}\\div\\frac{b^2c}{2a}\\) \\(\\frac{8x^{-3}\\times 3x^2}{6x^{-4}}\\) \\(\\frac{3x}{3x^2+6x}\\) Factorise the following expressions. \\(18x^2y-12xy^2\\) \\(x^3+4x^2y-3xy^2-12y^3\\) \\(25x^2-4y^2\\) \\(3x^2-14x+8\\) \\(x^2+10x+25\\) The characteristic equation of a perfect gas is given by \\(P V = mRT\\) where \\(m\\) is the mass, \\(P\\) is the pressure, \\(V\\) is the volume, \\(T\\) is the temperature and \\(R\\) is the universal gas constant. Make temperature the subject of the formula. The airflow over a turbine blade causes drag \\(D\\), which is given by \\(D= \\frac{\\rho C v^2 A}{2}\\), where \\(\\rho\\) is fluid density, \\(C\\) is the drag coefficient, \\(v\\) is fluid velocity and \\(A\\) is the frontal area of the blade. Make the frontal area the subject of the formula. Make \\(b\\) the subject of the following formula. \\[W=\\frac{t\\sqrt{a+b^2}}{2\\pi}\\] Solve the following quadratic equations by factorisation. \\(x^2-28x-60=0\\) \\(p^2=8p-15\\) \\(-2y^2(3+y^2)=(2y^2+2y-3)(-y^2+y-4)-2\\) Solve the following quadratic equations, giving results correct to 2 d.p. \\(4x^2+x-3=0\\) \\(x^2+x=5\\) \\(x+\\frac{1}{x}=5\\) Solve the following sets of simultaneous equations. \\(3x+4y=7\\), \\(5x+6y=11\\) \\(2x+y=7\\), \\(x^2-xy=6\\) \\(x + y=2\\), \\(x^2-xy+y^2 = 1\\) Solve the following inequalities. \\(7-3x&gt;x-5\\) \\(x^2\\ge 4\\) \\(x^2-x-6&lt;0\\) \\(x^2-3x-12\\le 2x+2\\) \\(\\frac{2}{2x-1} &gt; \\frac{3}{3x+1}\\) \\(\\frac{2x}{x-5}\\le 3\\) \\(x^3-9x^2&lt;0\\) "],["exercise-set-1-answers.html", "Exercise Set 1 Answers", " Exercise Set 1 Answers These exercises cover the topics of Algebra, Equations and Inequalities. Simplify each of the following (Hint: use the rules of exponents where needed). \\(x = 3pq+5pr=2qr+qp-6rp\\) \\(y = 5l^2mn+2nl^2m-3mln^2+l^2nm+4n^2ml-nm^2\\) \\(z = \\frac{(s^\\frac{1}{3})^\\frac{3}{4}\\times (t^\\frac{1}{4})^{-1}}{(t^\\frac{1}{2}\\times (s^{-\\frac{1}{4}})^{-1})}\\) Answers: We collect the so-called “like term” (if there are any): \\[\\begin{align*} Y_1 &amp;= 3pq + 5pr - 2qr + qp -6rp = 3pq +qp +5pr - 6rp - 2qr \\\\ &amp;= 4pq - pr - 2qr \\quad\\text{ we add and/or subtract &quot;like terms&quot;} \\end{align*}\\] We collect the so-called “like term” (if there are any): \\[\\begin{align} Y_2 &amp;= 5l^2 mn + 2nl^2m - 3mln^2 + l^2nm + 4n^2ml - nm^2\\\\ &amp;= 5l^2 mn + 2nl^2m + l^2nm - 3mln^2 + 4n^2ml - nm^2\\\\ &amp;= 8l^2 mn + n^2lm - nm^2 \\quad\\text{ we add and/or subtract &quot;like terms&quot;} \\end{align}\\] We first complete the calculation in both the numerator and the denominator using the laws &amp; rules of indices: \\[\\begin{align} Y_3 &amp;= \\frac{(s^\\frac{1}{3})^\\frac{3}{4} \\times (t^\\frac{1}{4})^{-1}}{(t^\\frac{1}{2})^4 \\times (s^{-\\frac{1}{4}})^{-1}}=\\frac{s^{\\tfrac{1}{3}\\times\\frac{3}{4}}\\times t^{-\\frac{1}{4}}}{t^{\\frac{1}{2} \\times 4}\\times s^{\\frac{1}{4}}}\\\\ &amp;= \\frac{s^\\frac{1}{4}\\times t^{-\\frac{1}{4}}}{t^2 \\times s^\\frac{1}{4}}\\quad \\text{we treat the powers of the same base together using laws of indices}\\\\ &amp;= s^{\\frac{1}{4}-\\frac{1}{4}} \\times t^{-\\frac{1}{4}-2} = 1 \\times t^{-\\frac{9}{4}} = \\frac{1}{t^\\frac{9}{4}} \\end{align}\\] It is best to write your final answer with positive exponents only. Expand the brackets in each of the following and simplify the expression. \\(-4x(2x-y)(3x+2y)\\) \\((a-2b)(2a-3b)(3a-4b)\\) \\(-\\{-2[x-3(y-4)]-5(z+6)\\}\\) \\((v^3-v^2-2)(1-3v+2v^2)\\) Answers: We have three factors. First we multiply \\(-4x\\) by \\(2x-y\\), then we multiply the outcome by \\((3x + 2y)\\). \\[\\begin{align*} -4x(2x-y)(3x+2y) &amp;= (-8x^2 + 4xy)(3x+2y)\\\\ &amp; = -24x^3 - 4x^2y + 8xy^2. \\end{align*}\\] Again, we have three factors, we repeat the same process (make sure you simplify the outcome in each step, i.e. add/subtract like-terms): \\[\\begin{align*} (a-2b)(2a-3b)(3a-4b) &amp;= (2a^2 - 3ab - 4ab + 6b^2)(3a - 4b)\\\\ &amp;= (2a^2 - 7ab + 6b^2)(3a - 4b)\\\\ &amp;= 6a^3 - 8a^2 b - 21a^2b + 28ab^2 + 18b^2a - 24b^3\\\\ &amp;= 6a^3 - 29a^2b + 46b^2a - 24b^3 \\end{align*}\\] We calculate the inner brackets first, then we remove the brackets. Lastly, we add/subtract like terms: \\[\\begin{align*} -\\{ -2[x - 3(y-4)]- 5(z+6)\\} &amp;= - \\{-2[x - 3y + 12] - 5(z+6)\\}\\\\ &amp;= - (-2x + 6y - 24 -5z -30)\\\\ &amp;= -(-2x + 6y -5z -54)\\\\ &amp;= 2x - 6y + 5z +54 \\end{align*}\\] \\[\\begin{align*} (v^3 - v^2 - 2)(1 - 3v + 2v^2) &amp;= v^3 - 3v^4 + 2v^5 - v^2 + 3v^3 - 2v^4 - 2 + 6v - 4v^2\\\\ &amp;= 2v^5 - 5v^4 + 4v^3 - 5v^2 + 6v - 2 \\end{align*}\\] Simplify each of the following. \\(\\frac{p}{q^3}\\div\\frac{p^3}{q}\\) \\(\\frac{a^2b}{2c}\\times\\frac{ac^2}{2b}\\div\\frac{b^2c}{2a}\\) \\(\\frac{8x^{-3}\\times 3x^2}{6x^{-4}}\\) \\(\\frac{3x}{3x^2+6x}\\) Answers: \\[ \\frac{p}{q^3} \\times \\frac{q}{p^3} = \\frac{pq}{q^3 p^3} = \\frac{1}{p^2 q^2} \\] We first multiply the first two fractions, then we divide the outcome by the third \\[\\begin{align*} \\left(\\frac{a^2 b}{2c} \\times \\frac{ac^2}{2b}\\right)\\div \\frac{b^2c}{2a} &amp;= \\frac{a^3 b c^2}{4cb}\\div \\frac{b^2 c}{2a}\\\\ &amp;= \\frac{a^3 b c^2}{4cb}\\times \\frac{2a}{b^2c}\\\\ &amp;= \\frac{2a^4 b c^2}{4b^3 c^2} = \\frac{a^4}{2b^2} \\end{align*}\\] \\[\\begin{align*} \\frac{8x^{-3} \\times 3x^2}{6x^{-4}} &amp;= \\frac{24x^{-3+2}}{6x^{-4}}\\\\ &amp;= 4x^{-1+4}\\\\ &amp;= 4x^3 \\end{align*}\\] \\[ \\frac{3x}{3x^2 + 6x} = \\frac{3x}{3x(x+2)} = \\frac{1}{x+2} \\] Factorise the following expressions. \\(18x^2y-12xy^2\\) \\(x^3+4x^2y-3xy^2-12y^3\\) \\(25x^2-4y^2\\) \\(3x^2-14x+8\\) \\(x^2+10x+25\\) Answers: We look for the common factors between the two terms, in this case \\(6xy\\) \\[ 18x^2y - 12xy^2 = 6xy(3x-2y) \\] This expression has 4 terms, we divide them into 2 groups each of 2 terms, then factorise these groups and use the difference of two squares to factorise the term with squares \\[\\begin{align*} x^3 + 4x^2y - 3xy^2 - 12y^3 &amp;= x^2(x+4y) - 3y^2(x+4y)\\\\ &amp;= (x+4y)(x^2-3y^2)\\\\ &amp;= (x+4y)(x-\\sqrt{3}y)(x+\\sqrt{3}y) \\end{align*}\\] This is another application of the difference between two squares identity \\(a^2 - b^2 = (a-b)(a+b)\\) \\[ 25x^2 - 4y^2 = (5x+2y)(5x-2y) \\] We are looking for two factors of \\(3\\times 8 = 24\\) with summation equals to -14. These factors are \\(-12\\) and \\(-2\\) \\[\\begin{align*} 3x^2 - 14x + 8 &amp;= 3x^2 - 12x - 2x + 8\\\\ &amp;= 3x(x-4) - 2(x-4) = (x-4)(3x-2) \\end{align*}\\] Again, we are looking for two factors of 25 with summation equal to 10. Thus, these factors are 5 and 5 \\[ x^2 + 10x + 25 = (x+5)(x+5) = (x+5)^2 \\] The characteristic equation of a perfect gas is given by \\(P V = mRT\\) where \\(m\\) is the mass, \\(P\\) is the pressure, \\(V\\) is the volume, \\(T\\) is the temperature and \\(R\\) is the universal gas constant. Make temperature the subject of the formula. Answer: $$ PV = mRT$$ Divide both sides by $mR$, then $$ T = \\frac{PV}{mR}$$ The airflow over a turbine blade causes drag \\(D\\), which is given by \\(D= \\frac{\\rho C v^2 A}{2}\\), where \\(\\rho\\) is fluid density, \\(C\\) is the drag coefficient, \\(v\\) is fluid velocity and \\(A\\) is the frontal area of the blade. Make the frontal area the subject of the formula. Answer: First multiply both sides by 2, then divide both sides of the outcome by $\\rho C v^2$ \\begin{align*} D &amp;= \\frac{\\rho C v^2 A}{2}\\\\ 2D &amp;= \\rho C v^2 A\\\\ A &amp;= \\frac{2D}{\\rho C v^2} \\end{align*} Make \\(b\\) the subject of the following formula. \\[W=\\frac{t\\sqrt{a+b^2}}{2\\pi}\\] Answer: First, multiply both sides by $2\\pi$ and divide both sides by $t$, then square both sides and make $b^2$ the subject \\begin{align*} W &amp;= \\frac{t\\sqrt{a + b^2}}{2\\pi}\\\\ \\frac{2\\pi}{t} &amp;= \\sqrt{a+b^2}\\\\ \\left(\\frac{2\\pi}{t}\\right)^2 &amp;= a + b^2\\\\ b^2 &amp;= \\frac{4\\pi^2}{t^2} - a \\end{align*} Now it is necessary to take the square root of both sides, recalling that there is both a positive and a negative root: $$ b = \\pm \\sqrt{\\frac{4\\pi^2}{t^2} - a} $$ Solve the following quadratic equations by factorisation. \\(x^2-28x-60=0\\) \\(p^2=8p-15\\) \\(-2y^2(3+y^2)=(2y^2+2y-3)(-y^2+y-4)-2\\) Answers: Factorise by inspection into the form \\((x+)(x+b)\\) where \\(ab = -60\\) and \\(a + b = -28\\) \\[\\begin{align*} x^2-28x-60 &amp;= 0\\\\ (x-30)(x+2) &amp; = 0\\\\ \\Rightarrow x=30, -2 \\end{align*}\\] First, put into quadratic form \\(ax^2 + bx + c = 0\\) and then factorise by inspection \\[\\begin{align*} p^2 = 8p - 18\\\\ p^2 - 8p + 15 &amp;= 0\\\\ (p-3)(p-5) &amp;= 0\\\\ \\Rightarrow p = 3,5 \\end{align*}\\] c.Multiply out and group powers of \\(y\\). Third and fourth powers cancel in this question.Then factorise by inspection, in this case into one term of the form \\((3y+a)\\) and one of the form \\((y+b)\\) \\[\\begin{align*} -2y^2(3+y^2) &amp;= (2y^2 + 2y - 3)(-y^2 + y -4) -2\\\\ -6y^2 - 2y^4 &amp;= -2y^4 + 2y^3 - 8y^2 - 2y^3 + 2y^2 - 8y = 3y^2 - 3y + 12 -2\\\\ 3y^2 - 11y + 10 &amp;= 0\\\\ (3y-5)(y-2)&amp;=0\\\\ \\Rightarrow y = 2,5 \\end{align*}\\] Solve the following quadratic equations, giving results correct to 2 d.p. \\(4x^2+x-3=0\\) \\(x^2+x=5\\) \\(x+\\frac{1}{x}=5\\) Answers: These questions require the application of the quadratic formula: for a quadratic which can be put into the form \\(ax^2 + bx + c = 0\\), \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\] \\(4x^2 + x - 3\\) \\[ x = \\frac{-1 \\pm \\sqrt{1^2 - 4\\times 4 \\times (-3)}}{8}\\\\ x = 3.38, -3.63 \\] First, rearrange \\(x^2 + x = 5\\) as \\(x^2 + x - 5 =0\\), then apply the quadratic formula \\[ x = \\frac{-1 \\pm \\sqrt{1^2 - 4\\times 1 \\times (-5)}}{2}\\\\ x = 1.79, -2.79 \\] First, rearrange \\(x + \\frac{1}{x} = 5\\) into \\(x^2 + 1 - 5 = 0\\) and then apply the quadratic formula. \\[ x = \\frac{5 \\pm \\sqrt{(-5)^2 - 4\\times 1 \\times 1}}{2}\\\\ x = 4.79, 0.21 \\] Solve the following sets of simultaneous equations. \\(3x+4y=7\\), \\(5x+6y=11\\) \\(2x+y=7\\), \\(x^2-xy=6\\) \\(x + y=2\\), \\(x^2-xy+y^2 = 1\\) Answers: Because these simultaneous equations are linear in \\(x\\) and \\(y\\), they can be solved by multiplying the equations and adding or subtracting them to cancel one of \\(x\\) or \\(y\\). Here, they can be solved by multiplying the left equation by 5 and the right equation by 3, then subtracting to cancel the \\(x\\) term: \\[ 3x+4y = 7,\\quad 5x + 6y = 11\\\\ 15 x + 20y = 35,\\quad 15x + 18y = 33\\\\ \\text{subtracting the right equaltion from the left:}\\\\ 2y = 2 \\Rightarrow y=1,x=1 \\] It will be seen that this is equivalent to Gaussian elimination of matrices. This could also have been solved via direct substitution (e.g., \\(x = (7-4y)/3\\)), as the following are. \\[ 2x + y = 7,\\quad x^2 - xy = 6\\\\ \\Rightarrow y=7-2x \\. \\text{substitute into right hand equation}\\\\ x^2 - x(7-2x) = 6\\\\ x^2 - 7x + 2x^2 = 6\\\\ 3x^2 - 7x - 6 = 0 \\] which can now be factorised by inspection to obtain the two solutions for \\(x\\) \\[ (3x+2)(x-3) = 0\\\\ \\Rightarrow x = -\\frac{2}{3},x = 3 \\] The value for \\(y\\) must be calculated for both cases of \\(x\\). It is simplest to use \\(y = 7 - 2x\\). The solutions are: \\[ x = -\\frac{2}{3},y=\\frac{25}{3},\\quad\\text{and }\\, x = 3, y = 1 \\] As before, substitute one of \\(x\\) or \\(y\\) using the linear equation \\[ x+y = 2, \\quad x^2 - xy + y^2 = 1\\\\ \\Rightarrow x = 2-y,\\quad (2-y)^2 - (2-y)y + y^2 = 1\\\\ 4 - 4y^2 + y^2 - 2y + y^2 + y^2 = 1\\\\ y^2 + 2y - 3 = 0\\\\ (y+3)(y-1) = 0\\\\ y=1, -3 \\] and again, we must give \\(x\\) for both cases. The two solutions are \\[ x=1,y=1\\quad\\text{and }x =5,y=-3 \\] Solve the following inequalities. \\(7-3x&gt;x-5\\) \\(x^2\\ge 4\\) \\(x^2-x-6&lt;0\\) \\(x^2-3x-12\\le 2x+2\\) \\(\\frac{2}{2x-1} &gt; \\frac{3}{3x+1}\\) \\(\\frac{2x}{x-5}\\le 3\\) \\(x^3-9x^2&lt;0\\) Answers: \\[\\begin{align*} 7-3x &amp;&gt; x-5\\\\ 13 &amp;&gt; 4x\\\\ x &amp;&lt; \\frac{13}{4} \\end{align*}\\] Here, we must include both the positive and the negative solution to the square root: \\[\\begin{align*} x^2 &amp;\\geq 4\\\\ \\pm x &amp;\\geq 2\\\\ x\\geq 2 &amp; x\\leq -2 \\end{align*}\\] It is often worth sketching the graph and examining the inequality to determine where the inequality is satisfied. \\[ x^2 - x - 6 &lt; 0 \\\\ (x+2)(x-3) &lt; 0 \\] The graph of \\(x^2\\) will form a \\(\\cup\\) shape, so the parts below the axis are between the crossing points of \\(x = -2\\) and \\(x=3\\). So the inequality is satisfied for \\[ -2&lt;x&lt;3 \\] Note the use of strict inequalities \\(x&lt;3\\) and \\(x&gt;-2\\) because the inequality is \\(&lt;0\\) (as opposed to \\(\\leq\\)). Group terms to form a quadratic, find the solutions and determine whether the interval between them or outside of them satisfied the inequality: \\[\\begin{align*} x ^2 - 3x - 12 &amp;\\leq 2x + 2\\\\ x^2 - 5x - 14 &amp;\\leq 0\\\\ (x-7)(x+2)&amp;\\leq 0 \\end{align*}\\] And the part of the curve below the axis is the range \\(-2\\leq x \\leq 7\\) Do not multiply through by a variable, since at times the term will be negative and necessitate a flip of the inequality. Instead, subtract one side from both sides: \\[\\begin{align*} \\frac{2}{2x-1} &amp;&gt; \\frac{3}{3x+1}\\\\ \\frac{2}{2x-2} - \\frac{3}{3x+1} &amp;&gt; 0\\\\ \\frac{2(3x+1) - 3(2x-1)}{(2x-1)(3x+1)} &amp;&gt; 0\\\\ \\frac{5}{(2x-1)(3x+1)} &amp;&gt; 0 \\end{align*}\\] In order for the inequality to hold, either both terms in the denominator are positive or both are negative. \\[ \\text{positve case: }2x-1&gt;0,\\quad 3x+1 &gt; 0\\\\ x &gt; \\frac{1}{2} \\text{ and } x &gt; \\frac{1}{3}\\\\ x &gt; \\frac{1}{2}\\\\ \\text{negative case: }2x-2 &lt; 0, \\quad 3x+1 &lt; 0\\\\ x&lt;\\frac{1}{2} \\text{ and } x &lt; \\frac{1}{3}\\\\ x &lt; \\frac{1}{3} \\] So the final solution is \\(x &gt; \\frac{1}{2}\\) and \\(x &lt; \\frac{1}{3}\\). \\[\\begin{align*} \\frac{2x}{x-5} &amp;\\leq 3 \\\\ \\frac{2x}{x-5} - 3 &amp;\\leq 0\\\\ \\frac{2x - 3(x-5)}{x-5} &amp; \\leq 0\\\\ \\frac{15-x}{x-5}&amp;\\leq 0 \\end{align*}\\] Which will ony old if one of the numerator or denominator is negative. So the inequality is solved by \\(x\\geq 15\\) and \\(x\\leq 5\\). Here, a cubic is presented, but a factor of \\(x^2\\) can be taken out \\[\\begin{align*} x^3 - 9x^2 &amp;&lt; 0 \\\\ x^2(x-9) &amp;&lt; 0\\\\ \\end{align*}\\] The \\(x^2\\) factor is always positive, so the whole inequality holds when \\(x-9\\) is negative. Hence, \\(x&lt;9\\) "],["exercise-set-2.html", "Exercise Set 2", " Exercise Set 2 These exercises cover the topics of Functions and Graphs. Sketch the graphs of the following functions on the interval \\(-2\\le x \\le 2\\). \\(y=-\\frac{1}{2}(x+1)\\) \\(y=x^2+2\\) \\(y=-2x^2+2\\) \\(y=x^2-x-1\\) \\(y=3^x\\) \\(y=\\frac{1}{x}\\) Tips: find where the functions cross the axes; use completing the square to find the maximum or minimum of quadratics; identify any asymptotes. Sketch the following graphs (with \\(x\\) in radians): \\(y=\\sin(x)\\) \\(y=\\sin(x+\\frac{\\pi}{2})\\) (does this look familiar?) \\(y=\\sin(2x)\\) \\(y=\\sin^2(x)\\) If we had the graph of a function \\(f(x)\\), describe what would change qualitatively for the graph of \\(f(a\\times x)\\) where \\(a\\) is a constant. Consider the cases: \\(a&gt;1\\) \\(0 &lt; a &lt; 1\\) \\(-1 &lt; a &lt; 0\\) \\(a&lt;-1\\) If we had the graph of a function \\(f(x)\\), describe what would change qualitatively for the graph of \\(f(x+b)\\) where \\(b\\) is a constant. Consider the cases: \\(b&gt;0\\) \\(b&lt;0\\) If we had the graph of a function \\(f(x)\\), describe what would change qualitatively for the graph of \\(f(x)+c\\) where \\(c\\) is a constant. Consider the cases: \\(c&gt;0\\) \\(c&lt;0\\) Find the solutions to the following equations without using a calculator. \\(x=\\log_4(64)\\) \\(x=\\log_{101}(101)\\) \\(x=\\ln(e)\\) \\(x=\\log_{56}(1)\\) \\(10^x=100000\\) \\(2^x=128\\) \\(\\log_3(x)=4\\) \\(\\log_x(125)=3\\) Calculate the following logarithms using a calculator. \\(\\ln(2)\\) \\(\\log(20)\\) \\(\\log_{16}(100)\\) - do this using your \\(\\log_a(x)\\) button and also using the change of base rule with your \\(\\log\\) or \\(\\ln\\) button. Calculate \\(4321\\times 9876\\) using logs: convert both values to logs, add, then convert back. Simplify the following expression involving logarithms. \\[\\log_a(x^2)+3\\log_a(x)-2\\log_a(4x^2).\\] Solve the following for \\(x\\). \\[2\\log_a(x)-\\log_a(x-1)=\\log_a(x+3).\\] Find \\(y\\) in terms of \\(x\\): \\[5\\log_a(y)-2\\log_a(x+4)=2\\log_a(y)+\\log_a(x).\\] Use natural logarthims to make \\(t\\) the subject of the formula \\[V=1-e^{\\frac{-t}{RC}}.\\] Show that the following hyperbolic identity holds: \\[\\cosh^2(x)-\\sinh^2(x)=1.\\] "],["exercise-set-2-answers.html", "Exercise Set 2 Answers", " Exercise Set 2 Answers These exercises cover the topics of Functions and Graphs. Sketch the graphs of the following functions on the interval \\(-2\\le x \\le 2\\). \\(y=-\\frac{1}{2}(x+1)\\) \\(y=x^2+2\\) \\(y=-2x^2+2\\) \\(y=x^2-x-1\\) \\(y=3^x\\) \\(y=\\frac{1}{x}\\) Tips: find where the functions cross the axes; use completing the square to find the maximum or minimum of quadratics; identify any asymptotes. Answers: Expanding: \\(y=-\\frac{1}{2}x-\\frac{1}{2}\\) which we recognise as a line with gradient \\(-\\frac{1}{2}\\) and y intercept \\(-\\frac{1}{2}\\). Solving for \\(y=0\\) gives the x intercept as \\((-1,0)\\). This is a parabola shifted up by 2 units. This is an “upside down” parabola, shifted up by two units. Completing the square \\(y=(x-\\frac{1}{2})^2-\\frac{5}{4}\\). Since the squared term is always non-negative, it is smallest when it is zero at \\(x=\\frac{1}{2}\\). This is the position of the minimum, and at this point \\(y=-\\frac{5}{4}\\). Factorising: \\((x-\\frac{1+\\sqrt{5}}{2})(x-\\frac{1-\\sqrt{5}}{2})\\), so the curve crosses the x axis at \\(x=\\frac{1+\\sqrt{5}}{2}\\) and \\(x=\\frac{1-\\sqrt{5}}{2}\\). Since the coefficient of \\(x^2\\) is positive, the parabola opens upwards. This is an exponential \\(a^x\\) with \\(a&gt;1\\). All exponential functions cross the y axis at \\(a^0=1\\) and the do not cross the \\(x\\) axis. This is a rational function with a vertical asymptote at \\(x=0\\). It is small and positive for large positive values of \\(x\\) and it is small and negative for large negative values of \\(x\\): the line \\(y=0\\) is a horizontal asymptote. Sketch the following graphs (with \\(x\\) in radians): \\(y=\\sin(x)\\) \\(y=\\sin(x+\\frac{\\pi}{2})\\) (does this look familiar?) \\(y=\\sin(2x)\\) \\(y=\\sin^2(x)\\) Answers: You should be familiar with this graph and the location of maxima, minima and axis intercepts. Note that \\(\\sin(x+\\frac{\\pi}{2})=\\cos(x)\\). This is double the angular frequency of the usual sine function. Note that all the y values are non-negative. If we had the graph of a function \\(f(x)\\), describe what would change qualitatively for the graph of \\(f(a\\times x)\\) where \\(a\\) is a constant. Consider the cases: \\(a&gt;1\\) \\(0 &lt; a &lt; 1\\) \\(-1 &lt; a &lt; 0\\) \\(a&lt;-1\\) Answers: For \\(a&gt;1\\), the graph is “squeezed” along the x axis, with the value at \\(x=0\\) remaining unchanged. For \\(0 &lt; a &lt;1\\), the graph is expanded along the x axis, again with the value at \\(x=0\\) unchanged. For \\(-1 &lt; a &lt;0\\), the graph is again expanded, along the x axis, but the minus sign in \\(a\\) also means that the graph is reflected in the y axis. For \\(a&lt;-1\\), the graph is squeezed horizontally and reflected in the y axis. If we had the graph of a function \\(f(x)\\), describe what would change qualitatively for the graph of \\(f(x+b)\\) where \\(b\\) is a constant. Consider the cases: \\(b&gt;0\\) \\(b&lt;0\\) Answers: For \\(b&gt;0\\), the \\(f(x+b)\\) is the graph of \\(f(x)\\) translated to the left by distance \\(b\\) For \\(b&lt;0\\), the whole graph is translated to the right by distance \\(b\\) If we had the graph of a function \\(f(x)\\), describe what would change qualitatively for the graph of \\(f(x)+c\\) where \\(c\\) is a constant. Consider the cases: \\(c&gt;0\\) \\(c&lt;0\\) Answers: For \\(c&gt;0\\), \\(f(x)+c\\) is the graph of \\(f(x)\\) shifted vertically up along the y axis For \\(c&lt;0\\), \\(f(x)+c\\) is the graph of \\(f(x)\\) shifted vertically down along the y axis Find the solutions to the following equations without using a calculator. \\(x=\\log_4(64)\\) \\(x=\\log_{101}(101)\\) \\(x=\\ln(e)\\) \\(x=\\log_{56}(1)\\) \\(10^x=100000\\) \\(2^x=128\\) \\(\\log_3(x)=4\\) \\(\\log_x(125)=3\\) Answers: \\(x = \\log_4(64) = \\log_4(4^3)=3\\) \\(x = \\log_{101}(101) =\\log_{101}(101^1)=1\\) \\(x = \\ln(e) =\\log_e(e^1)= 1\\) \\(x = log_{56}(1) =\\log_{56}(56^0)=0\\) \\(10^x = 100000=10^5 \\Rightarrow x = 5\\) \\(2^x = 128=2^7\\) x = 7$ the powers of 2 are useful to know \\(3^{\\log_3(x)} = 3^4 \\Rightarrow x = 3^4 = 3^2 \\times 3^2 = 81\\) \\(\\log_x(125) = 3 \\Rightarrow x^3 = 125 \\Rightarrow x = 5\\) Calculate the following logarithms using a calculator. \\(\\ln(2)\\) \\(\\log(20)\\) \\(\\log_{16}(100)\\) - do this using your \\(\\log_a(x)\\) button and also using the change of base rule with your \\(\\log\\) or \\(\\ln\\) button. Answers: To 2 d.p. \\(\\ln(2) = 0.69\\) \\(\\log(20) = 1.30\\) \\(\\log_{16}(100) = 1.66\\) or using the change of base rule with \\(\\log_{10}\\), \\(\\log_{16}=\\frac{\\log_{10}(100)}{\\log_{10}(16)}=\\frac{2}{\\log_{10}(16)}=1.66\\) Calculate \\(4321\\times 9876\\) using logs: convert both values to logs, add, then convert back. Answer: \\[\\log(4321 \\times 9876)=\\log(4321) + \\log(9876) = 7.630165..., \\text{ the product rule of logarithms}\\\\ 4321 \\times 9876=10^{7.630165...} = 42674196 \\] Simplify the following expression involving logarithms. \\[\\log_a(x^2)+3\\log_a(x)-2\\log_a(4x^2).\\] Answer: Applying the various log rules, starting with the power rules then product and quotient rules: \\[ \\begin{aligned} &amp; \\log_a(x^2)+3\\log_a(x)-2\\log_a(4x^2)\\\\ &amp;= \\log_a(x^2) + \\log_a(x^3) - log_a(16x^4)\\\\ &amp;= \\log_a(\\frac{x^5}{16x^4})\\\\ &amp;= \\log_a(\\frac{x}{16}) \\end{aligned} \\] Solve the following for \\(x\\). \\[2\\log_a(x)-\\log_a(x-1)=\\log_a(x+3).\\] Answer: \\[ \\begin{aligned} 2 \\log_a(x) - \\log_a(x-1) &amp;= \\log_a(x+3)\\\\ \\log_a\\left(\\frac{x^2}{x-1}\\right) &amp;= \\log_a(x+3)\\\\ \\log_a\\left(\\frac{x^2}{(x-1)(x+3)}\\right) &amp;= 0, \\text{ since } \\log_a(1) = 0\\\\ \\frac{x^2}{(x-1)(x+3)} &amp;= 1\\\\ x^2 &amp;= (x-1)(x+3) = x^2 + 2x - 3\\\\ x &amp;= \\frac{3}{2} \\end{aligned} \\] Find \\(y\\) in terms of \\(x\\): \\[5\\log_a(y)-2\\log_a(x+4)=2\\log_a(y)+\\log_a(x).\\] Answer: First, rearrange and then apply the logarithm power, product and quotient rules: \\[ \\begin{align} 5\\log_a(y)-2\\log_a(x+4)&amp;=2\\log_a(y)+\\log_a(x)\\\\ \\Rightarrow \\log_a(y^5) - \\log_a(y^2) &amp;= \\log_a(x) + 2\\log_a(x+4)\\\\ = \\log_a(y^3) &amp;= \\log_a((x+4)^2)\\\\ \\Rightarrow y^3 &amp;= x(x+4)^2\\\\ y &amp;= \\left(x(x+4)^2\\right)^{\\frac{1}{3}} \\end{align} \\] Use natural logarthims to make \\(t\\) the subject of the formula \\[V=1-e^{\\frac{-t}{RC}}.\\] Answer: Make the exponential term the subject and then take the natural logarithms of both sides \\[ \\begin{align} V &amp;= 1- e^{\\frac{-t}{RC}}\\\\ e^{\\frac{-t}{RC}} &amp;= 1- v\\\\ \\ln\\left(e^{\\frac{-t}{RC}}\\right) &amp;= \\ln(1- v)\\\\ \\frac{-t}{RC} &amp;= \\ln(1-v)\\\\ t &amp;= -RC\\ln(1-v) \\end{align} \\] Show that the following hyperbolic identity holds: \\[\\cosh^2(x)-\\sinh^2(x)=1.\\] Answer: Use the definitions of \\(\\cosh(x) = \\frac{e^x + e^-x}{2}\\) and \\(\\sinh(x) = \\frac{e^x - e^{-x}}{2}\\) \\[ \\begin{align} \\cosh^2(x) - \\sinh^2(x) &amp;=(\\cosh(x) + \\sinh{x})(\\cosh(x) - \\sinh(x))\\\\ &amp;=\\left(\\frac{e^x + e^{-x} + e^x - e^{-x}}{2}\\right)\\left(\\frac{e^x + e^{-x} - e^x + e^{-x}}{2}\\right)\\\\ &amp;=e^x e^{-x}\\\\ &amp;= 1 \\text{ as required} \\end{align} \\] "],["exercise-set-3.html", "Exercise Set 3", " Exercise Set 3 These exercises cover the topic of Trigonometry. Tip: always start by drawing a labelled diagram in trigonometry questions. Consider the smaller of the two angles between the hour hand and minute hand of a clock. Write the angles at the following times in both degrees and radians (in terms of \\(\\pi\\)). 6:00 3:00 4:00 4:30 6:45 Convert the following angles from degrees to radians (leave your answer in terms of \\(\\pi\\) where possible, or to 2 d.p.). \\(330^\\circ\\) \\(22.5^\\circ\\) \\(27^\\circ\\) \\(35^\\circ\\) A bearing is the angle measured clockwise from North to the direction of interest. A point \\(K\\) is 12km due west of a second point \\(L\\) and 25km due south of a third point \\(M\\). Calculate the bearing of \\(L\\) from \\(M\\). Solve (i.e. find all unkown angles and side lengths) the triangle \\(ABC\\) where \\(A = 53^\\circ\\), \\(B = 61^\\circ\\) and \\(a = 12.6\\)cm. Let \\(AOB\\) be a triangle. \\(OA = 60\\)mm, \\(AB = 180\\)mm and \\(OB = 200\\)mm. Find angle \\(A\\). An angle of elevation is an angle that an imaginary straight line must be raised from the horizontal ground to line up with a point of interest above the ground. An observer is standing at a point \\(M\\) which is \\(30\\)m from the base of a tower. On top of the tower is a vertical mast. If the angles of elevation of the top of the tower and the top of the mast from \\(M\\) are \\(40^\\circ\\) and \\(50^\\circ\\) respectively, calculate the height of the mast. The small hand of a clock is 75% the length of the long hand. Calculate the distance between the ends of the hands at 5 o’clock. A student \\(1.8\\)m tall is standing \\(24\\)m away from a tree and using a eye level instrument to measure the angle of elevation. The angle measured to the top of the tree is \\(12^\\circ 34&#39;\\), calculate the height of the tree. (Degrees can be further subdivided in to minutes denoted \\(x&#39;\\) and seconds denoted \\(x&#39;&#39;\\), with \\(1&#39;\\) being \\(1/60\\) of a degree and \\(1&#39;&#39;\\) being \\(1/60\\) of a minute. To use a calculator you will first need to convert minutes and seconds to decimals.) The angles of elevation of a navigation balloon that is flying in between two points on the ground \\(A\\) and \\(B\\) are \\(48^\\circ\\) and \\(62^\\circ\\) respectively. If \\(A\\) and \\(B\\) are \\(0.3\\)km apart, calculate the height of the balloon. The figure below shows a tetrahedron with an equilateral triangle of side 2m forming the base and isosceles triangles of equal side 3m forming the slanting faces. Calculate: The height of the tetrahedron \\(ND\\); The angle that edge \\(DA\\) makes with the plane \\(ABC\\); The angle between the planes \\(ACD\\) and \\(ACB\\). Write the following in the form \\(R\\cos(\\omega t\\pm \\beta)\\). \\(-2\\sin(\\omega t) + 5\\cos(\\omega t)\\) \\(-5\\cos(\\omega t) + 5 \\sin(\\omega t)\\) In a spring-mass system the motion of the mass is described by \\[x=A\\cos(\\omega t)+B\\sin(\\omega t)\\] where \\(x\\) is the distance of the mass from its equilibrium position, \\(\\omega\\) is the natural frequency of oscillations, and \\(A\\) and \\(B\\) are constants. For \\(A=\\sqrt{3}\\), \\(B=1\\) and \\(\\omega=10\\): Write \\(x\\) in the form \\(R\\cos(\\omega t-\\beta)\\) and state the amplitude of \\(x\\). Sketch one complete cycle of \\(x\\). "],["exercise-set-3-answers.html", "Exercise Set 3 Answers", " Exercise Set 3 Answers These exercises cover the topic of Trigonometry. Tip: always start by drawing a labelled diagram in trigonometry questions. Consider the smaller of the two angles between the hour hand and minute hand of a clock. Write the angles at the following times in both degrees and radians (in terms of \\(\\pi\\)). 6:00 3:00 4:00 4:30 6:45 Answers: \\(180^\\circ\\), \\(\\pi\\text{ rad}\\) \\(90^\\circ\\), \\(\\frac{\\pi}{2}\\text{ rad}\\) \\(60^\\circ\\), \\(\\frac{2\\pi}{3}\\text{ rad}\\) \\(45^\\circ\\), \\(\\frac{\\pi}{4}\\text{ rad}\\) \\(67.5^\\circ\\), \\(\\frac{3\\pi}{8}\\text{ rad}\\) Convert the following angles from degrees to radians (leave your answer in terms of \\(\\pi\\) where possible, or to 2 d.p.). \\(330^\\circ\\) \\(22.5^\\circ\\) \\(27^\\circ\\) \\(35^\\circ\\) Answers: \\(\\frac{11}{6}\\pi\\text{ rad}\\) \\(\\frac{\\pi}{8}\\text{ rad}\\) \\(\\frac{3}{20}\\pi\\text{ rad}\\) or \\(0.47\\text{ rad}\\) \\(\\frac{7}{36}\\pi\\text{ rad}\\) or \\(0.61\\text{ rad}\\) A bearing is the angle measured clockwise from North to the direction of interest. A point \\(K\\) is 12km due west of a second point \\(L\\) and 25km due south of a third point \\(M\\). Calculate the bearing of \\(L\\) from \\(M\\). Answers: Drawing a right angled triangle \\(MKL\\) the angle \\(\\angle KML\\) is \\[\\tan(\\angle KML)=\\frac{12}{25}\\quad\\implies\\quad \\angle KML=\\tan^{-1}\\frac{12}{25}=25.6^\\circ\\] The bearing of \\(L\\) from \\(M\\) is the angle clockwise from a line pointing North from \\(M\\), so the bearing is: \\[180^\\circ -25.6^\\circ=154.4^\\circ.\\] Solve (i.e. find all unkown angles and side lengths) the triangle \\(ABC\\) where \\(A = 53^\\circ\\), \\(B = 61^\\circ\\) and \\(a = 12.6\\)cm. Answers: We first find angle \\(C\\): \\[C=180^\\circ-61^\\circ-53^\\circ=66^\\circ.\\] Now using the sine rule: \\[b=\\frac{a}{\\sin(A)}\\sin(B)=\\frac{12.6}{\\sin(53^\\circ)}\\sin(61^\\circ)=13.8\\text{ cm}\\] and \\[c=\\frac{a}{\\sin(A)}\\sin(C)=\\frac{12.6}{\\sin(66^\\circ)}\\sin(53^\\circ)=14.4\\text{ cm}.\\] Let \\(AOB\\) be a triangle. \\(OA = 60\\)mm, \\(AB = 180\\)mm and \\(OB = 200\\)mm. Find angle \\(A\\). Answers: Using the cosine rule: \\[\\begin{align*}\\cos(A)&amp;=\\frac{b^2+c^2-a^2}{2bc}\\\\ &amp;=\\frac{60^2+180^2-200^2}{2\\times 60\\times 180}\\\\ &amp;=-0.185 \\end{align*}\\] which gives \\[A=\\cos^{-1}(-0.185)=101^\\circ.\\] An angle of elevation is an angle that an imaginary straight line must be raised from the horizontal ground to line up with a point of interest above the ground. An observer is standing at a point \\(O\\) which is \\(30\\)m from the base of a tower. On top of the tower is a vertical mast. If the angles of elevation of the top of the tower and the top of the mast from \\(M\\) are \\(40^\\circ\\) and \\(50^\\circ\\) respectively, calculate the height of the mast. Answers: Let the base of the tower be point \\(B\\), the top of the tower point \\(T\\) and the top of the mast point \\(M\\). Then there are two right-angled triangles: \\(OBT\\) and \\(OBM\\). We have \\(OB=30\\)m, angle \\(\\angle BOT=40^\\circ\\) and angle \\(\\angle BOM=50^\\circ\\). We need to calculate \\(TM\\). Using trig. ratios: \\[BT=OB\\tan(\\angle BOT)=30\\tan^{-1}(40^\\circ)=25.2\\text{ m}\\] \\[BM=OB\\tan(\\angle BOM)=30\\tan^{-1}(50^\\circ)=35.8\\text{ m}\\] and hence \\[TM=OM-OT=35.8-25.2=10.6\\text{ m}.\\] The small hand of a clock is 75% the length of the long hand. Calculate the distance between the ends of the hands at 5 o’clock. Answers: Let the distance between the ends of the hands be \\(d\\) and the length of the long hand \\(x\\). Then the length of the short hand is \\(0.75x\\). The angle at 5 o’clock is \\(150^\\circ\\). Using the cosine rule: \\[d^2=(0.75x)^2+x^2-1.5x^2\\cos(150^\\circ)=2.86x^2\\] and \\(d=1.69x.\\) A student \\(1.8\\)m tall is standing \\(24\\)m away from a tree and using an eye level instrument to measure the angle of elevation. The angle measured to the top of the tree is \\(12^\\circ 34&#39;\\), calculate the height of the tree. (Degrees can be further subdivided in to minutes denoted \\(x&#39;\\) and seconds denoted \\(x&#39;&#39;\\), with \\(1&#39;\\) being \\(1/60\\) of a degree and \\(1&#39;&#39;\\) being \\(1/60\\) of a minute. To use a calculator you will first need to convert minutes and seconds to decimals.) Answers: Let the student be at position \\(O\\) with eye-level \\(E\\), the base of the tree at position \\(B\\) and the top of the tree at position \\(T\\). Drawing a line parallel to \\(OB\\) from \\(E\\) to the tree, let the intersection point be \\(S\\). We have \\(OE=BS=1.8\\text{ m}\\) (or perhaps more realistically we could use 1.7m for eye-level), \\(OB=ES=24\\text{ m}\\) and \\(\\angle SET=12^\\circ 34&#39;\\). First, we convert the angle to decimal: \\[12^\\circ 34&#39;=12^\\circ + \\frac{34}{60}=12^\\circ + 0.57^\\circ=12.57^\\circ.\\] Now, \\[ST=ES\\tan(\\angle SET)=24\\tan(12.57^\\circ)=5.4\\text{ m}\\] so \\[BT=BS+ST=5.4+1.8=7.2\\text{ m}.\\] The angles of elevation of a navigation balloon that is flying in between two points on the ground \\(A\\) and \\(B\\) are \\(48^\\circ\\) and \\(62^\\circ\\) respectively. If \\(A\\) and \\(B\\) are \\(0.3\\)km apart, calculate the height of the balloon. Answers: Let the ballon be at point \\(C\\), then we have a triangle \\(ABC\\). Angle \\(C\\) is \\(C=180-48-62=70^\\circ\\). We now find the length of \\(BC\\) using the sine rule: \\[BC=\\frac{AB}{\\sin(C)}\\sin(A)=\\frac{0.3}{\\sin(70^\\circ)}\\sin(48^\\circ)=0.237\\text{ km}\\] Now dropping a perpendicular from \\(C\\) to the ground at a point \\(D\\) (on the line \\(AB\\)) we have a right angled triangle \\(BDC\\). Using \\(\\sin(\\theta)=Opp./Hyp.\\) the height is then \\[DC=BC\\sin(C)=0.237\\sin(62^\\circ)=0.21\\text{ km}.\\] The figure below shows a tetrahedron with an equilateral triangle of side 2m forming the base and isosceles triangles of equal side 3m forming the slanting faces. Calculate: The height of the tetrahedron \\(ND\\); The angle that edge \\(DA\\) makes with the plane \\(ABC\\); The angle between the planes \\(ACD\\) and \\(ACB\\). Answers: \\(ABC\\) is an equilateral triangle, so each of its angles are \\(60^\\circ\\). By symmetry, \\(N\\) is in the centre of triangle \\(ABC\\). Therefore \\(\\angle ANB=120^\\circ\\) and \\(\\angle BAN=30^\\circ\\). Using the sine rule: \\[AN=\\frac{2}{\\sin(120^\\circ)}\\sin(30^\\circ)=1.15\\text{ m}.\\] Now using Pythagoras on right-angled triangle \\(AND\\): \\[ND=\\sqrt{AD^2-AN^2}=\\sqrt{3^2-1.15^2}=\\sqrt{7.68}=2.77\\text{ m}\\] This is given by angle \\(\\angle NAD\\), which can be found using \\(\\cos(\\theta)=Adj./Hyp.\\) \\[\\cos(\\angle NAD)=\\frac{1.15}{3}=0.385\\implies \\angle NAD=\\cos^{-1}(0.385)=67.4^\\circ.\\] By similar reasoning to the above, this is left for the reader to verify as being \\(78.2^\\circ\\). Write the following in the form \\(R\\cos(\\omega t\\pm \\beta)\\). \\(-2\\sin(\\omega t) + 5\\cos(\\omega t)\\) \\(-5\\cos(\\omega t) + 5 \\sin(\\omega t)\\) Answers: Using the compound angle identity \\[R\\cos(\\omega t+\\beta)=R(\\cos(\\omega t)\\cos(\\beta)-\\sin(\\omega t)\\sin(\\beta))\\] and comparing the r.h.s. with \\(-2\\sin(\\omega t) + 5\\cos(\\omega t)\\) and equating coefficients of sin and cos, \\[R\\cos(\\beta)=5\\quad\\text{and}\\quad R\\sin(\\beta)=2.\\] Since \\(\\sin(\\theta)\\) and \\(\\cos(\\theta)\\) are both positive only for \\(0^\\circ\\leq\\theta\\leq 90^\\circ\\) (i.e. in the first quadrant) then \\[\\beta=\\tan^{-1}\\left(\\frac{2}{5}\\right)=21.80^\\circ\\quad\\text{or}\\quad 0.38\\text{ rad}.\\] Now using \\[\\cos^2(\\theta)+\\sin^2(\\theta)=1\\] we have \\[R^2cos^2(\\beta)+R^2\\sin(\\beta)=R^2(\\cos^2(\\beta)+\\sin(\\beta))=R^2\\] so \\[R=\\sqrt{5^2+2^2}=\\sqrt{29}.\\] Finally we have \\[-2\\sin(\\omega t) + 5\\cos(\\omega t)=\\sqrt{29}\\cos(\\omega t + 0.38).\\] Using the compound angle identity \\[R\\cos(\\omega t-\\beta)=R(\\cos(\\omega t)\\cos(\\beta)+\\sin(\\omega t)\\sin(\\beta))\\] and comparing the r.h.s. with \\(-5\\cos(\\omega t) + 5\\sin(\\omega t)\\) and equating coefficients of sin and cos, \\[R\\cos(\\beta)=-5\\quad\\text{and}\\quad R\\sin(\\beta)=5.\\] Since \\(\\sin(\\theta)\\) is positive and \\(\\cos(\\theta)\\) is negative only for \\(90^\\circ\\leq\\theta\\leq 180^\\circ\\) (i.e. in the second quadrant) then \\[\\beta=180^\\circ-\\tan^{-1}\\left(\\frac{5}{5}\\right)=135^\\circ\\quad\\text{or}\\quad 2.36\\text{ rad}.\\] Now using \\[\\cos^2(\\theta)+\\sin^2(\\theta)=1\\] we have \\[R^2cos^2(\\beta)+R^2\\sin(\\beta)=R^2(\\cos^2(\\beta)+\\sin(\\beta))=R^2\\] so \\[R=\\sqrt{5^2+5^2}=5\\sqrt{2}.\\] Finally we have \\[-5\\cos(\\omega t) + 5\\sin(\\omega t)=5\\sqrt{2}\\cos(\\omega t + 2.36).\\] In a spring-mass system the motion of the mass is described by \\[x=A\\cos(\\omega t)+B\\sin(\\omega t)\\] where \\(x\\) is the distance of the mass from its equilibrium position, \\(\\omega\\) is the natural frequency of oscillations, and \\(A\\) and \\(B\\) are constants. For \\(A=\\sqrt{3}\\), \\(B=1\\) and \\(\\omega=10\\): Write \\(x\\) in the form \\(R\\cos(\\omega t-\\beta)\\) and state the amplitude of \\(x\\). Sketch one complete cycle of \\(x\\). Answers: Using the compound angle identity \\[R\\cos(\\omega t-\\beta)=R(\\cos(\\omega t)\\cos(\\beta)+\\sin(\\omega t)\\sin(\\beta))\\] and comparing the r.h.s. with \\(\\sqrt{3}\\cos(10 t) + \\sin(10 t)\\) and equating coefficients of sin and cos, \\[R\\cos(\\beta)=\\sqrt{3}\\quad\\text{and}\\quad R\\sin(\\beta)=1.\\] Since \\(\\sin(\\theta)\\) and \\(\\cos(\\theta)\\) are both positive only for \\(0^\\circ\\leq\\theta\\leq 90^\\circ\\) (i.e. in the first quadrant) then \\[\\beta=\\tan^{-1}\\left(\\frac{1}{\\sqrt{3}}\\right)=30\\circ\\quad\\text{or}\\quad \\frac{\\pi}{6}\\text{ rad}.\\] Now using \\[\\cos^2(\\theta)+\\sin^2(\\theta)=1\\] we have \\[R^2cos^2(\\beta)+R^2\\sin(\\beta)=R^2(\\cos^2(\\beta)+\\sin(\\beta))=R^2\\] so \\[R=\\sqrt{\\sqrt{3}^2+1^2}=2\\] Finally we have \\[x(t)=\\sqrt{3}\\cos(10 t) + \\sin(10 t)=2\\cos(10 t -\\frac{\\pi}{6}).\\] From this we can read off the amplitude as \\(R=2\\). The period is \\(T=\\frac{2\\pi}{10}=\\frac{\\pi}{5}\\) and the displacement of the peak at \\(0\\) is \\(\\frac{\\pi/6}{10}=\\frac{\\pi}{60}\\). "],["exercise-set-4.html", "Exercise Set 4", " Exercise Set 4 These exercises cover the topic of Complex Numbers. Let \\(u=2+3i\\) and \\(v=5+8i\\). Determine the following. \\(u+v\\) \\(u-v\\) \\(uv\\) \\(vu\\) \\(\\frac{u}{v}\\) \\(\\frac{v}{u}\\) \\(u^2\\) \\(v^2\\) \\(u^2+v^2\\) State the complex conjugates of the following numbers. Also caclulate their modulus. \\(3-4i\\) \\(2+2i\\) \\(2+15i\\) \\(3i\\) \\(i\\) \\(5\\) Let \\(z=3-i\\). Find \\(z^2+7z+13\\) in Cartesian form. Find the roots of the following quadratic equations. \\(z^2+2z+26=0\\) \\(z^2-2z+3=0\\) \\(3z^2-7z+13=0\\) Express the following complex numbers in polar form and exponential form. \\(1+i\\) \\(-1+i\\) \\(\\sqrt{3}-i\\) \\(z_1=2-3i\\) \\(z_2=3+4i\\) \\(\\frac{z_1-2}{z_2}\\) \\(z_1z_2-\\frac{z_1-z_2}{z_2}\\) \\(w_1=-\\sqrt{2}+\\sqrt{2}i\\) \\(w_2=-\\frac{1}{2}+\\frac{\\sqrt{3}}{2}i\\) \\(w_1w_2\\) \\(\\frac{w_1}{w_2}\\) Express the following complex numbers in Cartesian form. \\(2e^{i\\frac{\\pi}{12}}\\) \\(5e^{i 23^\\circ}\\) \\(2e^{i (-45^\\circ)}\\) Find the following roots and express them in Cartesian form. \\(1^\\frac{1}{3}\\) \\(1^\\frac{1}{4}\\) The cube roots of \\(\\sqrt{2}-\\sqrt{2}i\\) The square roots of \\(3-4i\\) Sketch the following complex numbers in the complex plane. \\(2+3i\\) \\(-1-i\\) \\(-1+i\\) \\(-2i\\) \\(-3\\) What is \\(i^i\\) in Cartesian form? Find all complex numbers \\(z\\) such that \\(\\overline{z}=z^2\\). "],["exercise-set-4-answers.html", "Exercise Set 4 Answers", " Exercise Set 4 Answers Let \\(u=2+3i\\) and \\(v=5+8i\\). Determine the following. \\(u+v\\) \\(u-v\\) \\(uv\\) \\(vu\\) \\(\\frac{u}{v}\\) \\(\\frac{v}{u}\\) \\(u^2\\) \\(v^2\\) \\(u^2+v^2\\) Answers: \\(u+v = (2 + 3i) + (5 + 8i) = (2 + 5) + i(3 + 8) = 7 + 11i\\) \\(u - v = (2+3i) - (5 + 8i) = -3 - 5i\\) \\[uv = (2 + 3i)(5+8i) = (2\\times 5) + i(8\\times 2) + i(3\\times 5) + i^2(2\\times 8)\\\\ = 10 + 16i + 15i + i^2 24\\\\ = 10 + 31i - 24\\\\ = -14 + 31i\\] \\(vu = (5+8i)(2 + 3i) = -14 + 31i\\), the same as \\(uv\\) \\[ \\frac{u}{v} = \\frac{2+3i}{5 + 8i} \\] rationalise the denominator by multiplying numerator and denominator by the complex conjugate of the denominator, \\(\\overline{v}=5-8i\\) \\[ \\frac{2 + 3i}{5+8i} = \\frac{(2+3i)(5-8i)}{(5+8i)(5-8i)} = \\frac{(2+3i)(5-8i)}{5^2 + 8^2} = \\frac{10 - 16i + 15i - 24i^2}{89}\\\\ = \\frac{10-i + 24}{89} = \\frac{34 - i}{89} = \\frac{34}{89} - \\frac{i}{89} = 0.382 - 0.011 i \\] Similarly \\[\\frac{v}{u} = \\frac{34}{13} + \\frac{i}{13} = 2.615 + 0.077 i \\] \\(u^2 = (2 + 3i)^2 = -5 + 12i\\) \\(v^2 = (5 + 8i)^2= -39 + 80i\\) \\(u^2 + v^2 = -44 + 92i\\) State the complex conjugates of the following numbers. Also caclulate their modulus. \\(3-4i\\) \\(2+2i\\) \\(2+15i\\) \\(3i\\) \\(i\\) \\(5\\) Answers: \\(z = 3-4i,\\quad \\bar{z} = 3 + 4i,\\quad |z| = 5\\) \\(z = 2+2i,\\quad \\bar{z} = 2 -2i,\\quad |z| = \\sqrt{8} = 2\\sqrt{2} = 2.83\\) \\(z = 2+ 15i,\\quad \\bar{z} = 2 - 15i,\\quad |z| = \\sqrt{229} = 15.13\\) \\(z=3i, \\quad \\bar{z} = -3i,\\quad |z| = 3\\) \\(z = i, \\quad \\bar{z} = -i,\\quad |z| = 1\\) \\(z = 5, \\quad \\bar{z} = 5,\\quad |z| = 5\\) Let \\(z=3-i\\). Find \\(z^2+7z+13\\) in Cartesian form. Answers: \\(z^2 + 7 z + 13 = 42 - 13 i\\) Find the roots of the following quadratic equations. \\(z^2+2z+26=0\\) \\(z^2-2z+3=0\\) \\(3z^2-7z+13=0\\) Answers: Using the quadratic formula: \\(z_1=-1-5i\\), \\(z_2=-1+5i\\) \\(z_1 = 1 - i \\sqrt{2}\\), \\(z_2 = 1 + i \\sqrt{2}\\) \\(z_1 = 7/6 - \\frac{i \\sqrt{107}}{6}\\), \\(z_2 = 7/6 - \\frac{i \\sqrt{107}}{6}\\) Note that in all cases, \\(z_1=\\overline{z}_2\\). This is not a coincidence! Extra exercise: show that if \\(z_1\\) is a root of a quadractic, then so is \\(\\overline{z}_1\\) (use the rules of complex conjugates). By the Fundamental Theorem of Algebra, these are then the only two roots of the quadratic. Express the following complex numbers in polar form and exponential form. \\(1+i\\) \\(-1+i\\) \\(\\sqrt{3}-i\\) \\(z_1=2-3i\\) \\(z_2=3+4i\\) \\(\\frac{z_1-2}{z_2}\\) \\(z_1z_2-\\frac{z_1-z_2}{z_2}\\) \\(w_1=-\\sqrt{2}+\\sqrt{2}i\\) \\(w_2=-\\frac{1}{2}+\\frac{\\sqrt{3}}{2}i\\) \\(w_1w_2\\) \\(\\frac{w_1}{w_2}\\) Answers: The polar form is \\[z=r(\\cos(\\theta)+i\\sin(\\theta))\\] and the exponential form is \\[z=re^{i\\theta}.\\] Note these both contain the polar coordinates \\((r,\\theta)\\) but the exponential form is more compact. In the answers below we will just state the values of \\(r\\) and \\(\\theta\\). This is in the first quadrant. \\(r=\\sqrt{x^2+y^2}=\\sqrt{1^2+1^2}=\\sqrt{2}\\) \\(\\theta=\\tan^{-1}\\left(\\frac{x}{y}\\right)=\\tan^{-1}\\left(\\frac{1}{1}\\right)=\\frac{\\pi}{4}\\) This is in the second quadrant. \\(r=\\sqrt{2}\\), \\(\\theta=\\)\\(-\\tan^{-1}\\left(\\frac{1}{1}\\right)=\\frac{3\\pi}{4}\\). This is in the fourth quadrant. \\(r=2\\), \\(\\theta=2\\pi - \\tan^{-1}\\left(\\frac{1}{\\sqrt{3}}\\right)=\\frac{11\\pi}{6}.\\) This is in the fourth quadrant. \\(r_1=\\sqrt{13}\\), \\(\\theta_1=2\\pi - \\tan^{-1}\\left(\\frac{3}{2}\\right)=5.30\\) to 2 d.p. This is in the first quadrant. \\(r_2=5\\), \\(\\theta_2=\\tan^{-1}\\left(\\frac{4}{3}\\right)=0.93\\) to 2 d.p. In Cartesian form: \\(\\frac{z_1-2}{z_2}=-\\frac{12}{25}-\\frac{9}{25}i\\). This is in the third quadrant. \\(r=\\frac{3}{5}\\), \\(\\theta=\\pi + \\tan^{-1}\\left(\\frac{9/25}{12/25}\\right)=3.79\\) to 2 d.p. \\(z_1z_2=-1-7i\\). Using the result from the previous part we have \\[z_1z_2-\\frac{z_1-2}{z_2}=-1-7i +\\frac{12}{25}+\\frac{9}{25}i= -\\frac{13}{25}-\\frac{166}{25}i\\] This is in the third quadrant. \\(r=\\frac{\\sqrt{1109}}{5}\\), \\(\\theta=4.63\\) to 2 d.p. This is in the second quadrant. \\(r_1=2\\), \\(\\theta_1=\\pi-\\tan^{-1}\\frac{\\sqrt{2}}{\\sqrt{2}}=\\frac{3\\pi}{4}\\). This is in the second quadrant. \\(r_2=1\\), \\(\\theta_2=\\frac{2\\pi}{3}.\\) \\(w_1w_2\\) is easy to calculate in exponential form: \\(w_1w_2=r_1e^{i\\theta_1}r_2e^{i\\theta_2}=r_1r_2e^{i(\\theta_1+\\theta_2)}=2e^{i(\\frac{17\\pi}{12})}\\). \\(\\frac{w_1}{w_2}\\) is easy to calculate in exponential form: \\(\\frac{w_1}{w_2}=r_1e^{i\\theta_1}/r_2e^{i\\theta_2}=\\frac{r_1}{r_2}e^{i(\\theta_1-\\theta_2)}=2e^{i(\\frac{\\pi}{12})}\\). Express the following complex numbers in Cartesian form. \\(2e^{i\\frac{\\pi}{12}}\\) \\(5e^{i 23^\\circ}\\) \\(2e^{i (-45^\\circ)}\\) Answers: \\(\\frac{1 + \\sqrt{3}}{\\sqrt{2}} + \\frac{\\sqrt{3}-1}{\\sqrt{2}}i\\) \\(4.60+1.95i\\) to 2 d.p. \\(\\sqrt{2} - \\sqrt{2} i\\) Find the following roots and express them in Cartesian form. \\(1^\\frac{1}{3}\\) \\(1^\\frac{1}{4}\\) The cube roots of \\(\\sqrt{2}-\\sqrt{2}i\\) The square roots of \\(3-4i\\) Answers: In exponential form the roots are: \\[1^\\frac{1}{3}= \\begin{cases} e^{i0}\\\\ e^{i\\frac{2\\pi}{3}}\\\\ e^{i\\frac{4\\pi}{3}} \\end{cases}. \\] These translate to the Cartesian forms: \\[1^\\frac{1}{3}= \\begin{cases} 1\\\\ -\\frac{1}{2} + \\sqrt{3}/2 i\\\\ -\\frac{1}{2} - \\sqrt{3}/2 i \\end{cases}. \\] In exponential form the roots are: \\[1^\\frac{1}{4}= \\begin{cases} e^{i0}\\\\ e^{i\\frac{2\\pi}{4}}=e^{i\\frac{\\pi}{2}}\\\\ e^{i\\frac{4\\pi}{4}}=e^{i\\pi}\\\\ e^{i\\frac{6\\pi}{4}}=e^{i\\frac{3\\pi}{2}}\\\\ \\end{cases}. \\] These translate to the Cartesian forms: \\[1^\\frac{1}{4}= \\begin{cases} 1\\\\ i\\\\ -1\\\\ -i \\end{cases}. \\] In exponential form the roots are: \\[(\\sqrt{2}-\\sqrt{2}i)^\\frac{1}{3}= \\begin{cases} \\sqrt[3]{2}e^{i(-\\frac{\\pi}{12})}\\\\ \\sqrt[3]{2}e^{i(-\\frac{\\pi}{12}+\\frac{2\\pi}{3})}=\\sqrt[3]{2}e^{i\\frac{7\\pi}{12}}\\\\ \\sqrt[3]{2}e^{i(-\\frac{\\pi}{12}+\\frac{4\\pi}{3})}=\\sqrt[3]{2}e^{i\\frac{5\\pi}{4}} \\end{cases}. \\] These translate to the Cartesian forms: \\[(\\sqrt{2}-\\sqrt{2}i)= \\begin{cases} \\frac{1 + \\sqrt(3)}{2^{7/6}} - \\frac{\\sqrt{3} - 1}{2^{7/6}}i\\\\ \\frac{1 - \\sqrt(3)}{2^{7/6}} - \\frac{1+\\sqrt{3}}{2^{7/6}}i\\\\ -\\frac{1}{2^{1/6}} + \\frac{1}{2^{1/6}}i\\\\ \\end{cases}. \\] This one can be factorised (relatively) easily: \\[3-4i=4-4i+i^2=(2-i)^2\\] so that \\[\\sqrt{3-4i}=\\pm(2-i)=\\begin{cases} 2-i\\\\ -2+i \\end{cases}.\\] Sketch the following complex numbers in the complex plane. \\(2+3i\\) \\(-1-i\\) \\(-1+i\\) \\(-2i\\) \\(-3\\) Answers: Figure 15.1: The numbers in Question 8 in the complex plane. What is \\(i^i\\) in Cartesian form? Hint: start by writing \\(i\\) in exponential form. Find all complex numbers \\(z\\) such that \\(\\overline{z}=z^2\\). Hint: start by writing \\(z\\) in exponential form. "],["exercise-set-5.html", "Exercise Set 5", " Exercise Set 5 These exercises cover the topic of vectors. Note that the questions are independent of one another, so the definition of a vector \\(\\mathbf{a}\\) in one question does not carry over to another question with a vector called \\(\\mathbf{a}\\). The points \\(A\\), \\(B\\) and \\(C\\) have position vectors \\[\\begin{align*} \\mathbf{a}&amp;=5\\mathbf{i}-6\\mathbf{j}+4\\mathbf{k}\\\\ \\mathbf{b}&amp;=2\\mathbf{i}+5\\mathbf{j}-3\\mathbf{k}\\\\ \\mathbf{c}&amp;=-\\mathbf{i}+4\\mathbf{k}. \\end{align*}\\] Find: \\(5\\mathbf{a}-2\\mathbf{b}-3\\mathbf{c}\\) \\(\\overrightarrow{AC}+\\overrightarrow{AB}\\) \\(\\mathbf{a}\\cdot\\mathbf{b}\\) The angle between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) \\(\\mathbf{a}\\times\\mathbf{c}\\) Let \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) be vectors. What is the vector \\(\\mathbf{r}\\) joining the points whose position vectors are \\(\\mathbf{p}\\) and \\(2\\mathbf{q}\\)? What is the vector \\(\\mathbf{s}\\) joing the points whose position vectors are \\(\\mathbf{2q}\\) and \\(-6\\mathbf{p}+14\\mathbf{q}\\)? What do you notice about \\(\\mathbf{r}\\) and \\(\\mathbf{s}\\)? What does this tell you about the points whose position vectors are \\(\\mathbf{p}\\), \\(2\\mathbf{q}\\) and \\(-6\\mathbf{p}+14{q}\\)? Let \\(\\mathbf{u}=5\\mathbf{i}+2\\mathbf{j}\\) and \\(\\mathbf{v}=3\\mathbf{i}-\\mathbf{j}-3\\mathbf{k}\\). Find \\((4\\mathbf{u}-6\\mathbf{v})\\cdot(2\\mathbf{u}+3\\mathbf{v})\\). Given that the position vectors of the points \\(A\\), \\(B\\), and \\(C\\) are \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\), write down the position vectors of the midpoints of the lines \\(BC\\) and \\(CA\\). Prove that the vector joining the midpoints of two sides of a triangle is parallel to the third side and half of its length. Show that the vectors \\[\\begin{align*} \\mathbf{a}&amp;=8\\mathbf{i}+2\\mathbf{j}-3\\mathbf{k}\\\\ \\mathbf{b}&amp;=3\\mathbf{i}-6\\mathbf{j}+4\\mathbf{k} \\end{align*}\\] are perpendicular to one another (Hint: use the dot product). A block of mass \\(m=1\\,\\text{kg}\\) is sliding down a frictionless slope under gravity. The slope is at an angle of \\(45^\\circ\\) to the horizontal. Recall that the force on the block is given by Newton’s second law: \\(\\mathbf{F}=m\\mathbf{g}\\) where \\(\\mathbf{g}=-9.8\\mathbf{k}\\,\\text{ms}^{-2}\\) is the gravitational acceleration. Resolve the force vector in the directions parallel and perpendicular to the slope. (Hint: you need to project the force vector onto unit vectors that are parallel and perpendicular to the slope.) Let \\[\\mathbf{a}=\\begin{pmatrix} 2\\\\ -1\\\\ 3 \\end{pmatrix} ,\\qquad \\mathbf{b}=\\begin{pmatrix} 5\\\\ 1\\\\ -4 \\end{pmatrix}. \\] Show that \\((\\mathbf{a}-\\mathbf{b})\\times(\\mathbf{a}+\\mathbf{b})=2\\mathbf{a}\\times\\mathbf{b}\\). Show that \\[(\\mathbf{a}+\\mathbf{b})\\cdot((\\mathbf{b}+\\mathbf{c})\\times (\\mathbf{c}+\\mathbf{a}))=2\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c}).\\] The quantity \\(\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})\\) is known as the scalar triple product: its geometric interpretation is the volume of the parallelepiped defined by the three vectors. Let \\[\\mathbf{a}=\\begin{pmatrix} \\alpha\\\\ 0\\\\ -1 \\end{pmatrix} ,\\qquad \\mathbf{b}=\\begin{pmatrix} 3\\\\ 2\\\\ 5 \\end{pmatrix} ,\\qquad \\mathbf{c}=\\begin{pmatrix} 2\\\\ -1\\\\ 4 \\end{pmatrix}. \\] Find the values of \\(\\alpha\\) for which \\(\\mathbf{a}\\times\\mathbf{b}\\) is perpendicular to \\(\\mathbf{a}\\times\\mathbf{c}\\). The torque is the rotational equivalent of linear force. It is given by the product of the magnitude of the force and the perpendicular distance of the line of action of a force from the axis of rotation. We can calculate torque using the cross product: \\[\\boldsymbol\\tau = \\mathbf{r}\\times \\mathbf{F}\\] where \\(\\boldsymbol\\tau\\) is the torque vector, \\(\\mathbf{r}\\) is the position vector (from the axis of rotation to the point where the force is applied) and \\(\\mathbf{F}\\) is the force vector. Find the torque about the point \\(\\mathbf{i}+2\\mathbf{j}-\\mathbf{k}\\) of a force \\(\\mathbf{F}=3\\mathbf{i}+\\mathbf{k}\\) acting on a particle at position \\(2\\mathbf{i}-\\mathbf{j}+3\\mathbf{k}\\). "],["exercise-set-5-answers.html", "Exercise Set 5 Answers", " Exercise Set 5 Answers These exercises cover the topic of vectors. Note that the questions are independent of one another, so the definition of a vector \\(\\mathbf{a}\\) in one question does not carry over to another question with a vector called \\(\\mathbf{a}\\). The points \\(A\\), \\(B\\) and \\(C\\) have position vectors \\[\\begin{align*} \\mathbf{a}&amp;=5\\mathbf{i}-6\\mathbf{j}+4\\mathbf{k}\\\\ \\mathbf{b}&amp;=2\\mathbf{i}+5\\mathbf{j}-3\\mathbf{k}\\\\ \\mathbf{c}&amp;=-\\mathbf{i}+4\\mathbf{k}. \\end{align*}\\] Find: \\(5\\mathbf{a}-2\\mathbf{b}-3\\mathbf{c}\\) \\(\\overrightarrow{AC}+\\overrightarrow{AB}\\) \\(\\mathbf{a}\\cdot\\mathbf{b}\\) The angle between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) \\(\\mathbf{a}\\times\\mathbf{c}\\) Answers: \\(5\\mathbf{a}-2\\mathbf{b}-3\\mathbf{c}=24\\mathbf{i}-40\\mathbf{j}+14\\mathbf{k}\\) \\(\\overrightarrow{AC}=\\mathbf{c}-\\mathbf{a}=-6\\mathbf{i}+6\\mathbf{j}\\) \\(\\overrightarrow{AB}=\\mathbf{b}-\\mathbf{a}=-3\\mathbf{i}+11\\mathbf{j}-7\\mathbf{k}\\) \\(\\overrightarrow{AC}+\\overrightarrow{AB}=-9\\mathbf{i}+17\\mathbf{j}-7\\mathbf{k}\\) \\(\\mathbf{a}\\cdot \\mathbf{b}=(5\\times 2)+ (-6\\times5)+(4\\times -3)=-32\\) \\(|\\mathbf{a}|=\\sqrt{77}\\) \\(|\\mathbf{b}|=\\sqrt{38}\\) \\(\\cos(\\theta)=\\frac{\\mathbf{a}\\cdot \\mathbf{b}}{|\\mathbf{a}||\\mathbf{b}|}=\\frac{-32}{\\sqrt{77}\\sqrt{38}}=-0.5916\\) \\(\\theta=\\cos^{-1}(-0.5916)=126.3^\\circ\\) \\[\\begin{align*} \\mathbf{a}\\times \\mathbf{c}&amp;=\\begin{vmatrix} \\mathbf{i}&amp;\\mathbf{j}&amp;\\mathbf{k}\\\\ 5&amp;-6&amp;4\\\\ -1&amp;0&amp;4 \\end{vmatrix}\\\\ &amp;= \\mathbf{i}\\begin{vmatrix} -6&amp;4\\\\ 0&amp;4 \\end{vmatrix} -\\mathbf{j}\\begin{vmatrix} 5&amp;4\\\\ -1&amp;4 \\end{vmatrix} +\\mathbf{k}\\begin{vmatrix} 5&amp;-6\\\\ -1&amp;0 \\end{vmatrix}\\\\ &amp;=\\mathbf{i}(-24-0)-\\mathbf{j}(20-(-4))+\\mathbf{k}(0-6)\\\\ &amp;=-24\\mathbf{i}-24\\mathbf{j}-6\\mathbf{k} \\end{align*}\\] Let \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) be vectors. What is the vector \\(\\mathbf{r}\\) joining the points whose position vectors are \\(\\mathbf{p}\\) and \\(2\\mathbf{q}\\)? What is the vector \\(\\mathbf{s}\\) joing the points whose position vectors are \\(\\mathbf{2q}\\) and \\(-6\\mathbf{p}+14\\mathbf{q}\\)? What do you notice about \\(\\mathbf{r}\\) and \\(\\mathbf{s}\\)? What does this tell you about the points whose position vectors are \\(\\mathbf{p}\\), \\(2\\mathbf{q}\\) and \\(-6\\mathbf{p}+14{q}\\)? Answers: \\(\\mathbf{r}=2\\mathbf{q}-\\mathbf{p}\\) \\(\\mathbf{s}=(-6\\mathbf{p}+14\\mathbf{q})-2\\mathbf{q}=12\\mathbf{q}-6\\mathbf{p}\\) They are parallel: \\(\\mathbf{s}=6\\mathbf{r}\\) so they have the same direction. The points are collinear, i.e. all lie in a straight line. Let \\(\\mathbf{u}=5\\mathbf{i}+2\\mathbf{j}\\) and \\(\\mathbf{v}=3\\mathbf{i}-\\mathbf{j}-3\\mathbf{k}\\). Find \\((4\\mathbf{u}-6\\mathbf{v})\\cdot(2\\mathbf{u}+3\\mathbf{v})\\). Answers: \\((2\\mathbf{i}+14\\mathbf{j}+\\mathbf{k})\\cdot(19\\mathbf{i}+\\mathbf{j}-9\\mathbf{k})=38+14-162=-110.\\) Given that the position vectors of the points \\(A\\), \\(B\\), and \\(C\\) are \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\), write down the position vectors of the midpoints of the lines \\(BC\\) and \\(CA\\). Answers: \\(\\overline{BC}=\\mathbf{c}-\\mathbf{b}\\) \\(\\overline{OE}=\\mathbf{b}+\\frac{1}{2}(\\mathbf{c}-\\mathbf{b})=\\frac{1}{2}\\mathbf{c}-\\frac{1}{2}\\mathbf{b}\\) \\(\\overline{CA}=\\mathbf{a}-\\mathbf{c}\\) \\(\\overline{OF}=\\mathbf{c}+\\frac{1}{2}(\\mathbf{a}-\\mathbf{c})=\\frac{1}{2}\\mathbf{a}-\\frac{1}{2}\\mathbf{c}\\) Prove that the vector joining the midpoints of two sides of a triangle is parallel to the third side and half of its length. Answers: We need to set up some labels for the points of the triangle. Let these have position vectors \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}\\). The vectors defining the sides of the triangle are \\(\\mathbf{a}=\\overline{BC}=\\mathbf{C}-\\mathbf{B}\\) \\(\\mathbf{b}=\\overline{CA}=\\mathbf{A}-\\mathbf{C}\\) \\(\\mathbf{c}=\\overline{AB}=\\mathbf{B}-\\mathbf{A}\\) The midpoint of side \\(CA\\) is \\(\\mathbf{C}+\\frac{1}{2}\\mathbf{b}=\\mathbf{C}+\\frac{1}{2}\\mathbf{A}-\\frac{1}{2}\\mathbf{C}=\\frac{1}{2}\\mathbf{C}+\\frac{1}{2}\\mathbf{A}\\) The midpoint of side \\(AB\\) is \\(\\mathbf{A}+\\frac{1}{2}\\mathbf{c}=\\mathbf{A}+\\frac{1}{2}\\mathbf{B}-\\frac{1}{2}\\mathbf{A}=\\frac{1}{2}\\mathbf{A}+\\frac{1}{2}\\mathbf{B}\\) The vector between these midpoints is \\[(\\frac{1}{2}\\mathbf{C}+\\frac{1}{2}\\mathbf{A})-(\\frac{1}{2}\\mathbf{A}+\\frac{1}{2}\\mathbf{B})=\\frac{1}{2}\\mathbf{C}-\\frac{1}{2}\\mathbf{B}\\] which is half of the vector \\(\\mathbf{a}=\\overline{BC}\\), as required. Show that the vectors \\[\\begin{align*} \\mathbf{a}&amp;=8\\mathbf{i}+2\\mathbf{j}-3\\mathbf{k}\\\\ \\mathbf{b}&amp;=3\\mathbf{i}-6\\mathbf{j}+4\\mathbf{k} \\end{align*}\\] are perpendicular to one another (Hint: use the dot product). Answers: Using the dot product: \\(\\mathbf{a}\\cdot\\mathbf{b}=(8\\times 3)+(2\\times-6)+(-3\\times 4)=24-12-12=0\\) and since this is zero and the vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are non-zero, these are perpendicular vectors. A block of mass \\(m=1\\,\\text{kg}\\) is sliding down a frictionless slope under gravity. The slope is at an angle of \\(45^\\circ\\) to the horizontal. Recall that the force on the block is given by Newton’s second law: \\(\\mathbf{F}=m\\mathbf{g}\\) where \\(\\mathbf{g}=-9.8\\mathbf{k}\\,\\text{ms}^{-2}\\) is the gravitational acceleration. Resolve the force vector in the directions parallel and perpendicular to the slope. (Hint: you need to project the force vector onto unit vectors that are parallel and perpendicular to the slope.) Answers: We can simplify this to a two dimensional problem by aligning the coordinate axes sensibly. Let’s align the axes so that the \\(y\\) axis is perpendicular to the slope and the slope is increasing in the \\(x\\) direction, then we just need the \\(\\mathbf{i}\\) and \\(\\mathbf{k}\\) directions. The unit vector parallel to the (increasing) slope is \\(\\mathbf{u}=\\frac{1}{\\sqrt{2}}\\mathbf{i}+\\frac{1}{\\sqrt{2}}\\mathbf{k}\\) and the unit vector perpendicular to the slope is \\(\\mathbf{v}=-\\frac{1}{\\sqrt{2}}\\mathbf{i}+\\frac{1}{\\sqrt{2}}\\mathbf{k}\\). The force vector is \\(\\mathbf{F}=m\\mathbf{g}=-9.8\\mathbf{k}\\). First projecting onto the parallel vector: \\(\\mathbf{F}\\cdot \\mathbf{u}=(0\\times \\frac{1}{\\sqrt{2}})+ (-9.8 \\times \\frac{1}{\\sqrt{2}})=-6.9\\). and then onto the perpendicular vector: \\(\\mathbf{F}\\cdot \\mathbf{v}=(0\\times -\\frac{1}{\\sqrt{2}})+ (-9.8 \\times \\frac{1}{\\sqrt{2}})=-6.9\\). Hence we have \\(\\mathbf{F}=-6.9\\mathbf{u}-6.9\\mathbf{v}\\). Let \\[\\mathbf{a}=\\begin{pmatrix} 2\\\\ -1\\\\ 3 \\end{pmatrix} ,\\qquad \\mathbf{b}=\\begin{pmatrix} 5\\\\ 1\\\\ -4 \\end{pmatrix}. \\] Show that \\((\\mathbf{a}-\\mathbf{b})\\times(\\mathbf{a}+\\mathbf{b})=2\\mathbf{a}\\times\\mathbf{b}\\). Answers: \\(\\mathbf{a}-\\mathbf{b}=-3\\mathbf{i}-2\\mathbf{j}+7\\mathbf{k}\\) \\(\\mathbf{a}+\\mathbf{b}=7\\mathbf{i}-\\mathbf{k}\\) \\[\\begin{align*} (\\mathbf{a}-\\mathbf{b})\\times(\\mathbf{a}+\\mathbf{b})&amp;= \\begin{vmatrix} \\mathbf{i}&amp;\\mathbf{j}&amp;\\mathbf{k}\\\\ -3&amp;-2&amp;7\\\\ 7&amp;0&amp;-1 \\end{vmatrix}\\\\ &amp;= \\mathbf{i}\\begin{vmatrix} -2&amp;7\\\\ 0&amp;-1 \\end{vmatrix} -\\mathbf{j}\\begin{vmatrix} -3&amp;7\\\\ 7&amp;-1 \\end{vmatrix} +\\mathbf{k}\\begin{vmatrix} -3&amp;-2\\\\ 7&amp;0 \\end{vmatrix}\\\\ &amp;=\\mathbf{i}(2-0)-\\mathbf{j}(3-49)+\\mathbf{k}(0-(-14))\\\\ &amp;=2\\mathbf{i}+46\\mathbf{j}+14\\mathbf{k} \\end{align*}\\] \\[\\begin{align*} 2\\mathbf{a}\\times\\mathbf{b}&amp;= \\begin{vmatrix} \\mathbf{i}&amp;\\mathbf{j}&amp;\\mathbf{k}\\\\ 4&amp;-2&amp;6\\\\ 5&amp;1&amp;-4 \\end{vmatrix}\\\\ &amp;= \\mathbf{i}\\begin{vmatrix} -2&amp;6\\\\ 1&amp;-4 \\end{vmatrix} -\\mathbf{j}\\begin{vmatrix} 4&amp;6\\\\ 5&amp;-4 \\end{vmatrix} +\\mathbf{k}\\begin{vmatrix} 4&amp;-2\\\\ 5&amp;1 \\end{vmatrix}\\\\ &amp;=\\mathbf{i}(8-6)-\\mathbf{j}(-16-30)+\\mathbf{k}(4-(-10))\\\\ &amp;=2\\mathbf{i}+46\\mathbf{j}+14\\mathbf{k} \\end{align*}\\] Hence \\((\\mathbf{a}-\\mathbf{b})\\times(\\mathbf{a}+\\mathbf{b})=2\\mathbf{a}\\times\\mathbf{b}\\) Show that \\[(\\mathbf{a}+\\mathbf{b})\\cdot((\\mathbf{b}+\\mathbf{c})\\times (\\mathbf{c}+\\mathbf{a}))=2\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c}).\\] The quantity \\(\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})\\) is known as the scalar triple product: its geometric interpretation is the volume of the parallelepiped defined by the three vectors. Answers: \\[\\begin{align*} (\\mathbf{a}+\\mathbf{b})\\cdot((\\mathbf{b}+\\mathbf{c})\\times (\\mathbf{c}+\\mathbf{a}))&amp;=(\\mathbf{a}+\\mathbf{b})\\cdot(\\mathbf{b}\\times\\mathbf{c}+\\mathbf{b}\\times\\mathbf{a}+\\mathbf{c}\\times\\mathbf{c}+\\mathbf{c}\\times\\mathbf{a})\\\\ &amp;=(\\mathbf{a}+\\mathbf{b})\\cdot(\\mathbf{b}\\times\\mathbf{c}+\\mathbf{b}\\times\\mathbf{a}+\\mathbf{c}\\times\\mathbf{a})\\\\ &amp;=\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})+(\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{a})+\\mathbf{a}\\cdot(\\mathbf{c}\\times\\mathbf{a})+\\mathbf{b}\\cdot(\\mathbf{b}\\times\\mathbf{c})+(\\mathbf{b}\\cdot(\\mathbf{b}\\times\\mathbf{a})+\\mathbf{b}\\cdot(\\mathbf{c}\\times\\mathbf{a})\\\\ &amp;=\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})+\\mathbf{b}\\cdot(\\mathbf{c}\\times\\mathbf{a})\\\\ &amp;=\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})+\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})\\\\ &amp;=2\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c}) \\end{align*}\\] Let \\[\\mathbf{a}=\\begin{pmatrix} \\alpha\\\\ 0\\\\ -1 \\end{pmatrix} ,\\qquad \\mathbf{b}=\\begin{pmatrix} 3\\\\ 2\\\\ 5 \\end{pmatrix} ,\\qquad \\mathbf{c}=\\begin{pmatrix} 2\\\\ -1\\\\ 4 \\end{pmatrix}. \\] Find the values of \\(\\alpha\\) for which \\(\\mathbf{a}\\times\\mathbf{b}\\) is perpendicular to \\(\\mathbf{a}\\times\\mathbf{c}\\). Answers: We find: \\(\\mathbf{a}\\times\\mathbf{b}=2\\mathbf{i}-(5\\alpha+3)\\mathbf{j}+2\\alpha\\mathbf{k}\\) and \\(\\mathbf{a}\\times\\mathbf{c}=-\\mathbf{i}-(4\\alpha+2)\\mathbf{j}+\\alpha\\mathbf{k}\\). Then for perpendicularity we solve \\((\\mathbf{a}\\times\\mathbf{b})\\cdot(\\mathbf{a}\\times\\mathbf{c})=0\\). We have \\[ (\\mathbf{a}\\times\\mathbf{b})\\cdot(\\mathbf{a}\\times\\mathbf{c})=(9\\alpha+2)(\\alpha+1) \\] Hence \\[\\alpha=-\\frac{2}{9},\\quad\\text{or}\\quad \\alpha={-1}.\\] The torque is the rotational equivalent of linear force. It is given by the product of the magnitude of the force and the perpendicular distance of the line of action of a force from the axis of rotation. We can calculate torque using the cross product: \\[\\boldsymbol\\tau = \\mathbf{r}\\times \\mathbf{F}\\] where \\(\\boldsymbol\\tau\\) is the torque vector, \\(\\mathbf{r}\\) is the position vector (from the axis of rotation to the point where the force is applied) and \\(\\mathbf{F}\\) is the force vector. Find the torque about the point \\(\\mathbf{i}+2\\mathbf{j}-\\mathbf{k}\\) of a force \\(\\mathbf{F}=3\\mathbf{i}+\\mathbf{k}\\) acting on a particle at position \\(2\\mathbf{i}-\\mathbf{j}+3\\mathbf{k}\\). Answers: \\(\\mathbf{r}=(2\\mathbf{i}-\\mathbf{j}+3\\mathbf{k})-(\\mathbf{i}+2\\mathbf{j}-\\mathbf{k})=\\mathbf{i}-3\\mathbf{j}+4\\mathbf{k}\\) Then \\[\\begin{align*} \\mathbf{r}\\times \\mathbf{F}&amp;= \\begin{vmatrix} \\mathbf{i}&amp;\\mathbf{j}&amp;\\mathbf{k}\\\\ 1&amp;-3&amp;4\\\\ 3&amp;0&amp;1 \\end{vmatrix}\\\\ &amp;=-3\\mathbf{i}+11\\mathbf{j}+9\\mathbf{k}. \\end{align*}\\] "],["exercise-set-6.html", "Exercise Set 6", " Exercise Set 6 These exercises cover solving linear equations by Gaussian elimination. Use Gaussian elimination (elementary row operations) to determine whether or not the following systems of equations have solutions, and, when they do, to find all solutions. In the cases where there is more than one solution, find the general solution and also give one particular solution. (Tip: Choosing the elementary row operations carefully can minimise the use of fractions). \\[\\begin{align*} x + y - z &amp;= -3, \\\\ 2x + 2y - z &amp;= -2, \\\\ 3x + 2y - 3z&amp;= -7; \\end{align*}\\] \\[\\begin{align*} 2x - 2y + 3z&amp;= 5, \\\\ 3x - 2y + 4z&amp;= 6, \\\\ 4x + y + 2z&amp;= 2; \\end{align*}\\] \\[\\begin{align*} 3x + 2y + 13z&amp;= 2, \\\\ 4x + 2y + 16z&amp;= 1, \\\\ x + y + 5z &amp;= 1; \\end{align*}\\] \\[\\begin{align*} 2x + z &amp;= 3, \\\\ x + y + z &amp;= 2, \\\\ x + 3y + 2z&amp;= 3; \\end{align*}\\] \\[\\begin{align*} 2x + 4y + z - 3t&amp;= 7, \\\\ 5x + 10y + 6z - 4t&amp;= 14, \\\\ 7x + 14y + 5z - 9t&amp;= 23; \\end{align*}\\] \\[\\begin{align*} 3x + 4y &amp;= 1, \\\\ 2x - 4y + 2z&amp;= 4, \\\\ 3x + z &amp;= 2, \\\\ x - 2y + z &amp;= 2; \\end{align*}\\] \\[\\begin{align*} 3x - y + 2z + t&amp;= 3, \\\\ x + 2y - z + t&amp;= -3, \\\\ 3x + y + z + t&amp;= 0, \\\\ 3x + 2y + t&amp;= -1. \\end{align*}\\] Balancing chemical equations. In the combustion of propane gas, propane (\\(C_3H_8\\)) combines with oxygen (\\(O_2\\)) to form carbon dioxide (\\(CO_2\\)) and water (\\(H_2O\\)) according to the equation \\[x_1 C_3H_8 + x_2 O_2\\to x_3CO_2+x_4H_2O.\\] To balance this equation, we must find natural numbers \\(x_1, x_2, x_3, x_4\\) such that the total numbers of \\(C\\), \\(H\\) and \\(O\\) atoms are equal on both sides. Formulate this problem as a set of linear equations and solve using Guassian elimination. Electrical networks. Consider the circuit diagram below. The voltage drop across a resistor is (by Ohm’s law) \\(V=IR\\) where \\(V\\) is in volts, the current \\(I\\) is in amps and the resistance \\(R\\) is in ohms. Kirchhoff’s junction law: the sum of currents flowing into a junction is equal to the sum of currents flowing out of that junction. Kirchoff’s Voltage law: The sum of the voltage drops in one direction around a loop equals the sum of the voltage sources in the same direction around the loop. Using these laws it is possible to derive the following set of linear equations for the currents \\(I_1, I_2, I_3\\): \\[\\begin{align*} 11I_1-3I_2&amp;=30\\\\ -3I_1+6I_2-I_3&amp;=5\\\\ -I_2+3I_3&amp;=-25. \\end{align*}\\] Determine the currents. (Extra challenge: derive the equations!). Network analysis. The diagram below represents the traffic flow in a one-way road network. Each directed edge represents a one-way road and is labelled with the flow through that road. The nodes represent road junctions. By considering the flow in and out of each node and the total flow in and out of the network, formulate a system of linear equations for the unknown flows \\(x_1,\\dotsc,x_5\\) and solve them by Gaussian elimination. You will find there is a “free variable”, but the requirement for non-negative flows puts some restrictions on the variables. In particular, what are the restrictions on \\(x_1\\) and \\(x_5\\)? "],["exercise-set-6-answers.html", "Exercise Set 6 Answers", " Exercise Set 6 Answers These exercises cover solving linear equations by Gaussian elimination. Use Gaussian elimination (elementary row operations) to determine whether or not the following systems of equations have solutions, and, when they do, to find all solutions. In the cases where there is more than one solution, find the general solution and also give one particular solution. (Tip: Choosing the elementary row operations carefully can minimise the use of fractions). \\[\\begin{align*} x + y - z &amp;= -3, \\\\ 2x + 2y - z &amp;= -2, \\\\ 3x + 2y - 3z&amp;= -7; \\end{align*}\\] \\[\\begin{align*} 2x - 2y + 3z&amp;= 5, \\\\ 3x - 2y + 4z&amp;= 6, \\\\ 4x + y + 2z&amp;= 2; \\end{align*}\\] \\[\\begin{align*} 3x + 2y + 13z&amp;= 2, \\\\ 4x + 2y + 16z&amp;= 1, \\\\ x + y + 5z &amp;= 1; \\end{align*}\\] \\[\\begin{align*} 2x + z &amp;= 3, \\\\ x + y + z &amp;= 2, \\\\ x + 3y + 2z&amp;= 3; \\end{align*}\\] \\[\\begin{align*} 2x + 4y + z - 3t&amp;= 7, \\\\ 5x + 10y + 6z - 4t&amp;= 14, \\\\ 7x + 14y + 5z - 9t&amp;= 23; \\end{align*}\\] \\[\\begin{align*} 3x + 4y &amp;= 1, \\\\ 2x - 4y + 2z&amp;= 4, \\\\ 3x + z &amp;= 2, \\\\ x - 2y + z &amp;= 2; \\end{align*}\\] \\[\\begin{align*} 3x - y + 2z + t&amp;= 3, \\\\ x + 2y - z + t&amp;= -3, \\\\ 3x + y + z + t&amp;= 0, \\\\ 3x + 2y + t&amp;= -1. \\end{align*}\\] Answers: We have the augmented matrix \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; -1 &amp; -3\\\\ 2 &amp; 2 &amp; -1 &amp; -2 \\\\ 3 &amp; 2 &amp; -3 &amp; -7 \\end{array}\\right) \\] Eliminate entries below the leading entry in the first column by performing \\(R_2 \\to R_2 - 2 R_1\\) and \\(R_3 \\to R_3 - 3R_1\\). We have \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; -1 &amp; -3\\\\ 0 &amp; 0 &amp; 1 &amp; 4 \\\\ 0 &amp; -1 &amp; 0 &amp; 2 \\end{array}\\right). \\] Swap the bottom two rows, i.e. \\(R_2 \\leftrightarrow R_3\\), to obtain \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; -1 &amp; -3\\\\ 0 &amp; -1 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 4 \\end{array}\\right). \\] The matrix is now in echelon form. Eliminate the entries in column above the leading entry in column \\(3\\), that is applying \\(R_1 \\to R_1 + R_3\\) gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; -1 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 4 \\end{array}\\right). \\] Then, using \\(R_2 \\to -R_2\\), we have \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 4 \\end{array}\\right) \\] Eliminate entries in the column above the leading entry in column \\(2\\), i.e. apply \\(R_1 \\to R_1 - R_2\\) to obtain \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; 3\\\\ 0 &amp; 1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 4 \\end{array}\\right) \\] The matrix is now in reduced echelon form and we read off the solution \\[ (x,y,z) = (3,-2,4). \\] We have the augmented matrix \\[ \\left(\\begin{array}{rrr|r} 2 &amp; -2 &amp; 3 &amp; 5\\\\ 3 &amp; -2 &amp; 4 &amp; 6\\\\ 4 &amp; 1 &amp; 2 &amp; 2 \\end{array}\\right) \\] We eliminate the entries below the leading entry in column \\(1\\). Performing \\(R_2 \\to R_2 - \\frac{3}{2}R_1\\) and \\(R_3 \\to R_3 - 2R_1\\) we arrive at \\[ \\left(\\begin{array}{rrr|r} 2 &amp; -2 &amp; 3 &amp; 5\\\\ 0 &amp; 1 &amp; -\\frac{1}{2} &amp; -\\frac{3}{2}\\\\ 0 &amp; 5 &amp; -4 &amp; -8 \\end{array}\\right). \\] Next, we eliminate the entry below the leading entry in column \\(2\\) via \\(R_3 \\to R_3 - 5R_2\\) to get \\[ \\left(\\begin{array}{rrr|r} 2 &amp; -2 &amp; 3 &amp; 5\\\\ 0 &amp; 1 &amp; -\\frac{1}{2} &amp; -\\frac{3}{2}\\\\ 0 &amp; 0 &amp; -\\frac{3}{2} &amp; -\\frac{1}{2} \\end{array}\\right). \\] The matrix is now in echelon form. We proceed to bring the matrix into reduced echelon form. Multiply Row \\(3\\) by \\(- \\frac{2}{3}\\), i.e. \\(R_2 \\to - \\frac{2}{3}R_3\\), to have \\[ \\left(\\begin{array}{rrr|r} 2 &amp; -2 &amp; 3 &amp; 5\\\\ 0 &amp; 1 &amp; -\\frac{1}{2} &amp; -\\frac{3}{2}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{1}{3} \\end{array}\\right). \\] To eliminate the entries above the leading entry in column \\(3\\), we perform the operations \\(R_2 \\to R_2 + \\frac{1}{2}R_3\\) and \\(R_1 \\to R_1 - 3R_3\\), which give \\[ \\left(\\begin{array}{rrr|r} 2 &amp; -2 &amp; 0 &amp; 4\\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{4}{3}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{1}{3} \\end{array}\\right) \\] To eliminate the entry above the leading entry in column \\(2\\), we perform the operation \\(R_1 \\to R_1 + 2R_2\\), which gives \\[ \\left(\\begin{array}{rrr|r} 2 &amp; 0 &amp; 0 &amp; \\frac{4}{3}\\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{4}{3}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{1}{3} \\end{array}\\right). \\] Applying \\(R_1 \\to \\frac{1}{2}R_1\\) gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; \\frac{2}{3}\\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{4}{3}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{1}{3} \\end{array}\\right). \\] The matrix is now in reduced echelon form and we read off the solution \\[ (x,y,z) = \\frac{1}{3}(2,-4,1). \\] We have the augmented matrix \\[ \\left(\\begin{array}{rrr|r} 3 &amp; 2 &amp; 13 &amp; 2 \\\\ 4 &amp; 2 &amp; 16 &amp; 1 \\\\ 1 &amp; 1 &amp; 5 &amp; 1 \\end{array}\\right). \\] We start by swapping rows so that we have a small number in the top-left corner — this avoids having to deal with fractions — applying \\(R_1 \\leftrightarrow R_3\\) and then \\(R_2 \\leftrightarrow R_3\\) gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 5 &amp; 1 \\\\ 3 &amp; 2 &amp; 13 &amp; 2 \\\\ 4 &amp; 2 &amp; 16 &amp; 1 \\end{array}\\right). \\] To eliminate the entries below the leading entry in column \\(1\\), we apply \\(R_2 \\to R_2 - 3R_1\\) and \\(R_3 \\to R_3 - 4R_1\\), which give \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 5 &amp; 1 \\\\ 0 &amp; -1 &amp; -2 &amp; -1 \\\\ 0 &amp; -2 &amp; -4 &amp; -3 \\end{array}\\right). \\] To eliminate the entry below the leading entry in column \\(2\\), we apply \\(R_3 \\to R_3 - 2R_2\\), which gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 5 &amp; 1 \\\\ 0 &amp; -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 \\end{array}\\right). \\] The matrix is now in echelon form. There is a leading entry in the right-most column (representing the solution vector), from which we may infer that the system is inconsistent (there are no solutions). We have the augmented matrix \\[ \\left(\\begin{array}{rrr|r} 2 &amp; 0 &amp; 1 &amp; 3\\\\ 1 &amp; 1 &amp; 1 &amp; 2\\\\ 1 &amp; 3 &amp; 2 &amp; 3 \\end{array}\\right) \\] We start by swapping rows, i.e. performing \\(R_1\\leftrightarrow R_3\\) and \\(R_2\\leftrightarrow R_3\\) to get \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 2\\\\ 1 &amp; 3 &amp; 2 &amp; 3 \\\\ 2 &amp; 0 &amp; 1 &amp; 3 \\end{array}\\right). \\] To eliminate the entries below the leading entry in column \\(1\\), we perform the operations \\(R_2 \\to R_2 - R_1\\) and \\(R_3 \\to R_3-2R_1\\), arriving at \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 2\\\\ 0 &amp; 2 &amp; 1 &amp; 1\\\\ 0 &amp; -2 &amp; -1 &amp; -1 \\end{array}\\right). \\] To eliminate the entry below the leading entry in column \\(2\\), we perform the operation \\(R_3 \\to R_3 + R_2\\) and arrive at \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 2\\\\ 0 &amp; 2 &amp; 1 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] The matrix is now in echelon form — the last row is consistent. We proceed to bring the matrix into reduced echelon form. Applying \\(R_2 \\to \\frac{1}{2}R_2\\), we have \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 2\\\\ 0 &amp; 1 &amp; \\frac{1}{2} &amp; \\frac{1}{2}\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] To eliminate the entry above the leading entry in column \\(2\\), we perform the operation \\(R_1 \\to R_1 - R_2\\), which gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{3}{2}\\\\ 0 &amp; 1 &amp; \\frac{1}{2} &amp; \\frac{1}{2}\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] Note that there is no leading entry in column three or in the solution vector, so there are infinitely many solutions, and we can take the variable \\(z\\) to be arbitrary, say \\(z=\\lambda\\). We then read off the solutions \\[ (x,y,z)=\\frac{1}{2}(3-\\lambda,1-\\lambda,2\\lambda). \\] For a particular solution, for instance let \\(\\lambda=0\\), we get \\[ (x,y,z)= \\frac{1}{2}(3,1,0). \\] We have the augmented matrix \\[ \\left(\\begin{array}{rrrr|r} 2 &amp; 4 &amp; 1 &amp; -3 &amp; 7 \\\\ 5 &amp; 10 &amp; 6 &amp; -4 &amp; 14 \\\\ 7 &amp; 14 &amp; 5 &amp; -9 &amp; 23 \\end{array}\\right). \\] Start with \\(R_2 \\to R_2 - \\frac{5}{2}R_1\\) and \\(R_3 \\to R_3 - \\frac{7}{2}R_1\\), which give \\[ \\left(\\begin{array}{rrrr|r} 2 &amp; 4 &amp; 1 &amp; -3 &amp; 7 \\\\ 0 &amp; 0 &amp; \\frac{7}{2} &amp; \\frac{7}{2} &amp; -\\frac{7}{2} \\\\ 0 &amp; 0 &amp; \\frac{3}{2} &amp; \\frac{3}{2} &amp; -\\frac{3}{2} \\end{array}\\right), \\] and perform \\(R_3 \\to R_3 - \\frac{3}{7}R_2\\), to have \\[ \\left(\\begin{array}{rrrr|r} 2 &amp; 4 &amp; 1 &amp; -3 &amp; 7 \\\\ 0 &amp; 0 &amp; \\frac{7}{2} &amp; \\frac{7}{2} &amp; -\\frac{7}{2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] The above is in echelon form. We see that the right-most column contains no leading entry and neither does (at least) one of the other columns. In this case, neither column 2 nor 4 of the matrix of coefficients contains a leading entry. This means that there will be infinitely many solutions. We proceed, scaling rows \\(1\\) and \\(2\\), i.e. applying \\(R_1 \\to \\frac{1}{2}R_1\\) and \\(R_2 \\to \\frac{2}{7}R_2\\), which give \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; \\frac{1}{2} &amp; -\\frac{3}{2} &amp; \\frac{7}{2} \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] Then, we bring the above matrix into reduced echelon form by performing \\(R_1 \\to R_1 - \\frac{1}{2}R_2\\), that is \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; 0 &amp; -2 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] Now we can easily read off the solutions. As there are 2 columns of the matrix of coefficients with no leading entry, two of the variables can be chosen arbitrarily. We take \\(y\\) and \\(t\\) to be arbitrary, say \\(y=\\lambda\\) and \\(t=\\mu\\). Then the frist and second rows give us the equations \\[ x = 4 - 2\\lambda + 2\\mu \\qquad \\text{and} \\qquad z = -1 - \\mu, \\] respectively. So the parameterised solution is \\[ (4 - 2\\lambda + 2\\mu, \\lambda, -1-\\mu, \\mu) \\] where \\(\\lambda\\), \\(\\mu\\) are arbitrary. For a particular solution, let for instance \\(\\lambda = y = 0\\) and \\(\\mu = t = 0\\) to give \\[ (x,y,z,t) = (4,0,-1,0). \\] We swap the rows initially to get the augmented matrix \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 1 &amp; 2\\\\ 3 &amp; 4 &amp; 0 &amp; 1\\\\ 2 &amp; -4 &amp; 2 &amp; 4\\\\ 3 &amp; 0 &amp; 1 &amp; 2 \\end{array}\\right) \\] To eliminate the entries below the leading entry in column \\(1\\), we perform \\(R_2 \\to R_2 - 3R_1\\), \\(R_3 \\to R_3-2R_1\\) and \\(R_4 \\to R_4-3R_1\\) to arrive at \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 1 &amp; 2\\\\ 0 &amp; 10 &amp; -3 &amp; -5\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 6 &amp; -2 &amp; -4 \\end{array}\\right). \\] To eliminate the entry below the leading entry in column \\(2\\), we perform \\(R_4 \\to R_4 - \\frac{3}{5}R_2\\) to have \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 1 &amp; 2\\\\ 0 &amp; 10 &amp; -3 &amp; -5\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; -\\frac{1}{5} &amp; -1 \\end{array}\\right). \\] Applying \\(R_3 \\leftrightarrow R_4\\), we obtain \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 1 &amp; 2\\\\ 0 &amp; 10 &amp; -3 &amp; -5\\\\ 0 &amp; 0 &amp; - \\frac{1}{5} &amp; -1\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] The matrix is now in echelon form. We proceed to bring the matrix into reduced echelon form. Applying \\(R_3 \\to -5R_3\\), gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 1 &amp; 2\\\\ 0 &amp; 10 &amp; -3 &amp; -5\\\\ 0 &amp; 0 &amp; 1 &amp; 5\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] To eliminate the entries above the leading entry in column \\(3\\), we perform \\(R_2 \\to R_2 + 3R_3\\) and \\(R_1 \\to R_1-1R_3\\) to arrive at \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 0 &amp; -3\\\\ 0 &amp; 10 &amp; 0 &amp; 10\\\\ 0 &amp; 0 &amp; 1 &amp; 5\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] Applying \\(R_2 \\to \\frac{1}{10}R_2\\), gives \\[ \\left(\\begin{array}{rrr|r} 1 &amp; -2 &amp; 0 &amp; -3\\\\ 0 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1 &amp; 5\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] To eliminate the entry above the leading entry in column \\(2\\), we perform \\(R_1 \\to R_1 + 2R_2\\) to arrive at \\[ \\left(\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; -1\\\\ 0 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1 &amp; 5\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right). \\] This matrix is in reduced echelon form. Note that even though there is no leading entry in the right hand column, it is the case that every other column contains a leading entry. This means that there is a unique solution to this system of equations. We read off the solution \\[ (x,y,z) = (-1,1,5). \\] We swap the rows initially to get the augmented matrix \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 1 &amp; -3\\\\ 3 &amp; -1 &amp; 2 &amp; 1 &amp; 3\\\\ 3 &amp; 1 &amp; 1 &amp; 1 &amp; 0\\\\ 3 &amp; 2 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right) \\] To eliminate the entries below the leading entry in column \\(1\\), we perform \\(R_2 \\to R_2-3R_1\\), \\(R_3 \\to R_3-3R_1\\) and \\(R_4 \\to R_4-3R_1\\), which give \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 1 &amp; -3\\\\ 0 &amp; -7 &amp; 5 &amp; -2 &amp; 12\\\\ 0 &amp; -5 &amp; 4 &amp; -2 &amp; 9\\\\ 0 &amp; -4 &amp; 3 &amp; -2 &amp; 8 \\end{array}\\right). \\] To avoid fractions, we now perform \\(R_4 \\to R_4 - R_3\\) and then \\(R_4 \\leftrightarrow R_2\\) to obtain \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 1 &amp; -3\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1\\\\ 0 &amp; -5 &amp; 4 &amp; -2 &amp; 9\\\\ 0 &amp; -7 &amp; 5 &amp; -2 &amp; 12 \\end{array}\\right). \\] To eliminate the entries below the leading entry in column \\(2\\), we perform \\(R_3 \\to R_3 + 5R_2\\) and \\(R_4 \\to R_4+7R_2\\) to arrive at \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 1 &amp; -3\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; -1 &amp; -2 &amp; 4\\\\ 0 &amp; 0 &amp; -2 &amp; -2 &amp; 5 \\end{array}\\right). \\] To eliminate the entry under the leading entry in column \\(3\\), we perform \\(R_4 \\to R_4 - 2R_3\\) to have \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 1 &amp; -3\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; -1 &amp; -2 &amp; 4\\\\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; -3 \\end{array}\\right). \\] The matrix is now in echelon form. We proceed to put the matrix into reduced echelon form. Applyin \\(R_4 \\to \\frac{1}{2}R_4\\) gives \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 1 &amp; -3\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; -1 &amp; -2 &amp; 4\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{2} \\end{array}\\right). \\] To eliminate the entries above the leading entry in column \\(4\\), we perform \\(R_3 \\to R_3 + 2R_4\\) and \\(R_1 \\to R_1 - R_4\\) to obtain \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 0 &amp; -\\frac{3}{2}\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{2} \\end{array}\\right). \\] Perform by \\(R_3 \\to -R_3\\) to get \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; -1 &amp; 0 &amp; -\\frac{3}{2}\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{2} \\end{array}\\right). \\] To eliminate the entries above the leading entry in column \\(3\\), we perform \\(R_2 \\to R_2+R_3\\) and \\(R_1 \\to R_1+R_3\\) which give \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 2 &amp; 0 &amp; 0 &amp; -\\frac{5}{2}\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -2\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{2} \\end{array}\\right). \\] Then, we eliminate the entry above the leading entry in column \\(2\\), performing \\(R_1 \\to R_1 - 2R_2\\) and arrive at the reduced echolon form \\[ \\left(\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{3}{2}\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; -2\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{2} \\end{array}\\right). \\] We read off the (unique) solution \\[ (x,y,z,t) = \\left(\\tfrac{3}{2},-2,-1,-\\tfrac{3}{2}\\right). \\] Balancing chemical equations. In the combustion of propane gas, propane (\\(C_3H_8\\)) combines with oxygen (\\(O_2\\)) to form carbon dioxide (\\(CO_2\\)) and water (\\(H_2O\\)) according to the equation \\[x_1 C_3H_8 + x_2 O_2\\to x_3CO_2+x_4H_2O.\\] To balance this equation, we must find natural numbers \\(x_1, x_2, x_3, x_4\\) such that the total numbers of \\(C\\), \\(H\\) and \\(O\\) atoms are equal on both sides. Formulate this problem as a set of linear equations and solve using Guassian elimination. Answer: Write both the left and the right side of the chemical reaction equation as linear equations: \\[ \\begin{pmatrix} 3 &amp; 0 &amp; 0 &amp; 0\\\\ 8 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\end{pmatrix} \\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{pmatrix} = \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2\\\\ 0 &amp; 0 &amp; 2 &amp; 1\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{pmatrix} \\] Now subtract one side from the other to give an equation equal to zero \\[ \\begin{pmatrix} 3 &amp; 0 &amp; -1 &amp; 0\\\\ 8 &amp; 0 &amp; 0 &amp; -2\\\\ 0 &amp; 2 &amp; -2 &amp; -1\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{pmatrix} = \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix} \\] with each row stating (respectively) that the number \\(C\\), \\(H\\), and \\(O\\) atoms are conserved as required. Next, we write the augmented matrix and perform Gaussian Elimination. \\[ R_2 \\rightarrow R_2 - \\frac{8}{3}R_1\\\\ \\left( \\begin{array}{c c c c | c} 3 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{8}{3} &amp; -2 &amp; 0\\\\ 0 &amp; 2 &amp; -2 &amp; -1 &amp; 0\\end{array} \\right)\\\\ R_2 \\leftrightarrow R_3\\\\ \\left( \\begin{array}{c c c c | c} 3 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; -2 &amp; -1 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{8}{3} &amp; -2 &amp; 0 \\end{array} \\right)\\\\ R_1 \\rightarrow \\frac{1}{3}R_1,\\ R_2 \\rightarrow \\frac{1}{2}R_2,\\ R_3\\rightarrow \\frac{3}{8}R_3\\\\ \\left( \\begin{array}{c c c c | c} 1 &amp; 0 &amp; -\\frac{1}{3} &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; -1 &amp; -\\frac{1}{2} &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{4} &amp; 0\\end{array} \\right). \\] Which is in echelon form. To put into reduced echelon form, we need to work back up the rows and eliminate entries above the pivot, starting with rightmost leading value \\[ R_2 \\rightarrow R_2 + R_3\\\\ \\left(\\begin{array}{c c c c|c} 1 &amp; 0 &amp; -\\frac{1}{3} &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{5}{4} &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{4} &amp; 0\\end{array}\\right)\\\\ R_1 \\rightarrow R_1 + \\frac{1}{3}R_3\\\\ \\left( \\begin{array}{c c c c|c} 1 &amp; 0 &amp; 0 &amp; -\\frac{1}{4} &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{5}{4} &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; -\\frac{3}{4} &amp; 0\\end{array}\\right) \\] which is now in reduced echelon form with \\(x_4\\) being a free variable, which we call \\(\\alpha\\). Hence, the solutions are \\[ \\begin{align} x_1 &amp;= \\frac{1}{4}\\alpha\\\\ x_2 &amp;= \\frac{5}{4}\\alpha\\\\ x_3 &amp;= \\frac{3}{4}\\alpha\\\\ x_4 &amp; = \\alpha \\end{align} \\] The smallest \\(\\alpha\\) that gives all \\(x_i\\) as natural numbers is 4. Hence, \\((x_1,x_2,x_3,x_4) = (1,5,3,4)\\). It makes sense that one of the chemical amounts was a free variable, since it is the ratio of chemicals that is important in balancing the equations, not their absolute value. There were also only three linear equations in four unknowns, so a unique solution was not expected. Electrical networks. Consider the circuit diagram below. The voltage drop across a resistor is (by Ohm’s law) \\(V=IR\\) where \\(V\\) is in volts, the current \\(I\\) is in amps and the resistance \\(R\\) is in ohms. Kirchhoff’s junction law: the sum of currents flowing into a junction is equal to the sum of currents flowing out of that junction. Kirchoff’s Voltage law: The sum of the voltage drops in one direction around a loop equals the sum of the voltage sources in the same direction around the loop. Using these laws it is possible to derive the following set of linear equations for the currents \\(I_1, I_2, I_3\\): \\[\\begin{align*} 11I_1-3I_2&amp;=30\\\\ -3I_1+6I_2-I_3&amp;=5\\\\ -I_2+3I_3&amp;=-25. \\end{align*}\\] Determine the currents. (Extra challenge: derive the equations!). Answer: Form the linear equations into matrix form, and then perform Gaussian elimination on the augmented matrix. \\[ \\begin{pmatrix} 11 &amp; -3 &amp; 0 \\\\ -3 &amp; 6 &amp; -1 \\\\ 0 &amp; -1 &amp; 3\\end{pmatrix} \\begin{pmatrix}I_1\\\\I_2\\\\I_3\\end{pmatrix} = \\begin{pmatrix}30\\\\5\\\\-25\\end{pmatrix}\\\\ \\left(\\begin{array}{c c c | c} 11 &amp; -3 &amp; 0 &amp; 30 \\\\ -3 &amp; 6 &amp; -1 &amp; 5\\\\ 0 &amp; -1 &amp; 3 &amp; -25 \\end{array}\\right)\\\\ R_2 \\rightarrow R_2 + \\frac{3}{11}R_1\\\\ \\left(\\begin{array}{c c c | c} 11 &amp; -3 &amp; 0 &amp; 30 \\\\ 0 &amp; \\frac{57}{11} &amp; -1 &amp; \\frac{145}{11}\\\\ 0 &amp; -1 &amp; 3 &amp; -25\\end{array}\\right)\\\\ R_3 \\rightarrow R_3 + \\frac{11}{57}R_2\\\\ \\left(\\begin{array}{c c c | c} 11 &amp; -3 &amp; 0 &amp; 30\\\\ 0 &amp; \\frac{57}{11} &amp; -1 &amp; \\frac{145}{11}\\\\ 0 &amp; 0 &amp; \\frac{160}{57} &amp; -\\frac{1280}{57}\\end{array}\\right)\\\\ R_1 \\rightarrow \\frac{1}{11}R_1,\\ R_2 \\rightarrow \\frac{11}{57}R_2,\\ R_3 \\rightarrow \\frac{57}{160}R_3\\\\ \\left(\\begin{array}{c c c |c} 1 &amp; -\\frac{3}{11} &amp; 0 &amp; \\frac{30}{11}\\\\ 0 &amp; 1 &amp; -\\frac{11}{57} &amp; \\frac{145}{57}\\\\ 0 &amp; 0 &amp; 1 &amp; -8\\end{array}\\right) \\] This is now in echelon form, and can be brought to reduced echelon form by eliminating the column values above the pivot point, starting with the rightmost leading value. \\[ R_2 \\rightarrow R_2 + \\frac{11}{57}R_3\\\\ \\left(\\begin{array}{c c c |c} 1 &amp; -\\frac{3}{11} &amp; 0 &amp; \\frac{30}{11}\\\\ 0 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1 &amp; -8\\end{array}\\right)\\\\ R_1 \\rightarrow R_1 + \\frac{3}{11}R_2\\\\ \\left(\\begin{array}{c c c |c} 1 &amp; 0 &amp; 0 &amp; 3\\\\ 0 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1 &amp; -8\\end{array}\\right) \\] So \\(I_1 = 3, I_2 = 1, I_3 = -8\\). Network analysis. The diagram below represents the traffic flow in a one-way road network. Each directed edge represents a one-way road and is labelled with the flow through that road. The nodes represent road junctions. By considering the flow in and out of each node and the total flow in and out of the network, formulate a system of linear equations for the unknown flows \\(x_1,\\dotsc,x_5\\) and solve them by Gaussian elimination. You will find there is a “free variable”, but the requirement for non-negative flows puts some restrictions on the variables. In particular, what are the restrictions on \\(x_1\\) and \\(x_5\\)? Answer: The five equations characterising the flow of traffic on the network are \\[\\begin{align*} x_1 + x_2 &amp;= 30\\\\ x_2 + x_3 &amp;= 100\\\\ x_3 + 20 &amp;= x_4\\\\ x_4 + 10 &amp;= x_5\\\\ 100 + x_1 &amp;= x_5 \\end{align*}\\] the first four coming from conservation of flow in and out of each node and the last coming from the total flow in and out of the network. Which can be placed into augmented matrix form as \\[ \\left(\\begin{array}{c c c c c |c} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 30\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 100\\\\ 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 20\\\\ 0 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 10\\\\ -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 100\\end{array}\\right), \\] which is already not far from echelon form. Due to the simple nature of the matrix, the bottom row can be eliminated with the following operations \\[ R_5 \\rightarrow R_5 + R_1 - R_2 - R_3 - R_4\\\\ \\left(\\begin{array}{c c c c c |c} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 30\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 100\\\\ 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 20\\\\ 0 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 10\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right). \\] Now, to move to reduced echelon form \\[ R_3 \\rightarrow -R_3,\\ R_4 \\rightarrow -R_4\\\\ \\left(\\begin{array}{c c c c c |c} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 30\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 100\\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 &amp; -20\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; -10\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right). \\] \\[ R_3 \\rightarrow R_3 + R_4\\\\ \\left(\\begin{array}{c c c c c |c} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 30\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 100\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; -30\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; -10\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right)\\\\ R_2 \\rightarrow R_2 - R_3\\\\ \\left(\\begin{array}{c c c c c |c} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 30\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 130\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; -30\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; -10\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right)\\\\ R_1 \\rightarrow R_1 - R_2\\\\ \\left(\\begin{array}{c c c c c |c} 1 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; -100\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 130\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; -30\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; -10\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right). \\] Here, \\(x_5\\) is a free variable, let’s call it \\(\\alpha\\). This means that the solutions for \\(x_1,x_2,x_3,x_4,x_5\\) are \\[ \\begin{align} x_1 &amp;= \\alpha - 100\\\\ x_2 &amp;= 130 - \\alpha\\\\ x_3 &amp;= -30 + \\alpha\\\\ x_4 &amp;= -10 + \\alpha\\\\ x_5 &amp;= \\alpha\\ . \\end{align} \\] The requirement for non-negative flows of cars means that \\(100\\leq \\alpha \\leq 130\\), hence \\(0\\leq x_1 \\leq 30\\) and \\(100 \\leq x_5 \\leq 130\\). This is reasonable since \\(x_5\\) can never be lower than the 100 flowing into the network on the left, and any contribution from \\(x_1\\) must also leave via \\(x_5\\). "],["exercise-set-7.html", "Exercise Set 7", " Exercise Set 7 Write down the \\(2\\times 2\\) matrix \\(R_\\pi\\) that rotates a vector anticlockwise by \\(\\pi\\). Apply this to a vector \\(\\mathbf{v}=\\left(\\begin{smallmatrix}v_1\\\\v_2\\end{smallmatrix}\\right)\\). Write down the \\(2\\times 2\\) matrix \\(M_x\\) that reflects a vector in the \\(x\\)-axis. Similarly write down the \\(2\\times 2\\) matrix \\(M_y\\) that reflects a vector in the \\(y\\)-axis. Multiply these two matrices to find the transformation that first reflects in the \\(x\\)-axis and then reflects in the \\(y\\)-axis. Compare this to the matrix \\(R_\\pi\\). Note that \\(R^2_\\theta=R_{2\\theta}\\) (why?). Use this to find the “double angle identity” for \\(\\cos\\) and \\(\\sin\\). Can you find other trigonometric identities using \\(R_\\theta\\)? For each of the following pairs of matrices \\(A\\) and \\(B\\), find (when possible), \\(A+B\\), \\(A-B\\), \\(A^2\\), \\(B^2\\), \\(AB\\), \\(BA\\). \\(A = \\begin{pmatrix} 2 &amp; 3 \\\\ 4 &amp; 1 \\end{pmatrix}\\qquad B = \\begin{pmatrix} -1 &amp; 2 \\\\ -2 &amp; 0 \\end{pmatrix}\\) \\(A = \\begin{pmatrix} 4 &amp; -5 \\\\ 6 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}\\qquad B = \\begin{pmatrix} 5 &amp; 2 &amp; -3 \\\\ 1 &amp; 3 &amp; -1 \\\\ 2 &amp; 2 &amp; -1 \\end{pmatrix}\\) \\(A = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1 \\end{pmatrix} \\qquad B = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\end{pmatrix}\\) \\(A = \\begin{pmatrix} -1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1\\\\ 1 &amp; 0 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; -1 \\end{pmatrix} \\qquad B = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; -1 &amp; 1 \\end{pmatrix}\\). Find two \\(3 \\times 3\\) matrices \\(A\\) and \\(B\\) such that \\(AB=BA\\). Now find two \\(3 \\times 3\\) matrices such that \\(AB\\neq BA\\). For any two numbers \\(a\\) and \\(b\\), if \\(ab=0\\) then at least one of \\(a\\) or \\(b\\) must be \\(0\\). Does an analagous result hold for matrices? That is, if \\(AB=0_{n\\times m}\\) must at least one of the matrices \\(A\\) or \\(B\\) be the zero matrix? Determine whether the following matrices are invertible or singular by computing their determinants. If they are invertible, find the inverse. \\(\\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}\\) \\(\\begin{pmatrix} 6 &amp; 3 \\\\ -4 &amp; -2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 4 &amp; -28 &amp; 48 \\\\ -27 &amp; 162 &amp; -216 \\\\ 32 &amp; -160 &amp; 192 \\end{pmatrix}\\) \\(\\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta)\\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; 3 &amp; -5 \\\\ -2 &amp; 1 &amp; 4 \\\\ 1 &amp; 2 &amp; -4 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; -1 &amp; 4 \\\\ 2 &amp; 3 &amp; 3 \\\\ 3 &amp; 1 &amp; 8 \\end{pmatrix}\\) Find the eigenvalues and eigenvectors of each of the following matrices \\(A\\). Determine whether the matrix is diagonalisable and, if so, find the matrices \\(D\\) and \\(P\\) in the diagonalisation \\(D=P^{-1}AP\\). \\(\\begin{pmatrix} 1 &amp; 0 \\\\ 2 &amp; 2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; 2 \\\\ 0 &amp; 1 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; 2 \\\\ 2 &amp; -2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; -2 &amp; -1 \\\\ 2 &amp; 6 &amp; 2 \\\\ -1 &amp; -2 &amp; 1 \\end{pmatrix}\\) \\(\\begin{pmatrix} -2 &amp; 1 &amp; 1 \\\\ -11 &amp; 4 &amp; 5 \\\\ -1 &amp; 1 &amp; 0 \\end{pmatrix}\\) \\(\\begin{pmatrix} 2 &amp; \\sqrt 2 &amp; 0 \\\\ \\sqrt 2 &amp; 2 &amp; \\sqrt 2 \\\\ 0 &amp; \\sqrt 2 &amp; 2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\end{pmatrix}\\) \\(\\begin{pmatrix} 5 &amp; 5 &amp; 1 \\\\ -2 &amp; -1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}\\). For the matrices in a. and d. in the previous question, find a formula for \\(A^n\\). Linear Difference Equations. A population of Wildebeest can be classified into two life stages: juvenile and adult. Each year \\(60\\%\\) of the juveniles survive to become adults, adults give birth on average to \\(0.5\\) juvelines and \\(70\\%\\) of adults survive the year. If there are \\(200\\) juveniles and \\(200\\) adults in one year, what is the long term population of juveniles and adults? What is the long term ratio of juveniles to adults? Hint: write this as a matrix equation and use diagonalisation (save some time and use a computer to find the eigenvalues and eigenvectors for this question). How about in the case when the adult survival rate increases to \\(80\\%\\)? In this case also give the long term growth rate of the juvenile and adult populations. What happens in the case that the adult survival rate drops to \\(60\\%\\)? "],["exercise-set-7-answers.html", "Exercise Set 7 Answers", " Exercise Set 7 Answers Write down the \\(2\\times 2\\) matrix \\(R_\\pi\\) that rotates a vector anticlockwise by \\(\\pi\\). Apply this to a vector \\(\\mathbf{v}=\\left(\\begin{smallmatrix}v_1\\\\v_2\\end{smallmatrix}\\right)\\). Answers: The matrix \\(R_\\theta\\) for anticlockwise rotation by angle \\(\\theta\\) is \\[ R_\\theta = \\begin{pmatrix}\\cos \\theta &amp; - \\sin \\theta \\\\ \\sin\\theta &amp; \\cos\\theta\\end{pmatrix}\\] So the matrix that rotates a vector anticlockwise by \\(\\pi\\) is \\[R_\\pi = \\begin{pmatrix}-1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix}\\] Applying this to a vector: \\[\\begin{pmatrix}-1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix}\\begin{pmatrix}v_1\\\\v_2\\end{pmatrix} = \\begin{pmatrix}-v_1\\\\-v_2\\end{pmatrix}\\] Write down the \\(2\\times 2\\) matrix \\(M_x\\) that reflects a vector in the \\(x\\)-axis. Similarly write down the \\(2\\times 2\\) matrix \\(M_y\\) that reflects a vector in the \\(y\\)-axis. Multiply these two matrices to find the transformation that first reflects in the \\(x\\)-axis and then reflects in the \\(y\\)-axis. Compare this to the matrix \\(R_\\pi\\). Answers: \\[M_x = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix}\\] \\[M_y = \\begin{pmatrix} -1 &amp; 0 \\\\0 &amp; 1\\end{pmatrix}\\] \\[M_xM_y = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix}\\begin{pmatrix} -1 &amp; 0 \\\\0 &amp; 1\\end{pmatrix} = \\begin{pmatrix}-1 &amp; 0 \\\\ 0 &amp; -1\\end{pmatrix} = R_\\pi\\] Note that \\(R^2_\\theta=R_{2\\theta}\\) (why?). Use this to find the “double angle identity” for \\(\\cos\\) and \\(\\sin\\). Can you find other trigonometric identities using \\(R_\\theta\\)? Answers: \\(R^2_\\theta=R_\\theta R_\\theta\\), that is, apply \\(R_\\theta\\) twice: this is the same as rotating by \\(2\\theta\\). \\[R_{2\\theta} = R_\\theta^2 = R_\\theta R_\\theta\\\\ \\begin{pmatrix}\\cos (2\\theta) &amp; -\\sin(2\\theta) \\\\ \\sin(2\\theta) &amp; \\cos(2\\theta)\\end{pmatrix} = \\begin{pmatrix}\\cos \\theta &amp; - \\sin \\theta \\\\ \\sin\\theta &amp; \\cos\\theta\\end{pmatrix}\\begin{pmatrix}\\cos \\theta &amp; - \\sin \\theta \\\\ \\sin\\theta &amp; \\cos\\theta\\end{pmatrix}\\\\ \\begin{pmatrix}\\cos (2\\theta) &amp; -\\sin(2\\theta) \\\\ \\sin(2\\theta) &amp; \\cos(2\\theta)\\end{pmatrix} = \\begin{pmatrix} \\cos^2(\\theta) - \\sin^2(\\theta) &amp; -2\\cos(\\theta)\\sin(\\theta) \\\\ 2\\cos(\\theta)\\sin(\\theta) &amp; \\cos^2(\\theta) - \\sin^2(\\theta)\\end{pmatrix}\\] Where each entry gives the double angle formula for \\(\\sin\\) and \\(\\cos\\). The general sum and difference identities for \\(\\sin\\) and \\(\\cos\\) can be derived by noting that \\(R_{a+b} = R_a R_b\\) and \\(R_{a-b} = R_a R_{-b}\\). Similarly, any identity for positive integer multiples \\(n\\theta\\) can be derived from \\(R_{n\\theta}=R^n_{\\theta}\\). For each of the following pairs of matrices \\(A\\) and \\(B\\), find (when possible), \\(A+B\\), \\(A-B\\), \\(A^2\\), \\(B^2\\), \\(AB\\), \\(BA\\). \\(A = \\begin{pmatrix} 2 &amp; 3 \\\\ 4 &amp; 1 \\end{pmatrix}\\qquad B = \\begin{pmatrix} -1 &amp; 2 \\\\ -2 &amp; 0 \\end{pmatrix}\\) \\(A = \\begin{pmatrix} 4 &amp; -5 \\\\ 6 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix}\\qquad B = \\begin{pmatrix} 5 &amp; 2 &amp; -3 \\\\ 1 &amp; 3 &amp; -1 \\\\ 2 &amp; 2 &amp; -1 \\end{pmatrix}\\) \\(A = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1 \\end{pmatrix} \\qquad B = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\end{pmatrix}\\) \\(A = \\begin{pmatrix} -1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1\\\\ 1 &amp; 0 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; -1 \\end{pmatrix} \\qquad B = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; -1 &amp; 1 \\end{pmatrix}\\). Answers: We have \\[\\begin{align*} &amp;A + B = \\begin{pmatrix} 1 &amp; 5 \\\\ 2 &amp; 1 \\end{pmatrix}, \\qquad A - B = \\begin{pmatrix} 3 &amp; 1 \\\\ 6 &amp; 1 \\end{pmatrix}, \\\\ &amp;A^2 = \\begin{pmatrix} 16 &amp; 9 \\\\ 12 &amp; 13 \\end{pmatrix}, \\qquad B^2 = \\begin{pmatrix} -3 &amp; -2 \\\\ 2 &amp; -4 \\end{pmatrix},\\\\ &amp;AB = \\begin{pmatrix} -8 &amp; 4 \\\\ -6 &amp; 8\\end{pmatrix}, \\qquad BA = \\begin{pmatrix} 6 &amp; -1 \\\\ -4 &amp; -6 \\end{pmatrix}. \\end{align*}\\] Because \\(A\\) is a \\(3\\times 2\\) and \\(B\\) is a \\(3\\times 3\\) matrix, \\(A + B\\), \\(A - B\\), \\(A^2\\) and \\(AB\\) are not defined. We have \\[\\begin{align*} B^2 = \\begin{pmatrix} 21 &amp; 10 &amp; -14 \\\\ 6 &amp; 9 &amp; -5 \\\\ 10 &amp; 8 &amp; -7 \\end{pmatrix}, \\qquad BA = \\begin{pmatrix} 32 &amp; -26 \\\\ 22 &amp; -3 \\\\ 20 &amp; -9 \\end{pmatrix}. \\end{align*}\\] We have \\[\\begin{align*} &amp;A + B = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 2 &amp; 1 &amp; 1 \\end{pmatrix}, \\qquad A - B = \\begin{pmatrix} 1 &amp; 1 &amp; -1 \\\\ -1 &amp; 0 &amp; 1 \\\\ 0 &amp; -1 &amp; 1 \\end{pmatrix}, \\\\ &amp;A^2 = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 2 &amp; 1 &amp; 1 \\end{pmatrix}, \\qquad B^2 = \\begin{pmatrix} 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\end{pmatrix}, \\\\ &amp;AB = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}, \\qquad BA = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}. \\end{align*}\\] Because \\(A\\) is a \\(4\\times 4\\) and \\(B\\) is a \\(4\\times 3\\) matrix, \\(A + B\\), \\(A - B\\), \\(B^2\\), and \\(BA\\) are not defined. We have \\[\\begin{align*} A^2 = \\begin{pmatrix} 3 &amp; 0 &amp; 1 &amp; -2\\\\ 1 &amp; 2 &amp; 2 &amp; 0\\\\ 0 &amp; 1 &amp; 1 &amp; 1\\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{pmatrix}, \\qquad AB = \\begin{pmatrix} 2 &amp; -1 &amp; 0\\\\ 2 &amp; 0 &amp; 2\\\\ 1 &amp; 1 &amp; 1\\\\ 1 &amp; 2 &amp; 0 \\end{pmatrix}. \\end{align*}\\] Find two \\(3 \\times 3\\) matrices \\(A\\) and \\(B\\) such that \\(AB=BA\\). Now find two \\(3 \\times 3\\) matrices such that \\(AB\\neq BA\\). Answers: Two diagonal matrices will always commute: \\[ \\begin{pmatrix}a &amp; 0 &amp; 0 \\\\ 0 &amp; b &amp; 0 \\\\ 0 &amp; 0 &amp; c\\end{pmatrix}\\begin{pmatrix}d &amp; 0 &amp; 0 \\\\ 0 &amp; e &amp; 0 \\\\ 0 &amp; 0 &amp; f\\end{pmatrix} = \\begin{pmatrix}d &amp; 0 &amp; 0 \\\\ 0 &amp; e &amp; 0 \\\\ 0 &amp; 0 &amp; f\\end{pmatrix} \\begin{pmatrix}a &amp; 0 &amp; 0 \\\\ 0 &amp; b &amp; 0 \\\\ 0 &amp; 0 &amp; c\\end{pmatrix} = \\begin{pmatrix}ad &amp; 0 &amp; 0 \\\\ 0 &amp; be &amp; 0 \\\\ 0 &amp; 0 &amp;cf\\end{pmatrix}\\] but there are also non-diagonal matrices that commute. Two matrices that do not commute are \\[A = \\begin{pmatrix}0 &amp; 0 &amp; a \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{pmatrix}, \\quad B = \\begin{pmatrix}0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0\\\\b &amp; 0 &amp; 0\\end{pmatrix}\\] \\[AB = \\begin{pmatrix}ab &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\neq BA = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; ab\\end{pmatrix}\\] For any two numbers \\(a\\) and \\(b\\), if \\(ab=0\\) then at least one of \\(a\\) or \\(b\\) must be \\(0\\). Does an analagous result hold for matrices? That is, if \\(AB=0_{n\\times m}\\) must at least one of the matrices \\(A\\) or \\(B\\) be the zero matrix? Answers: A similar result does not hold for matrices, neither \\(A\\) nor \\(B\\) must be zero if \\(AB=0_{n\\times m}\\). This occurs when the columns of \\(B\\) are in the null space of \\(A\\), that is to say, they are mapped to zero. For instance: \\[\\begin{pmatrix} a &amp; b &amp; 0 \\\\ c &amp; d &amp; 0 \\\\ e &amp; f &amp; 0\\end{pmatrix}\\begin{pmatrix}0 &amp; 0 \\\\ 0 &amp; 0 \\\\g &amp; h\\end{pmatrix} = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}\\] Determine whether the following matrices are invertible or singular by computing their determinants. If they are invertible, find the inverse. \\(\\begin{pmatrix} 2 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}\\) \\(\\begin{pmatrix} 6 &amp; 3 \\\\ -4 &amp; -2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 4 &amp; -28 &amp; 48 \\\\ -27 &amp; 162 &amp; -216 \\\\ 32 &amp; -160 &amp; 192 \\end{pmatrix}\\) \\(\\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta)\\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; 3 &amp; -5 \\\\ -2 &amp; 1 &amp; 4 \\\\ 1 &amp; 2 &amp; -4 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; -1 &amp; 4 \\\\ 2 &amp; 3 &amp; 3 \\\\ 3 &amp; 1 &amp; 8 \\end{pmatrix}\\) Answers: We have \\(\\det(A) = 2\\times 1 - 1\\times 1 = 1\\), so using the formula for the inverse of a \\(2\\times 2\\) matrix: \\[ A^{-1} = \\begin{pmatrix} 1 &amp; -1 \\\\ -1 &amp; 2 \\end{pmatrix}. \\] Alternatively, using the Gaussian elmination method \\[ \\left(\\begin{array}{cc|cc} 2 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 \\end{array}\\right), \\] we perform the EROs \\(R_1 \\to R_1 - R_2\\) and \\(R_2 \\to R_2 - R_1\\) to obtain \\[ \\left(\\begin{array}{cc|cc} 1 &amp; 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 &amp; 2 \\end{array}\\right), \\] as required. We have \\(\\det(A) = 0\\), so the matrix is singular. Alternatively, using the Gaussian elmination method \\[ \\left(\\begin{array}{cc|cc} 6 &amp; 3 &amp; 1 &amp; 0 \\\\ -4 &amp; -2 &amp; 0 &amp; 1 \\end{array}\\right), \\] we perform the EROs \\(R_1 \\to \\frac{1}{6}R_1\\) and then \\(R_2 \\to R_2 + 4R_1\\) to obtain \\[ \\left(\\begin{array}{cc|cc} 1 &amp; \\frac{1}{2} &amp; \\frac{1}{6} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{4}{6} &amp; 1 \\end{array}\\right), \\] which has leading entries in the solution vectors, hence shows that the system of equations is inconsistent, and therefore that \\(A\\) is singular. We have the augmented matrix \\[ \\left(\\begin{array}{ccc|ccc} 4 &amp; -28 &amp; 48 &amp; 1 &amp; 0 &amp; 0\\\\ -27 &amp; 162 &amp; -216 &amp; 0 &amp; 1 &amp; 0\\\\ 32 &amp; -160 &amp; 192 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right). \\] Performing the EROs \\(R_1 \\to 27\\times 8\\times R_1\\), \\(R_2 \\to -32 R_2\\) and \\(R_3 \\to 27 R_3\\) gives \\[ \\left(\\begin{array}{ccc|ccc} 864 &amp; -6048 &amp; 10368 &amp; 216 &amp; 0 &amp; 0\\\\ 864 &amp; -5184 &amp; 6912 &amp; 0 &amp; -32 &amp; 0\\\\ 864 &amp; -4320 &amp; 5184 &amp; 0 &amp; 0 &amp; 27 \\end{array}\\right). \\] Then, performing \\(R_2 \\to R_2 - R_1\\) and \\(R_3 \\to R_3 - R_1\\) gives \\[ \\left(\\begin{array}{ccc|ccc} 864 &amp; -6048 &amp; 10368 &amp; 216 &amp; 0 &amp; 0\\\\ 0 &amp; 864 &amp; -3456 &amp; -216 &amp; -32 &amp; 0\\\\ 0 &amp; 1728 &amp; -5184 &amp; -216 &amp; 0 &amp; 27 \\end{array}\\right). \\] Then, performing \\(R_3 \\to R_3 + 2R_2\\) and \\(R_1 \\to R_1 + 7R_3\\) gives \\[ \\left(\\begin{array}{ccc|ccc} 864 &amp; 0 &amp; 13824 &amp; -1296 &amp; -224 &amp; 0\\\\ 0 &amp; 864 &amp; -3456 &amp; -216 &amp; -32 &amp; 0\\\\ 0 &amp; 0 &amp; 1728 &amp; 216 &amp; 64 &amp; 27 \\end{array}\\right). \\] Then, performing \\(R_1 \\to R_1 + 8R_3\\) and \\(R_2 \\to R_2 + 2R_3\\) gives \\[ \\left(\\begin{array}{ccc|ccc} 864 &amp; 0 &amp; 0 &amp; 432 &amp; 288 &amp; 216\\\\ 0 &amp; 864 &amp; 0 &amp; 216 &amp; 96 &amp; 54\\\\ 0 &amp; 0 &amp; 1728 &amp; 216 &amp; 64 &amp; 27 \\end{array}\\right). \\] Finally, performing \\(R_1 \\to \\frac{1}{864} R_1\\), \\(R_2 \\to \\frac{1}{864} R_2\\) and \\(R_3 \\to \\frac{1}{1728} R_3\\) gives \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; 0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{3} &amp; \\frac{1}{4}\\\\ 0 &amp; 1 &amp; 0 &amp; \\frac{1}{4} &amp; \\frac{1}{9} &amp; \\frac{1}{16}\\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{1}{8} &amp; \\frac{1}{27} &amp; \\frac{1}{64} \\end{array}\\right) \\] and we read of \\[ A^{-1} = \\begin{pmatrix} \\frac{1}{2} &amp; \\frac{1}{3} &amp; \\frac{1}{4}\\\\ \\frac{1}{4} &amp; \\frac{1}{9} &amp; \\frac{1}{16}\\\\ \\frac{1}{8} &amp; \\frac{1}{27} &amp; \\frac{1}{64} \\end{pmatrix}. \\] We have \\(\\det(A) = \\cos^2 \\theta + \\sin^2 \\theta = 1\\), so \\[ A^{-1} = \\begin{pmatrix} \\cos(\\theta) &amp; \\sin(\\theta) \\\\ -\\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix}. \\] Alternatively, using the Gaussian elmination method \\[ \\left(\\begin{array}{cc|cc} \\cos(\\theta) &amp; -\\sin(\\theta) &amp; 1 &amp; 0 \\\\ \\sin(\\theta) &amp; \\cos(\\theta) &amp; 0 &amp; 1 \\end{array}\\right), \\] we perform the EROs \\(R_1 \\to \\cos(\\theta)R_1\\), \\(R_2 \\to \\sin(\\theta)R_2\\) to obtain \\[ \\left(\\begin{array}{cc|cc} \\cos^2(\\theta) &amp; -\\cos(\\theta)\\sin(\\theta) &amp; \\cos(\\theta) &amp; 0 \\\\ \\sin^2(\\theta) &amp; \\cos(\\theta)\\sin(\\theta) &amp; 0 &amp; \\sin(\\theta) \\end{array}\\right), \\] and then \\(R_1 \\to R_1 + R_2\\) to obtain \\[ \\left(\\begin{array}{cc|cc} 1 &amp; 0 &amp; \\cos(\\theta) &amp; \\sin(\\theta) \\\\ \\sin^2(\\theta) &amp; \\cos(\\theta)\\sin(\\theta) &amp; 0 &amp; \\sin(\\theta) \\end{array}\\right), \\] and then \\(R_2 \\to R_2 - \\sin^2(\\theta)R_1\\) and \\(R_2 \\to \\frac{1}{\\sin(\\theta)\\cos(\\theta)}R_2\\) to arrive at \\[ \\left(\\begin{array}{cc|cc} 1 &amp; 0 &amp; \\cos(\\theta) &amp; \\sin(\\theta)\\\\ 0 &amp; 1 &amp; -\\sin(\\theta) &amp; \\frac{\\sin(\\theta)(1 - \\sin^2(\\theta))}{\\sin(\\theta)\\cos(\\theta)} \\end{array}\\right), \\] which, by substituting \\(1 = \\sin^2(\\theta) + \\cos^2(\\theta)\\) second entry of the fourth columns, is equal to \\[ \\left(\\begin{array}{cc|cc} 1 &amp; 0 &amp; \\cos(\\theta) &amp; \\sin(\\theta)\\\\ 0 &amp; 1 &amp; -\\sin(\\theta) &amp; \\cos(\\theta) \\end{array}\\right), \\] as required. Note that \\(A\\) is the rotation anticlockwise by an angle \\(\\theta\\) and \\(A^{-1}\\) is simply rotation clockwise by the angle \\(\\theta\\), i.e. the matrix \\(R_{-\\theta}\\) where one uses the facts that \\(\\cos(-\\theta)=\\cos(\\theta)\\) and \\(\\sin(-\\theta)=-\\sin(\\theta)\\). We have the augmented matrix \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; 3 &amp; -5 &amp; 1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 4 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; -4 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right). \\] We perform EROs \\(R_2 \\to R_2 + 2R_1\\) and \\(R_3 \\to R_3 - R_1\\) to obtain \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; 3 &amp; -5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 7 &amp; -6 &amp; 2 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\end{array}\\right) \\] Swap the second and third rows, i.e. perform ERO \\(R_2 \\leftrightarrow R_3\\), and the perform \\(R_2 \\to -R_2\\) to have \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; 3 &amp; -5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 7 &amp; -6 &amp; 2 &amp; 1 &amp; 0 \\end{array}\\right). \\] Then, perform \\(R_3 \\to R_3 - 7R_2\\) and \\(R_1 \\to R_1 - 3R_2\\) to obtain \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; 0 &amp; -2 &amp; -2 &amp; 0 &amp; 3 \\\\ 0 &amp; 1 &amp; -1 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 &amp; 1 &amp; 7 \\end{array}\\right). \\] Then, perform \\(R_1 \\to R_1 + 2R_3\\) and \\(R_2 \\to R_2 + R_3\\) to obtain \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; 0 &amp; 0 &amp; -12 &amp; 2 &amp; 17 \\\\ 0 &amp; 1 &amp; 0 &amp; -4 &amp; 1 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 &amp; 1 &amp; 7 \\end{array}\\right). \\] So \\[ A^{-1} = \\begin{pmatrix} -12 &amp; 2 &amp; 17 \\\\ -4 &amp; 1 &amp; 6\\\\ -5 &amp; 1 &amp; 7 \\end{pmatrix}. \\] We have the augmented matrix \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; -1 &amp; 4 &amp; 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 3 &amp; 3 &amp; 0 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 8 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right). \\] We perform EROs \\(R_2 \\to R_2 - 2R_1\\) and \\(R_3 \\to R_3 - 3R_1\\) to obtain \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; -1 &amp; 4 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 5 &amp; -5 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 4 &amp; -4 &amp; -3 &amp; 0 &amp; 1 \\end{array}\\right), \\] Then, we perform \\(R_3 \\to R_3 - \\frac{4}{5}R_2\\) to obtain \\[ \\left(\\begin{array}{ccc|ccc} 1 &amp; -1 &amp; 4 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 5 &amp; -5 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -\\frac{7}{5} &amp; -\\frac{4}{5} &amp; 1 \\end{array}\\right), \\] which is in echelon form with (at least one) leading entries in the solution vectors, thus shows that the system of equations is inconsistent, therefore the matrix is singular. Find the eigenvalues and eigenvectors of each of the following matrices \\(A\\). Determine whether the matrix is diagonalisable and, if so, find the matrices \\(D\\) and \\(P\\) in the diagonalisation \\(D=P^{-1}AP\\). \\(\\begin{pmatrix} 1 &amp; 0 \\\\ 2 &amp; 2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; 2 \\\\ 0 &amp; 1 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; 2 \\\\ 2 &amp; -2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; -2 &amp; -1 \\\\ 2 &amp; 6 &amp; 2 \\\\ -1 &amp; -2 &amp; 1 \\end{pmatrix}\\) \\(\\begin{pmatrix} -2 &amp; 1 &amp; 1 \\\\ -11 &amp; 4 &amp; 5 \\\\ -1 &amp; 1 &amp; 0 \\end{pmatrix}\\) \\(\\begin{pmatrix} 2 &amp; \\sqrt 2 &amp; 0 \\\\ \\sqrt 2 &amp; 2 &amp; \\sqrt 2 \\\\ 0 &amp; \\sqrt 2 &amp; 2 \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\end{pmatrix}\\) \\(\\begin{pmatrix} 5 &amp; 5 &amp; 1 \\\\ -2 &amp; -1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}\\). Answers: In the following, we always let \\(B_{\\lambda} = \\lambda I - A\\) and denote the characteristic polynomial \\(p_{A}(\\lambda) = \\det(\\lambda I - A)\\). The characteristic polynomial \\(p_{A}(\\lambda)\\) of \\(A\\) is defined by \\[ p_{A}(\\lambda) = \\begin{vmatrix} \\lambda - 1 &amp; 0 \\\\ -2 &amp; \\lambda - 2 \\end{vmatrix} = (\\lambda-1)(\\lambda-2). \\] Thus, the eigenvalues are \\(\\lambda_1 = 1\\) and \\(\\lambda_2 = 2\\). For \\(\\lambda_1 = 1\\), substitute \\(\\lambda = \\lambda_1 = 1\\) into \\(B_{\\lambda}\\) and solve \\(B_{1}\\mathbf{x} = \\mathbf{0}\\), that is \\[ \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 &amp; 0 \\\\ -2 &amp; -1 \\end{pmatrix}\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2x -y \\end{pmatrix}. \\] Thus, \\(y = -2x\\) (with \\(x\\) arbitrary) and the set of eigenvectors corresponding to the eigenvalue \\(\\lambda_1 = 1\\) is given by \\[ \\begin{pmatrix} \\alpha \\\\ -2\\alpha \\end{pmatrix} \\colon \\alpha \\neq 0. \\] For \\(\\lambda_2 = 2\\), substitute \\(\\lambda = \\lambda_2 = 2\\) into \\(B_{\\lambda}\\) and solve \\(B_{2}\\mathbf{x} = \\mathbf{0}\\), that is \\[ \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ -2 &amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} x \\\\ -2x \\end{pmatrix}, \\] so the solution is \\(x = 0\\), with \\(y\\) arbitrary. The set of eigenvectors corresponding to the eigenvalue \\(\\lambda_2 = 2\\) is \\[ \\begin{pmatrix} 0 \\\\ \\beta \\end{pmatrix} \\colon \\beta \\neq 0 \\] Taking \\(\\alpha = \\beta = 1\\), we have the eigenvectors \\[ \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} \\qquad\\text{and} \\qquad \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] corresponding to the eigenvalues \\(\\lambda_1 = 1\\) and \\(\\lambda_2 = 2\\), respectively. Let \\[ P = \\begin{pmatrix} 1 &amp; 0 \\\\ -2 &amp; 1 \\end{pmatrix}, \\] the matrix whose columns are the eigenvectors listed above. Then we have \\[ D = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}. \\] We have \\[ p_{A}(\\lambda) = \\begin{vmatrix} \\lambda - 1 &amp; -2 \\\\ 0 &amp; \\lambda - 1 \\end{vmatrix} = (\\lambda - 1)^2. \\] so the only eigenvalue is \\(\\lambda = \\lambda_{1,2} = 1\\). To find the eigenvectors, we solve \\[ \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 &amp; -2 \\\\ 0 &amp; 0 \\end{pmatrix}\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} -2y \\\\ 0 \\end{pmatrix}. \\] Thus, we have \\(y = 0\\) and the eigenvectors are given by the set \\[ \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} \\colon \\alpha \\neq 0. \\] Since there is a repeated eigenvalue \\(\\lambda_{1,2} = 1\\) and there does not exists a set of two linearly independent eigenvectors for \\(\\lambda_{1,2}\\), \\(A\\) is not diagonalisable. We have \\[\\begin{align*} p_{A}(\\lambda) =\\det(\\lambda I - A) &amp;= \\begin{vmatrix} \\lambda-1 &amp; -2 \\\\ -2 &amp; \\lambda+2 \\end{vmatrix} \\\\ &amp;= (\\lambda-1)(\\lambda+2)-4 \\\\ &amp;= \\lambda^2+\\lambda-6 \\\\ &amp;= (\\lambda+3)(\\lambda-2), \\end{align*}\\] hence, the eigenvalues are \\(\\lambda_1 = -3\\) and \\(\\lambda_2 = 2\\). For \\(\\lambda_1 = -3\\), substitute \\(\\lambda = \\lambda_1 = -3\\) into \\(B_{\\lambda}\\) and solve \\(B_{-3}\\mathbf{x} = \\mathbf{0}\\), that is \\[ \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -4 &amp; -2 \\\\ -2 &amp; -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}. \\] We note that the top row is twice the bottom row, the solutions satisfy \\(y = -2x\\). We take \\(x = \\alpha\\) to be arbitrary, whence the set of eigenvectors corresponding to the eigenvalue \\(\\lambda_1 = -3\\) is \\[ \\begin{pmatrix} \\alpha \\\\ -2\\alpha \\end{pmatrix} \\colon \\alpha \\neq 0 \\] For \\(\\lambda_2 = 2\\), substitute \\(\\lambda = \\lambda_2 = 2\\) into \\(B_{\\lambda}\\) and solve \\(B_{2}\\mathbf{x} = \\mathbf{0}\\), that is \\[ \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; -2 \\\\ -2 &amp; 4 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}. \\] Again, we note that the bottom row is \\(-2\\) times the top row, thus solutions satisfy \\(x = 2y\\). We take \\(y = \\beta\\) to be arbitrary, whence the set of eigenvectors corresponding to the eigenvalue \\(\\lambda_2 = 2\\) is \\[ \\begin{pmatrix} 2\\beta\\\\ \\beta \\end{pmatrix} \\colon \\beta \\neq 0. \\] Taking \\(\\alpha = \\beta = 1\\), we have the eigenvectors \\[ \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} \\qquad\\text{and}\\qquad \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\] corresponding to the eigenvalues \\(\\lambda_1 = -3\\) and \\(\\lambda_2 = 2\\), respectively. Let \\[ P = \\begin{pmatrix} 1 &amp; 2 \\\\ -2 &amp; 1 \\end{pmatrix}, \\] then we have \\[ D= \\begin{pmatrix} -3 &amp; 0 \\\\ 0 &amp; 2 \\end{pmatrix}. \\] We have \\[\\begin{align*} p_{A}(\\lambda) = \\det(\\lambda I - A) &amp;= \\begin{vmatrix} \\lambda-1 &amp; 2 &amp; 1 \\\\ -2 &amp; \\lambda - 6 &amp; -2 \\\\ 1 &amp; 2 &amp; \\lambda -1 \\end{vmatrix} \\\\ &amp;= (\\lambda-2)^2(\\lambda-4) \\end{align*}\\] hence, the eigenvalues are \\(\\lambda_{1,2} = 2\\) and \\(\\lambda_3 = 4\\) (note that there are only two distinct eigenvalues). For \\(\\lambda_{1,2} = 2\\), substitute \\(\\lambda = \\lambda_{1,2} = 2\\) into \\(B_{\\lambda}\\) and solve \\(B_{-3}\\mathbf{x} = \\mathbf{0}\\), that is \\[ \\begin{pmatrix} 0 \\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2 &amp; 1 \\\\ -2 &amp; -4 &amp; -2 \\\\ 1 &amp; 2 &amp; 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}. \\] Here the second and third rows are multiples of the first row, so the general solution to the above system of equations is \\(x + 2y + z = 0\\). We can take \\(y = \\alpha\\) and \\(z = \\beta\\) to be arbitrary, and get the set of corresponding eigenvectors \\[ \\begin{pmatrix} -2\\alpha-\\beta \\\\ \\alpha \\\\ \\beta \\end{pmatrix} \\colon \\text{$\\alpha, \\beta$ not both equal to zero.} \\] In particular, note that \\[ \\mathbf{v}_1 = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} \\qquad\\text{and}\\qquad \\mathbf{v}_2 = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\] are two linearly independent eigenvectors corresponding to the eigenvalue \\(\\lambda_{1,2} = 2\\), where we take \\((\\alpha,\\beta) = (1,0)\\) to get \\(\\mathbf{v}_1\\) and \\((\\alpha,\\beta) = (0,1)\\) to get \\(\\mathbf{v}_2\\). For \\(\\lambda_3 = 4\\), substitute \\(\\lambda = \\lambda_3 = 4\\) into \\(B_{\\lambda}\\) and solve \\(B_{4}\\mathbf{x} = \\mathbf{0}\\), that is \\[ \\begin{pmatrix} 0 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix} 3 &amp; 2 &amp; 1 \\\\ -2 &amp; -2 &amp; -2 \\\\ 1 &amp; 2 &amp; 3\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}. \\] Applying EROs \\(R_1 \\to R_1 - 3R_3\\) and \\(R_2 \\to R_2 + 2R_3\\) gives \\[ \\begin{pmatrix} 0 &amp; -4 &amp; -8 \\\\ 0 &amp; 2 &amp; 4 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix}. \\] Then, applying \\(R_1 \\leftrightarrow R_3\\), \\(R_2 \\to \\frac{1}{2}R_2\\) and \\(R_3 \\to -\\frac{1}{4}R_3\\) gives \\[ \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 1 &amp; 2 \\end{pmatrix}, \\] and performing \\(R_3 \\to R_3 - R_2\\) and \\(R_1 \\to R_1 - 2R_2\\), we arrive at the reduced echelon form \\[ \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\] There is no leading entry in the third column of the matrix of coefficients, so we may take \\(z = \\gamma\\) to be arbitrary and hence read off the the set of eigenvectors corresponding to the eigenvalue \\(\\lambda_3 = 4\\) as \\[ \\begin{pmatrix} \\gamma \\\\ -2 \\gamma \\\\ \\gamma \\end{pmatrix} \\colon \\gamma \\neq 0. \\] In particular, taking \\(\\gamma = 1\\), we get the eigenvector \\[ \\mathbf{v}_3 = \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}. \\] We define the matrix \\(P\\) by \\[ P = (\\mathbf{v}_1 \\ \\mathbf{v}_2\\ \\mathbf{v}_3) = \\begin{pmatrix} -2 &amp; -1 &amp; 1\\\\ 1 &amp; 0 &amp; -2 \\\\ 0 &amp; 1 &amp; 1 \\end{pmatrix}, \\] then \\[ D = \\begin{pmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 4 \\end{pmatrix} \\] Note: In this example, we are able to diagonalise the \\(3 \\times 3\\) matrix \\(A\\) even though it only had two distinct eigenvalues. This is because we can find two linearly independent eigenvectors corresponding to one of the eigenvalues. We have \\[\\begin{align*} p_{A}(\\lambda) &amp;= \\det(\\lambda I - A) &amp;= \\begin{vmatrix} \\lambda+2 &amp; -1 &amp; -1 \\\\ 11 &amp; \\lambda -4 &amp; -5 \\\\ 1 &amp; -1 &amp; \\lambda \\end{vmatrix}\\\\ &amp;= (\\lambda+1)(\\lambda-1)(\\lambda-2), \\end{align*}\\] hence the eigenvalues of \\(A\\) are \\(\\lambda_1 = -1\\), \\(\\lambda_2 = 1\\), and \\(\\lambda_3 = 2\\). For \\(\\lambda_1 = -1\\), we substitute \\(\\lambda = \\lambda_1 = -1\\) into the matrix \\(B_{\\lambda}\\) and solve \\(B_{-1}\\mathbf{x} = \\mathbf{0}\\). The augmented matrix is \\[ \\begin{pmatrix} 1 &amp; -1 &amp; -1 \\\\ 11 &amp; -5 &amp; -5 \\\\ 1 &amp; -1 &amp; -1 \\end{pmatrix} \\] We perform EROs as follows: \\[\\begin{align*} \\text{$R_2 \\to R_2 - 11R_1$ and $R_3 \\to R_3 - R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; -1\\\\ 0 &amp; 6 &amp; 6 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}; \\\\ \\text{$R_2 \\to \\frac{1}{6}R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; -1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}; \\\\ \\text{$R_1 \\to R_1 + R_2$ and $R_3 \\to R_3 - R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\end{align*}\\] There is no leading entry in the last column so we take \\(z = \\alpha\\) to be arbitrary. The set of eigenvectors corresponding to the eigenvalue \\(\\lambda_1 = -1\\) is thus given by \\[ \\alpha \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} \\colon \\alpha \\neq 0. \\] For \\(\\lambda_2 = 1\\), we substitute \\(\\lambda = \\lambda_2 = 1\\) into \\(B_{\\lambda}\\) and solve \\(B_{1}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} 3 &amp; -1 &amp; -1 \\\\ 11 &amp; -3 &amp; -5 \\\\ 1 &amp; -1 &amp; 1 \\\\ \\end{pmatrix}, \\] which we bring into reduced echelon form as follows: \\[\\begin{align*} \\text{$R_1 \\leftrightarrow R_3$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; 1 \\\\ 11 &amp; -3 &amp; -5 \\\\ 3 &amp; -1 &amp; -1 \\end{pmatrix}; \\\\ \\text{$R_2 \\to R_2 - 11R_1$ and $R_3 \\to R_3 - 3R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; 1 \\\\ 0 &amp; 8 &amp; -16 \\\\ 0 &amp; 2 &amp; -4 \\end{pmatrix}; \\\\ \\text{$R_2 \\to R_2 - 11R_1$ and $R_3 \\to R_3 - R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; 1 \\\\ 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}; \\\\ \\text{$R_1 \\to R_1 + R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\end{align*}\\] There is no leading entry in the last column so we take \\(z = \\beta\\) to be arbitrary. The set of eigenvectors corresponding to the eigenvalue \\(\\lambda_2 = 1\\) is thus given by \\[ \\beta \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\colon \\beta \\neq 0. \\] For \\(\\lambda_3 = 2\\), we substitute \\(\\lambda = \\lambda_3 = 2\\) into \\(B_{\\lambda}\\) and solve \\(B_{3}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} 4 &amp; -1 &amp; -1 \\\\ 11 &amp; -2 &amp; -5 \\\\ 1 &amp; -1 &amp; 2 \\end{pmatrix} \\] which we transform into reduced echelon form via the following process: \\[\\begin{align*} \\text{$R_1 \\leftrightarrow R_3$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; 2 \\\\ 11 &amp; -2 &amp; -5 \\\\ 4 &amp; -1 &amp; -1 \\end{pmatrix}; \\\\ \\text{$R_2 \\to R_2 - 11R_1$ and $R_3 \\to R_3 - 4R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 9 &amp; -27 \\\\ 0 &amp; 3 &amp; -9 \\end{pmatrix}; \\\\ \\text{$R_2 \\to \\frac{1}{9}R_2$ and $R_3 \\to R_3 - 3R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 1 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}; \\\\ \\text{$R_1 \\to R_1 + R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\end{align*}\\] Again, there is no leading entry in the last column, thus, taking \\(z = \\gamma\\), we read off that the set eigenvectors corresponding to \\(\\lambda_3 = 2\\) is given by \\[ \\gamma \\begin{pmatrix} 1 \\\\ 3 \\\\ 1 \\end{pmatrix} \\colon \\gamma\\neq 0. \\] Let \\[ P = (\\mathbf{v}_1 \\ \\ \\mathbf{v}_2 \\ \\ \\mathbf{v}_3) = \\begin{pmatrix} 0 &amp; 1 &amp; 1 \\\\ -1 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}, \\] where, for \\(\\alpha = \\beta = \\gamma = 1\\), \\(\\mathbf{v}_1, \\mathbf{v}_2\\), and \\(\\mathbf{v}_3\\) are eigenvectors corresponding to the eigenvalues \\(\\lambda_1\\), \\(\\lambda_2\\) and \\(\\lambda_3\\), respectively. Then \\[ P^{-1}AP = D, \\] where \\[ D = \\begin{pmatrix} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 2 \\end{pmatrix}. \\] We have \\[\\begin{align*} p_{A}(\\lambda) = \\det(\\lambda I - A) &amp;= \\begin{vmatrix} \\lambda-2 &amp; -\\sqrt{2} &amp; 0 \\\\ - \\sqrt{2} &amp; \\lambda -2 &amp; - \\sqrt{2} \\\\ 0 &amp; - \\sqrt{2} &amp; \\lambda - 2 \\end{vmatrix} \\\\ &amp;= (\\lambda-2)\\begin{vmatrix} \\lambda -2 &amp; -\\sqrt{2} \\\\ -\\sqrt{2} &amp; \\lambda -2 \\end{vmatrix} + \\sqrt{2} \\begin{vmatrix} -\\sqrt{2} &amp; - \\sqrt{2} \\\\ 0 &amp; \\lambda-2 \\end{vmatrix} \\\\ &amp;= (\\lambda-2)\\left[(\\lambda-2)^2-2\\right] + \\sqrt{2} \\left[-\\sqrt{2} (\\lambda-2) \\right] \\\\ &amp;= (\\lambda-2)\\left[ \\lambda^2 - 4 \\lambda\\right] \\\\ &amp;= \\lambda(\\lambda-2)(\\lambda-4), \\end{align*}\\] so the eigenvalues are \\(\\lambda_1=0\\), \\(\\lambda_2=2\\), and \\(\\lambda_3=4\\). For \\(\\lambda_1 = 0\\), we substitute \\(\\lambda = \\lambda_1 = 0\\) into \\(B_{\\lambda}\\) and solve \\(B_{0}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} -2 &amp; - \\sqrt{2} &amp; 0 \\\\ -\\sqrt{2} &amp; -2 &amp; -\\sqrt{2} \\\\ 0 &amp; -\\sqrt{2} &amp; -2 \\end{pmatrix} \\] which we bring into reduced echelon form as follows: \\[\\begin{align*} \\text{$R_1 \\leftrightarrow R_2$} \\colon\\quad &amp; \\begin{pmatrix} - \\sqrt{2} &amp; - 2 &amp; -\\sqrt{2} \\\\ -2 &amp; -\\sqrt{2} &amp; 0 \\\\ 0 &amp; -\\sqrt{2} &amp; -2 \\end{pmatrix};\\\\ \\text{$R_1 \\to -\\frac{1}{\\sqrt{2}}R_1$ and $R_2 \\to R_2 + 2R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; \\sqrt{2} &amp; 1 \\\\ 0 &amp; \\sqrt{2} &amp; 2 \\\\ 0 &amp; -\\sqrt{2} &amp; -2 \\end{pmatrix}; \\\\ \\text{$R_1 \\to R_1 - R_2$ and $R_3 \\to R_3 + R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; \\sqrt{2} &amp; 2 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}; \\\\ \\text{$R_2 \\to \\frac{1}{\\sqrt{2}}R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; \\sqrt{2} \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\end{align*}\\] Thus, the eigenvectors corresponding to the eigenvalue \\(\\lambda_1=0\\) are given by as the set \\[ \\alpha \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\\\ 1 \\end{pmatrix} \\colon \\alpha \\neq 0. \\] Note, that this example shows that it is possible for an eigen to be zero, even though eigen are prohibited from being zero. Note also, that for any eigenvector \\(\\mathbf{x}\\) corresponding to a zero eigenvalue, we have \\(A\\mathbf{x} = \\mathbf{0}\\). For \\(\\lambda_2 = 2\\), we substitute \\(\\lambda = \\lambda_2 =2\\) into \\(B_{\\lambda}\\) and solve \\(B_2\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} 0 &amp; -\\sqrt{2} &amp; 0 \\\\ -\\sqrt{2} &amp; 0 &amp; -\\sqrt{2} \\\\ 0 &amp; - \\sqrt{2} &amp; 0 \\end{pmatrix}, \\] which, applying EROs \\(R_1 \\leftrightarrow R_2\\) and \\(R_3 \\to R_3 - R_2\\) we transform to \\[ \\begin{pmatrix} -\\sqrt{2} &amp; 0 &amp; -\\sqrt{2} \\\\ 0 &amp; - \\sqrt{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] and, performing \\(R_i \\to -\\frac{1}{\\sqrt{2}}R_i\\), for \\(i=1,2\\), to \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\] Thus, the set of eigenvectors corresponding to the eigenvalue \\(\\lambda_2 = 2\\), is given as \\[ \\beta \\begin{pmatrix} -1 \\\\0 \\\\ 1 \\end{pmatrix} \\colon \\beta \\neq 0. \\] For \\(\\lambda_3=4\\), we substitute \\(\\lambda = \\lambda_3 = 4\\) into the matrix \\(B_{\\lambda}\\) and solve \\(B_4\\mathbf{x} = \\mathbf{0}\\). We get the augmented matrix \\[ \\begin{pmatrix} 2 &amp; - \\sqrt{2} &amp; 0\\\\ - \\sqrt{2} &amp; 2 &amp; -\\sqrt{2} \\\\ 0 &amp; -\\sqrt{2} &amp; 2 \\end{pmatrix} \\] which, applying EROs, can be transformed into REF as follows: \\[\\begin{align*} \\text{$R_1 \\leftrightarrow R_2$} \\colon\\quad &amp; \\begin{pmatrix} -\\sqrt{2} &amp; 2 &amp; -\\sqrt{2} \\\\ 2 &amp; -\\sqrt{2} &amp; 0 \\\\ 0 &amp; -\\sqrt{2} &amp; 2 \\end{pmatrix} \\\\ \\text{$R_2 \\to R_2 + \\sqrt{2}R_1$} \\colon\\quad &amp; \\begin{pmatrix} -\\sqrt{2} &amp; 2 &amp; -\\sqrt{2} \\\\ 0 &amp; \\sqrt{2} &amp; - 2 \\\\ 0 &amp; -\\sqrt{2} &amp; 2 \\end{pmatrix} \\\\ \\text{$R_3 \\to R_3 + R_2$ and $R_1 \\to -\\frac{1}{\\sqrt{2}} R_1$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; -\\sqrt{2} &amp; 1 \\\\ 0 &amp; \\sqrt{2} &amp; - 2 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\\\ \\text{$R_1 \\to R_1 + R_2$ and $R_2 \\to \\frac{1}{\\sqrt{2}} R_2$} \\colon\\quad &amp; \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -\\sqrt{2} \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\end{align*}\\] Thus, the eigenvectors corresponding to the eigenvalue \\(\\lambda_3=4\\) are given by as the set \\[ \\gamma \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix} \\colon \\gamma \\neq 0. \\] Using \\(\\alpha = \\beta = \\gamma = 1\\) in the above sets to obtain particular eigenvectors \\(\\mathbf{v}_1, \\mathbf{v}_2\\) and \\(\\mathbf{v}_3\\), respectively, define the matrix \\(P\\) by \\[ P = (\\mathbf{v}_1 \\ \\ \\mathbf{v}_2 \\ \\ \\mathbf{v}_3) = \\begin{pmatrix} 1 &amp; -1 &amp; 1 \\\\ -\\sqrt{2} &amp; 0 &amp; \\sqrt{2} \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix}. \\] Then, \\[ P^{-1}AP = D, \\] where \\[ D = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 4 \\end{pmatrix}. \\] We have \\[\\begin{align*} p_{A}(\\lambda) = \\det(\\lambda I - A) &amp;= \\begin{vmatrix} \\lambda-1 &amp; 1 &amp; 1 \\\\ -1 &amp; \\lambda+1 &amp; 0 \\\\ -1 &amp; 0 &amp; \\lambda +1 \\end{vmatrix}\\\\ &amp;= (\\lambda-1) \\begin{vmatrix} \\lambda +1 &amp; 0 \\\\ 0 &amp; \\lambda+1 \\end{vmatrix} - \\begin{vmatrix} -1 &amp; 0 \\\\ -1 &amp; \\lambda+1 \\end{vmatrix} + \\begin{vmatrix} -1 &amp; \\lambda+1 \\\\ -1 &amp; 0 \\end{vmatrix} \\\\ &amp;= (\\lambda-1)(\\lambda+1)^2 + (\\lambda+1) + (\\lambda+1) \\\\ &amp;= (\\lambda+1)(\\lambda^2+1), \\end{align*}\\] thus the eigenvalues are \\(\\lambda_1 = -1\\), \\(\\lambda_2 = i\\) and \\(\\lambda_3 = -i\\) (where \\(i^2=-1\\)). For \\(\\lambda_1 = -1\\), we substitute \\(\\lambda = \\lambda_1 = -1\\) into \\(B_{\\lambda}\\) and solve \\(B_{-1}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} -2 &amp; 1 &amp; 1 \\\\ -1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\end{pmatrix}, \\] which we transform into reduced echelon form via \\(R_3 \\to R_3 - R_2\\), \\(R_1 \\to R_1 - 2R_2\\) and \\(R_1 \\leftrightarrow R_2\\) to obtain \\[ \\begin{pmatrix} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] and thus find the set of eigenvectors \\[ \\alpha \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} \\colon \\alpha \\neq 0 \\] corresponding to \\(\\lambda_1 = -1\\). For \\(\\lambda_2 = i\\), we substitute \\(\\lambda = \\lambda_2 = i\\) into \\(B_{\\lambda}\\) and solve \\(B_{i}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} i-1 &amp; 1 &amp; 1 \\\\ -1 &amp; i+1 &amp; 0 \\\\ -1 &amp; 0 &amp; i+1 \\end{pmatrix}. \\] First, performing \\(R_1 \\leftrightarrow R_2\\), \\(R_2 \\to R_2 + (i-1)R_1\\) and \\(R_3 \\to R_3 - R_1\\) gives \\[ \\begin{pmatrix} -1 &amp; i+1 &amp; 0 \\\\ 0 &amp; 1 + (i+1)(i-1) &amp; 1 \\\\ 0 &amp; -i-1 &amp; i+1 \\end{pmatrix} = \\begin{pmatrix} -1 &amp; i+1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; -(i+1) &amp; i+1 \\end{pmatrix}, \\] and applying \\(R_3 \\to R_3 - (i+1)R_2\\) and \\(R_1 \\to R_1 + (i+1)R_2\\) gives \\[ \\begin{pmatrix} -1 &amp; 0 &amp; (i+1) \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] which is in REF and thus we find the set of eigenvectors \\[ \\beta \\begin{pmatrix} (i+1) \\\\ 1 \\\\ 1 \\end{pmatrix} \\colon \\beta \\neq 0 \\] corresponding to \\(\\lambda_2 = i\\). For \\(\\lambda_3 = -i\\), we substitute \\(\\lambda = \\lambda_3 = -i\\) into \\(B_{\\lambda}\\) and solve \\(B_{-i}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} -i-1 &amp; 1 &amp; 1 \\\\ -1 &amp; -i+1 &amp; 0 \\\\ -1 &amp; 0 &amp; -i+1 \\end{pmatrix}. \\] First, performing \\(R_1 \\leftrightarrow R_2\\), \\(R_2 \\to R_2 - (i+1)R_1\\) and \\(R_3 \\to R_3 - R_1\\) gives \\[ \\begin{pmatrix} -1 &amp; -(i-1) &amp; 0 \\\\ 0 &amp; 1 + (i+1)(i-1) &amp; 1 \\\\ 0 &amp; i-1 &amp; -i+1 \\end{pmatrix} = \\begin{pmatrix} -1 &amp; -(i-1) &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; i-1 &amp; -(i-1) \\end{pmatrix}, \\] and applying \\(R_3 \\to R_3 + (i-1)R_2\\) and \\(R_1 \\to R_1 - (i-1)R_2\\) gives \\[ \\begin{pmatrix} -1 &amp; 0 &amp; -(i-1) \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] which is in REF and thus we find the set of eigenvectors \\[ \\gamma \\begin{pmatrix} -(i-1) \\\\ 1 \\\\ 1 \\end{pmatrix} \\colon \\gamma \\neq 0 \\] corresponding to \\(\\lambda_3 = -i\\). Setting \\(\\alpha = \\beta = \\gamma = 1\\), we find that the matrix \\[ P = \\begin{pmatrix} 0 &amp; i+1 &amp; -i+1 \\\\ -1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix} \\] satisfies \\[ P^{-1}AP = D, \\] where \\[ D = \\begin{pmatrix} -1 &amp; 0 &amp; 0 \\\\ 0 &amp; i &amp; 0 \\\\ 0 &amp; 0 &amp; -i \\end{pmatrix}. \\] We have \\[\\begin{align*} p_{A}(\\lambda) = \\det(\\lambda I - A) &amp;= \\begin{vmatrix} \\lambda-5 &amp; -5 &amp; -1 \\\\ 2 &amp; \\lambda+1 &amp; 0 \\\\ -1 &amp; -1 &amp; \\lambda-1 \\end{vmatrix}\\\\ &amp;= (\\lambda-5)\\begin{vmatrix} \\lambda+1 &amp; 0 \\\\ -1 &amp; \\lambda-1 \\end{vmatrix} - (-5) \\begin{vmatrix} 2 &amp; 0 \\\\ -1 &amp; \\lambda-1 \\end{vmatrix} \\\\ &amp;\\phantom{={}} + (-1) \\begin{vmatrix} 2 &amp; \\lambda+1 \\\\ -1 &amp; -1 \\end{vmatrix} \\\\ &amp;= (\\lambda-5)(\\lambda+1)(\\lambda-1) + 5(2(\\lambda-1)) - (-2 + \\lambda + 1) \\\\ &amp;= (\\lambda-1)\\big((\\lambda-5)(\\lambda+1) + 10 - 1\\big) \\\\ &amp;= (\\lambda-1)\\big(\\lambda^2 - 4\\lambda + 4\\big) \\\\ &amp;= (\\lambda-1)(\\lambda - 2)^2, \\end{align*}\\] thus the eigenvalues are \\(\\lambda_1 = 1\\), \\(\\lambda_{2,3} = 2\\). For \\(\\lambda_1 = 1\\), we substitute \\(\\lambda = \\lambda_1 = 1\\) into \\(B_{\\lambda}\\) and solve \\(B_{1}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} -4 &amp; -5 &amp; -1 \\\\ 2 &amp; 2 &amp; 0 \\\\ -1 &amp; -1 &amp; 0 \\end{pmatrix}. \\] Applying EROs \\(R_1 \\to R_1 + 2R_2\\) and \\(R_3 \\to R_3 + \\frac{1}{2}R_2\\) gives \\[ \\begin{pmatrix} 0 &amp; -1 &amp; -1 \\\\ 2 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\] Performing \\(R_1 \\leftrightarrow R_2\\), \\(R_1 \\to R_1 + 2R_2\\) and \\(R_1 \\to \\frac{1}{2}R_1\\) and \\(R_2 \\to -R_2\\) gives \\[ \\begin{pmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] which is in REF and thus we find the set of eigenvectors \\[ \\alpha\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} \\colon \\alpha \\neq 0 \\] corresponding to \\(\\lambda_1 = 1\\). For \\(\\lambda_{2,3} = 2\\), we substitute \\(\\lambda = \\lambda_{2,3} = 2\\) into \\(B_{\\lambda}\\) and solve \\(B_{2}\\mathbf{x} = \\mathbf{0}\\). We have the augmented matrix \\[ \\begin{pmatrix} -3 &amp; -5 &amp; -1 \\\\ 2 &amp; 3 &amp; 0 \\\\ -1 &amp; -1 &amp; 1 \\end{pmatrix}. \\] Performing \\(R_1\\leftrightarrow R_3\\), \\(R_2 \\to R_2 + 2R_1\\) and \\(R_3 \\to R_3 - 3R_1\\), we have \\[ \\begin{pmatrix} -1 &amp; -1 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\\\ 0 &amp; -2 &amp; -4 \\end{pmatrix}, \\] and further, performing \\(R_3 \\to R_3 + 2R_2\\), \\(R_1 \\to -R_1\\) and \\(R_1 \\to R_1 - R_2\\) gives \\[ \\begin{pmatrix} 1 &amp; 0 &amp; -3 \\\\ 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}, \\] which is in REF and thus we find the set of eigenvectors \\[ \\beta\\begin{pmatrix} 3 \\\\ -2 \\\\ 1 \\end{pmatrix} \\colon \\beta \\neq 0 \\] corresponding to \\(\\lambda_{2,3} = 2\\). This shows that there are only two linearly independent eigenvectors, hence \\(A\\) is not diagonalisable. For the matrices in a. and d. in the previous question, find a formula for \\(A^n\\). Answers: First, note that \\(P^{-1}AP = D\\) and thus \\(A = PDP^{-1}\\). Then, \\[\\begin{align*} A^2 &amp;= \\left(PDP^{-1}\\right)^2 \\\\ &amp;= \\left(PDP^{-1}\\right)\\left(PDP^{-1}\\right) \\\\ &amp;= PDP^{-1}PDP^{-1} \\\\ &amp;= PD I_n DP^{-1} \\\\ &amp;= P D^{2} P^{-1}. \\end{align*}\\] and more generally \\(A^n = P D^{n} P^{-1}\\). Now for a. \\[\\begin{align*} A^n &amp;= \\begin{pmatrix} 1 &amp; 0 \\\\ -2 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 2^n \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 2 &amp; 1\\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} 1 &amp; 0 \\\\ -2 &amp; 2^n \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 2 &amp; 1 \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} 1 &amp; 0 \\\\ 2^{n+1}-2 &amp; 2^n \\end{pmatrix}. \\end{align*}\\] For d., first compute \\(P^{-1}\\) using Gaussian elimination. Starting with the augmented matrix \\[\\left(\\begin{array}{rrr|rrr} 2 &amp; -1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; -2 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right).\\] Applying \\(R_1 \\leftrightarrow R_2\\) gives \\[\\left(\\begin{array}{rrr|rrr} -1 &amp; 0 &amp; -2 &amp; 0 &amp; 1 &amp; 0 \\\\ 2 &amp; -1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right),\\] performing \\(R_2 \\to R_2 + 2R_1\\) gives \\[\\left(\\begin{array}{rrr|rrr} -1 &amp; 0 &amp; -2 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; -3 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right),\\] performing \\(R_3 \\to R_3 + R_2\\) gives \\[\\left(\\begin{array}{rrr|rrr} -1 &amp; 0 &amp; -2 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; -3 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; -2 &amp; 1 &amp; 2 &amp; 1 \\end{array}\\right),\\] performing \\(R_1 \\to -R_1\\), \\(R_2 \\to -R_2\\) and \\(R_3 \\to -\\frac{1}{2}R_3\\) gives \\[\\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 2 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; -1 &amp; -2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -\\frac{1}{2} &amp; -1 &amp; -\\frac{1}{2} \\end{array}\\right),\\] and with \\(R_2 \\to R_2 - 3R_3\\) and \\(R_1 \\to R_1 - 2R_3\\) we arrive at \\[\\left(\\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; \\frac{1}{2} &amp; 1 &amp; \\frac{3}{2} \\\\ 0 &amp; 0 &amp; 1 &amp; -\\frac{1}{2} &amp; -1 &amp; -\\frac{1}{2} \\end{array}\\right).\\] We deduce that \\[ P^{-1} = \\frac{1}{2} \\begin{pmatrix} 2 &amp; 2 &amp; 2 \\\\ 1 &amp; 2 &amp; 3 \\\\ -1 &amp; -2 &amp; -1 \\end{pmatrix}. \\] We have \\[ D^n = \\begin{pmatrix} 2^n &amp; 0 &amp; 0\\\\ 0 &amp; 2^n &amp; 0 \\\\ 0 &amp; 0 &amp; 4^n \\end{pmatrix}, \\] and so \\[\\begin{align*} A^n &amp;= P D^n P^{-1} \\\\ &amp;=\\frac{1}{2} \\begin{pmatrix} 2^{n+2}-2^n-4^n &amp;2^{n+2}-2^{n+1}-2^{2 n+1} &amp; 2^{n+2}-3\\times 2^n-4^n \\\\ -2^{n+1}+2^{2 n+1} &amp;-2^{n+1}+2^{2 n+2} &amp;-2^{n+1}+2^{2 n+1} \\\\ 2^n-4^n &amp; 2^{n+1}-2^{2 n+1} &amp; 3\\times 2^n-4^n \\end{pmatrix}. \\end{align*}\\] Linear Difference Equations. A population of Wildebeest can be classified into two life stages: juvenile and adult. Each year \\(60\\%\\) of the juveniles survive to become adults, adults give birth on average to \\(0.5\\) juvelines and \\(70\\%\\) of adults survive the year. If there are \\(200\\) juveniles and \\(200\\) adults in one year, what is the long term population of juveniles and adults? What is the long term ratio of juveniles to adults? Hint: write this as a matrix equation and use diagonalisation (save some time and use a computer to find the eigenvalues and eigenvectors for this question). How about in the case when the adult survival rate increases to \\(80\\%\\)? In this case also give the long term growth rate of the juvenile and adult populations. What happens in the case that the adult survival rate drops to \\(60\\%\\)? Answers: Let \\(x_1(n)\\) denote the population of juveniles and \\(x_2(n)\\) the population of adults in year \\(n\\). Then we can formulate this as the coupled system of linear difference equations: \\[ \\begin{pmatrix} x_1(n)\\\\ x_2(n) \\end{pmatrix} = \\begin{pmatrix} 0&amp;0.5\\\\ 0.6&amp;0.7 \\end{pmatrix} \\begin{pmatrix} x_1(n-1)\\\\ x_2(n-1) \\end{pmatrix}. \\] Let \\[ \\mathbf{x}(n)= \\begin{pmatrix} x_1(n)\\\\ x_2(n) \\end{pmatrix} \\quad \\text{and} \\quad A= \\begin{pmatrix} 0&amp;0.5\\\\ 0.6&amp;0.7 \\end{pmatrix} \\] then the solution is \\[ \\mathbf{x}(n)=A^n\\mathbf{x}(0). \\] If the matrix \\(A\\) is diagonalisable, then we can find \\(A^n\\) by diagonalisation. We find that \\(A\\) has eigenvalues \\(\\lambda_1=1\\) and \\(\\lambda_2=-\\frac{3}{10}\\) with corresponding eigenvectors \\[\\mathbf{v}_1=\\begin{pmatrix}1\\\\2\\end{pmatrix},\\quad \\mathbf{v}_2\\begin{pmatrix}1\\\\-3/5\\end{pmatrix}.\\] So \\[ A^n=\\frac{1}{13} \\begin{pmatrix} 1&amp;1\\\\ 2&amp;-\\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} 1&amp;0\\\\ 0&amp;-\\frac{3}{10} \\end{pmatrix}^n \\begin{pmatrix} 3&amp;5\\\\ 10&amp;-5 \\end{pmatrix} \\] and \\[\\begin{align} \\mathbf{x}(n)&amp;=\\frac{1}{13}\\times 1^n \\times (3x_1(0)+5x_2(0)) \\begin{pmatrix} 1\\\\ 2 \\end{pmatrix} \\\\ &amp;+\\frac{1}{13}\\times\\left(-\\frac{3}{10}\\right)^n\\times(10x_1(0)-5x_2(0)) \\begin{pmatrix} 1\\\\ -\\frac{3}{5} \\end{pmatrix}\\tag{15.1} \\end{align}\\] or writing each population separately \\[ \\begin{aligned} x_1(n)&amp;=\\frac{1}{13}(3x_1(0)+5x_2(0))+\\frac{1}{13}\\left(-\\frac{3}{10}\\right)^n(10x_1(0)-5x_2(0))\\\\ x_2(n)&amp;=\\frac{2}{13}(3x_1(0)+5x_2(0))+\\frac{1}{13}\\left(-\\frac{3}{10}\\right)^n\\left( -\\frac{3}{5}\\right)(10x_1(0)-5x_2(0)). \\end{aligned} \\] From (15.1) we can see that in the long term the populations settle down to: \\[\\begin{equation} \\lim\\limits_{n\\to\\infty}\\mathbf{x}(n)=\\frac{1}{13}(3x_1(0)+5x_2(0)) \\begin{pmatrix} 1\\\\ 2 \\end{pmatrix}\\tag{15.2} \\end{equation}\\] that is, \\[\\begin{align*} \\lim\\limits_{n\\to\\infty}x_1(n)&amp;=\\frac{1}{13}(3x_1(0)+5x_2(0))\\approx 123,\\\\ \\lim\\limits_{n\\to\\infty}x_2(n)&amp;=\\frac{2}{13}(3x_1(0)+5x_2(0))\\approx 246. \\end{align*}\\] From (15.2) the long term ratio of juveniles to adults is \\[ \\lim\\limits_{n\\to\\infty}\\frac{x_1(n)}{x_2(n)}=\\frac{1}{2} \\] i.e. \\(1:2\\). Now when the survival rate increases to 80% the matrix \\(A\\) becomes \\[ A= \\begin{pmatrix} 0&amp;0.5\\\\ 0.6&amp;0.8 \\end{pmatrix} \\] which is still diagonalisable and has eigenvalues and corresponding eigenvectors \\[\\begin{align*} \\lambda_1=\\frac{1}{10} (4 + \\sqrt{46})\\approx 1.08,\\quad &amp; \\quad\\lambda_2=\\frac{1}{10} (4 - \\sqrt{46})\\approx-0.28,\\\\ \\mathbf{v}_1= \\begin{pmatrix} \\frac{1}{6}(-4 + \\sqrt{46})\\\\ 1 \\end{pmatrix},\\quad &amp; \\quad \\mathbf{v}_2=\\begin{pmatrix} \\frac{1}{6}(-4 - \\sqrt{46})\\\\ 1 \\end{pmatrix}. \\end{align*}\\] If we let \\(\\mathcal{P}=(\\mathbf{v}_1,\\mathbf{v}_2)\\) and write \\[ \\mathbf{x}(0)_\\mathcal{P}=\\begin{pmatrix} \\mu_1\\\\ \\mu_2 \\end{pmatrix} \\] then \\[ \\mathbf{x}(n)=\\mu_1\\lambda_1^n\\mathbf{v}_1+\\mu_2\\lambda_2^n\\mathbf{v}_2. \\] Since \\(|\\lambda_1|&gt;1\\) and \\(|\\lambda_2|&lt;1\\), for large \\(n\\) \\[ \\mathbf{x}(n)\\approx\\mu_1\\lambda_1^nv_1 \\] and hence both \\(x_1(n)\\to\\infty\\) and \\(x_2(n)\\to \\infty\\), with the long term ratio \\(x_1:x_2\\) being \\(\\frac{1}{6}(-4 + \\sqrt{46}):1\\), or approximately \\(1:2.2\\). In the case that the adult survival rate drops to 60%, we have \\[ A= \\begin{pmatrix} 0&amp;0.5\\\\ 0.6&amp;0.6 \\end{pmatrix} \\] which is diagonalisable and has eigenvalues and corresponding eigenvectors \\[\\begin{align*} \\lambda_1=\\frac{1}{10} (3 + \\sqrt{39})\\approx 0.92,\\quad &amp; \\quad\\lambda_2=\\frac{1}{10} (3 - \\sqrt{39})\\approx-0.32,\\\\ \\mathbf{v}_1= \\begin{pmatrix} \\frac{1}{6}(-3 + \\sqrt{39})\\\\ 1 \\end{pmatrix},\\quad &amp; \\quad \\mathbf{v}_2=\\begin{pmatrix} \\frac{1}{6}(-3 - \\sqrt{39})\\\\ 1 \\end{pmatrix}. \\end{align*}\\] If we let \\(\\mathcal{P}=(\\mathbf{v}_1,\\mathbf{v}_2)\\) and write \\[ \\mathbf{x}(0)_\\mathcal{P}=\\begin{pmatrix} \\mu_1\\\\ \\mu_2 \\end{pmatrix} \\] then \\[ \\mathbf{x}(n)=\\mu_1\\lambda_1^nv_1+\\mu_2\\lambda_2^nv_2, \\] but now since both \\(|\\lambda_1|&lt;1\\) and \\(|\\lambda_2|&lt;1\\), \\[ \\lim\\limits_{n\\to\\infty}x(n)=\\lim\\limits_{n\\to\\infty}\\begin{pmatrix} x_1(n)\\\\ x_2(n) \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix} \\] and the population dies out. "],["exercise-set-8.html", "Exercise Set 8", " Exercise Set 8 Find the first and second dervatives of the following expressions. \\(y=3x^5+2x-1\\) \\(y=4x^{-1}\\) \\(y=x^{\\frac{1}{2}}+x^{\\frac{1}{3}}\\) \\(y=x^7+\\sin(x)\\) \\(y=e^x+5\\) Find the derivative of \\((1+2x)(x-x^2)\\) in two ways: first, expand and find the derivative; second, use the product rule. Your answers should agree! Use the product rule to show that \\((af)&#39;(x)=af&#39;(x)\\) for any differentiable function \\(f\\) and number \\(a\\). Use the chain rule to show that \\((f(ax+b))&#39;=af&#39;(ax+b)\\) for any differentiable function \\(f\\) and numbers \\(a\\) and \\(b\\). Find the derivatives of the following functions (you may like to use the additional rules you have just derived in the previous two questions). \\(f(x)=\\sqrt{2}\\sin(x)\\) \\(f(x)=\\ln(2)\\ln(x)\\) \\(f(x)=\\cos(3x)\\) \\(f(x)=\\sin(5x+2)\\) \\(f(x)=3e^{2x}\\) \\(f(x)=3e^{2x+1}\\) Find the derivatives of the following functions using the product rule. \\(f(x)=3x^2\\sin(x)\\) \\(f(x)=\\sin(x)\\cos(x)\\) \\(f(x)=(x^3-x)e^x\\) \\(f(x)=x\\ln(x)\\) Find the derivatives of the following functions using the chain rule. \\(f(x)=\\cos(x^2)\\) \\(f(x)=\\sin(\\cos(x))\\) \\(f(x)=e^{\\sin(x)}\\) \\(f(x)=\\sin^{100}(x)\\) Find the derivatives of the following functions using the quotient rule. \\(f(x)=\\dfrac{1}{1+x^2}\\) \\(f(x)=\\dfrac{x^2}{1+x^2}\\) \\(f(x)=\\dfrac{x^3}{e^{3x}}\\) \\(f(x)=\\dfrac{x-\\sqrt{x}}{x^2}\\) \\(f(x)=\\dfrac{\\sin(x)}{\\cos(x)}\\) Find the derivatives of the following functions. \\(f(x)=\\sin^2(\\sin(x))\\) \\(f(x)=\\ln(x^2)\\) \\(f(x)=a^x\\) for any \\(a&gt;0\\) \\(f(x)=e^x\\sin(x^2)\\) Show that \\(\\frac{d}{dx}\\sinh(x)=\\cosh(x)\\) and \\(\\frac{d}{dx}\\cosh(x)=\\sinh(x)\\). Find all local maxima and minima of the following functions. Are there any global maxima or minima? Also sketch their graphs. \\(f(x)=x^2+3x+1\\) \\(f(x)=x^3-3x\\) A particle moving in a straight line has displacement \\(x\\) as a function of time \\(t\\geq 0\\) given by \\[x=-t^{3}+5t^{2}+t.\\] Find the velocity \\(v\\) and acceleration \\(a\\). What is the initial velocity? What is the largest positive displacement? At what time does the particle return to the origin? Find the points of inflection of the following functions. \\(f(x)=\\dfrac{x^3}{3}-\\dfrac{x^2}{2}-2x+5\\) \\(f(x)=x+\\sin(x)\\) A rectangular box with no lid is made from a thin sheet of metal. The base is \\(2x\\text{ mm}\\) long and \\(x\\text{ mm}\\) wide, and the volume is \\(48000\\text{ mm}^3\\). Show that the area \\(A\\) of metal used is given by \\[A=2x^2+144000x^{-1} \\text{ mm}^2.\\] Find the value of \\(x\\) for which the minimum area of metal is used along with the value of the minimum area. "],["exercise-set-8-answers.html", "Exercise Set 8 Answers", " Exercise Set 8 Answers Find the first and second dervatives of the following expressions. \\(y=3x^5+2x-1\\) \\(y=4x^{-1}\\) \\(y=x^{\\frac{1}{2}}+x^{\\frac{1}{3}}\\) \\(y=x^7+\\sin(x)\\) \\(y=e^x+5\\) Answers: \\[\\begin{align*} y &amp;= 3x^5 + 2x -1$\\\\ \\frac{dy}{dx} &amp;= 15x + 2\\\\ \\frac{d^2y}{dx^2} &amp;= 15 \\end{align*}\\] \\[\\begin{align*} y&amp;=4x^{-1}\\\\ \\frac{dy}{dx} &amp;= -4x^{-2}\\\\ \\frac{d^2y}{dx^2} &amp;= 8x^{-3} \\end{align*}\\] \\[\\begin{align*} y &amp;= x^\\frac{1}{2} + x^\\frac{1}{3}\\\\ \\frac{dy}{dx} &amp;= \\frac{1}{2}x^{-\\frac{1}{2}} + \\frac{1}{3}x^{-\\frac{2}{3}}\\\\ \\frac{d^2y}{dx^2} &amp;= -\\frac{1}{4}x^{-\\frac{3}{2}} + -\\frac{2}{9}x^{-\\frac{5}{3}} \\end{align*}\\] \\[\\begin{align*} y &amp;= x^6 \\sin(x)\\\\ \\frac{dy}{dx} &amp;= 7x^6 + \\cos (x)\\\\ \\frac{d^2y}{dx^2} &amp;= 42x^5 - \\sin(x) \\end{align*}\\] \\[\\begin{align*} y &amp;= e^x + 5\\\\ \\frac{dy}{dx} &amp;= e^x\\\\ \\frac{d^2y}{dx^2} &amp;= e^x \\end{align*}\\] Find the derivative of \\((1+2x)(x-x^2)\\) in two ways: first, expand and find the derivative; second, use the product rule. Your answers should agree! Answer: Method 1, expand and find the derivative: \\[\\begin{align*} y &amp;= (1+2x)(x-x^2)\\\\ &amp;=x - x^2 + 2x^2 - 2x^3\\\\ &amp;=-2x^3 + x^2 + x\\\\ \\frac{dy}{dx} &amp;= -6x^2 + 2x + 1 \\end{align*}\\] Method 2, product rule. Let \\(y=h(x) = f(x)g(x)\\) where \\(f(x) = (1+2x)\\) and \\(g(x) = (x-x^2)\\) \\[\\begin{align*} y &amp;= (1+2x)(x-x^2)\\\\ y&#39; &amp;= 2(x-x^2)+(1+2x)(1-2x)\\\\ &amp;=-6x^2 + 2x + 1 \\end{align*}\\] And as expected, we have obtained the same answer with both methods. Use the product rule to show that \\((af)&#39;(x)=af&#39;(x)\\) for any differentiable function \\(f\\) and number \\(a\\). Answer: We wish to show that \\((af)&#39;(x) = a f(x)\\), or in Leibniz notation, \\(\\frac{d}{dx}af(x)=a \\frac{d}{dx}f(x)\\). This can be shown using product rule, where one of the functions is \\(f(x)\\) and the other is simply the constant function \\(g(x)=a\\) \\[\\begin{align*} (gf)&#39;(x) &amp;= g(x)&#39; f(x) + g(x)f&#39;(x)\\\\ &amp;= 0f(x)+g(x)f&#39;(x)\\\\ &amp;=a f&#39;(x) \\end{align*}\\] or in Leibniz notation \\[\\begin{align*} \\frac{d}{dx}(g(x)f(x))&amp;=\\frac{d}{dx}(g(x))f(x)+g(x)\\frac{d}{dx}f(x)\\\\ &amp;=0f(x)+g(x)\\frac{d}{dx}f(x)\\\\ &amp;=a\\frac{d}{dx}f(x) \\end{align*}\\] using \\(g&#39;(x)=\\frac{d}{dx}g(x)=0\\) because the derivative of a constant function is zero. Use the chain rule to show that \\((f(ax+b))&#39;=af&#39;(ax+b)\\) for any differentiable function \\(f\\) and numbers \\(a\\) and \\(b\\). Answer: To show \\((f(ax+b))&#39; = a f&#39;(ax+b)\\), we can recognise the derivative as an application of chain rule: \\((f(g(x)))&#39; = g&#39;(x)f&#39;(g(x))\\), where \\(g(x) = ax+b\\). Since \\(g&#39;(x) = a\\), we have that \\[(f(ax+b))&#39;= a (f&#39;(ax+b))\\] Find the derivatives of the following functions (you may like to use the additional rules you have just derived in the previous two questions). \\(f(x)=\\sqrt{2}\\sin(x)\\) \\(f(x)=\\ln(2)\\ln(x)\\) \\(f(x)=\\cos(3x)\\) \\(f(x)=\\sin(5x+2)\\) \\(f(x)=3e^{2x}\\) \\(f(x)=3e^{2x+1}\\) Answers: \\[f(x)= \\sqrt{2} \\sin (x)\\\\ f&#39;(x) = \\sqrt{2} \\cos (x)\\] \\[f(x) = \\ln (2) \\ln(x)\\\\ f&#39;(x) = \\frac{\\ln 2}{x}\\] \\[f(x) = \\cos (3x)\\\\ f&#39;(x) = -3 \\sin (3x)\\] \\[f(x) = \\sin (5x+2)\\\\ f&#39;(x) = 5 \\cos (5x+2)\\] \\[f(x) = 3e^{2x}\\\\ f&#39;(x) = 6e^{2x}\\] \\[f(x) = 3e^{2x+1}\\\\ f&#39;(x) = 6e^{2x+1}\\] Find the derivatives of the following functions using the product rule. \\(f(x)=3x^2\\sin(x)\\) \\(f(x)=\\sin(x)\\cos(x)\\) \\(f(x)=(x^3-x)e^x\\) \\(f(x)=x\\ln(x)\\) Answers: \\[f(x) = 3x^2 \\sin (x)\\\\ f&#39;(x) = 6x \\sin(x) + 3x^2 \\cos(x)\\] \\[ f(x) = \\sin(x)\\cos(x) \\quad(=\\frac{1}{2}\\sin(2x))\\\\ f&#39;(x) = \\cos^2(x) - \\sin^2(x) = \\cos(2x)\\] \\[f(x) = (x^3 - x)e^x\\\\ f&#39;(x) = (3x^2 - 1)e^x + (x^3 - x)e^x\\\\ = (x^3 + 3x^2 - x - 1)e^x\\] \\[f(x) = x \\ln (x)\\\\ f&#39;(x) = \\ln(x) + 1\\] Find the derivatives of the following functions using the chain rule. \\(f(x)=\\cos(x^2)\\) \\(f(x)=\\sin(\\cos(x))\\) \\(f(x)=e^{\\sin(x)}\\) \\(f(x)=\\sin^{100}(x)\\) Answers: These are all applications of chain rule for taking derivatives of functions of the from \\(f(x) = h(g(x))\\) This is a composition of two functions, \\(h(x) = \\cos(x)\\), \\(g(x) = x^2\\) \\[f&#39;(x) = -2x \\sin(x^2)\\] Taking the derivative of \\(h(g(x))\\) where \\(h(x) = \\sin(x)\\) and \\(g(x) = \\cos(x)\\) \\[f&#39;(x) = -\\sin(x)\\cos(\\cos(x))\\] \\(f&#39;(x) = (h(g(x)))&#39;\\) where \\(h(x) = e^x\\) and \\(g(x) = \\sin(x)\\) \\[f&#39;(x) = \\cos(x)e^{\\sin(x)}\\] Here, \\(h(x) = x^{100}\\) and \\(g(x) = \\sin(x)\\) \\[f&#39;(x) = 100\\cos(x)\\sin^{99}(x)\\] Find the derivatives of the following functions using the quotient rule. \\(f(x)=\\dfrac{1}{1+x^2}\\) \\(f(x)=\\dfrac{x^2}{1+x^2}\\) \\(f(x)=\\dfrac{x^3}{e^{3x}}\\) \\(f(x)=\\dfrac{x-\\sqrt{x}}{x^2}\\) \\(f(x)=\\dfrac{\\sin(x)}{\\cos(x)}\\) Answers: \\(f(x) = \\frac{1}{1+x^2}\\). Applying quotient rule: \\[f&#39;(x) = \\frac{-2x}{(1+x^2)^2}\\] \\(f(x) = \\frac{x^2}{1+x^2}\\). \\[f&#39;(x) = \\frac{2x(1+x^2) - x^2(2x)}{(1+x^2)^2} = \\frac{2x}{(1+x^2)^2}\\] \\(f(x) = \\frac{x^3}{e^{3x}}\\) \\[f&#39;(x) = \\frac{3x^2 \\times e^{3x} - x^3\\times 3e^{3x}}{e^{6x}} = \\frac{3(x-1)x^2}{e^{3x}}\\] \\(f(x) = \\frac{x-x^{\\frac{1}{2}}}{x^2}\\) \\[f&#39;(x) = \\frac{(1-\\frac{1}{2}x^{-\\frac{1}{2}})\\times x^2 - (x-x^\\frac{1}{2})\\times 2x}{x^4}\\\\ = \\frac{x^2 - \\frac{1}{2}x^{\\frac{3}{2}} - 2x^2 + 2x^{\\frac{3}{3}}}{x^4}\\\\ =\\frac{-x^2 + \\frac{3}{2}x^{\\frac{3}{2}}}{x^4} = -x^{-2} + \\frac{3}{2}x^{-\\frac{5}{2}}\\] \\(f(x) = \\frac{\\sin(x)}{\\cos{x}}\\) \\[f&#39;(x) = \\frac{\\cos^2(x) + \\sin^2(x)}{\\cos^2 (x)} = \\frac{1}{\\cos^2(x)} = \\sec^2(x)\\] Find the derivatives of the following functions. \\(f(x)=\\sin^2(\\sin(x))\\) \\(f(x)=\\ln(x^2)\\) \\(f(x)=a^x\\) for any \\(a&gt;0\\) \\(f(x)=e^x\\sin(x^2)\\) Answers: We recognise this as a composition of three functions \\(f(x)=s(t(u(x)))\\) with \\(u(x)=\\sin(x)\\) \\(t(x)=\\sin(x)\\) \\(s(x)=x^2\\) First applying the chain rule to \\(s\\) and \\(t\\) \\[\\frac{df(x)}{dx}=\\frac{ds(t)}{dt}\\frac{dt(u(x))}{dx}\\] Then applying the chain rule to \\(t\\) and \\(u\\) \\[\\frac{df(x)}{dx}=\\frac{ds(t)}{dt}\\left(\\frac{dt(u)}{du}\\frac{du(x)}{dx}\\right)\\] so we have \\[\\frac{df(x)}{dx}=2\\sin(\\sin(x))\\times \\cos(\\sin(x))\\times\\cos(x).\\] We could first turn the power into a product to obtain \\(f(x)=2\\ln(x)\\), then the derivative is \\[\\frac{df(x)}{dx}=2\\frac{1}{x}.\\] Alternatively, we could use the chain rule. Letting \\(u(x)=x^2\\) we have \\[\\begin{align*} \\frac{df(x)}{dx}&amp;=\\frac{\\ln(u)}{du}\\frac{du(x)}{dx}\\\\ &amp;=\\frac{1}{u}2x\\\\ &amp;=\\frac{2x}{x^2}\\\\ &amp;=\\frac{2}{x}. \\end{align*}\\] We can use the natural logarithm to obtain \\[f(x)=e^{\\ln(a^x)}=e^{x\\ln(a)}.\\] Then \\[\\frac{df(x)}{dx}=\\ln(a)e^{x\\ln(a)}=\\ln(a)e^{\\ln(a^x)}=\\ln(a)a^x.\\] We have \\(f(x)=s(x)t(u(x))\\) with \\(s(x)=e^x\\), \\(t(x)=\\sin(x)\\), \\(u(x)=x^2\\). We have a product and a composition. First applying the product rule \\[\\frac{df(x)}{dx}=\\frac{ds(x)}{dx}t(u(x))+s(x)\\frac{dt(u(x))}{dx}.\\] Then appying the chain rule \\[ \\frac{df(x)}{dx}=\\frac{ds(x)}{dx}t(u(x))+s(x)\\frac{dt(u)}{du}\\frac{du(x)}{dx}.\\] So, we have \\[\\begin{align*} \\frac{df(x)}{dx}&amp;=e^x\\sin(x^2)+e^x\\cos(x^2)2x\\\\ &amp;=e^x(\\sin(x^2)+2x\\cos(x^2)) \\end{align*}\\] Show that \\(\\frac{d}{dx}\\sinh(x)=\\cosh(x)\\) and \\(\\frac{d}{dx}\\cosh(x)=\\sinh(x)\\). Answer: \\(\\dfrac{d}{dx}\\sinh(ax)=\\dfrac{d}{dx}\\dfrac{e^{ax}-e^{-ax}}{2}=\\dfrac{ae^{ax}-(-a)e^{-ax}}{2}=a\\dfrac{e^{ax}+e^{-ax}}{2}=a\\cosh(ax)\\) and \\(\\dfrac{d}{dx}\\cosh(ax)=\\dfrac{d}{dx}\\dfrac{e^{ax}+e^{-ax}}{2}=\\dfrac{ae^{ax}+(-a)e^{-ax}}{2}=a\\dfrac{e^{ax}-e^{-ax}}{2}=a\\sinh(ax)\\). Find all local maxima and minima of the following functions. Are there any global maxima or minima? Also sketch their graphs. \\(f(x)=x^2+3x+1\\) \\(f(x)=x^3-3x\\) Answers: For graph sketching we want to find the following: Where the curve crosses the \\(x\\)-axis and \\(y\\)-axis. The coordinates of any local maxima and minima. The general shape of the curve. Any asymptotes. We have \\(f&#39;(x)=2x+3\\). Stationary points are where \\(f&#39;(x)=0 \\implies x=\\frac{-3}{2}\\). To check for local maxima and minima we find the second derivative: \\(f&#39;&#39;(x)=2\\). Since this is always postive, we only have one local minimum at \\(x=\\frac{-3}{2}\\), \\(y=-\\frac{5}{4}\\). From the dominant \\(x^2\\) term being positive, we see this is a parabola which opens upwards, hence this minimum will be the global minimum and there are no local or global maxima. The curve crosses the \\(x\\)-axis where \\(f(x)=0\\). Using the quadratic formula: \\[x=\\frac{-3\\pm\\sqrt{9-4}}{2}=\\frac{-3\\pm\\sqrt{5}}{2}\\] so the approximate solutions are \\(x_1=-2.618\\) and \\(x_2=-0.382\\). The curve crosses the \\(y\\)-axis at \\(f(0)=1\\). Sketch: We have \\(f&#39;(x)=3x^2-3\\). Stationary points are where \\(f&#39;(x)=0 \\implies x=\\pm 1\\). To check for local maxima and minima we find the second derivative: \\(f&#39;&#39;(x)=6x\\). This implies that \\((-1,2)\\) is a local maximum and \\((1,-2)\\) is a local minimum. From the dominant \\(x^3\\) term, we see that \\(f(x)\\) can take arbitrarily large positive and negative values, hence there are no global maxima or minima. The curve crosses the \\(x\\)-axis where \\(f(x)=0\\). Factorising the cubic, we have: \\[f(x)=x(x-\\sqrt{3})(x+\\sqrt{3})\\] so the solutions are \\(x_0=0\\) and approximately \\(x_1=-1.732\\), \\(x_2=1.732\\). The curve crosses the \\(y\\)-axis at \\(f(0)=0\\). Sketch: A particle moving in a straight line has displacement \\(x\\) as a function of time \\(t\\geq 0\\) given by \\[x=-t^{3}+5t^{2}+t.\\] Find the velocity \\(v\\) and acceleration \\(a\\). What is the initial velocity? What is the largest positive displacement? At what time does the particle return to the origin? Answers: \\[\\begin{align*} x &amp;= -t^3 + 5t^2 + t\\\\ v &amp;= \\frac{dx}{dt} = -3t^2 + 10t + 1\\\\ a &amp;= \\frac{dv}{dt} = \\frac{d^2x}{dt^2} = -6t + 10 \\end{align*}\\] \\(v(0) = 1\\) To find the largest positive displacement, we must find the turning points of \\(x\\). These times are found by solving for \\(v(t) = 0\\) \\[v = -3t^2 + 10t + 1 = 0\\\\ \\text{solved by } t = \\frac{5}{3}\\pm \\frac{2\\sqrt{7}}{3}\\] The solution \\(\\frac{5}{3}-\\frac{2\\sqrt{7}}{3}\\) is in negative time, and the shape of the dominant \\(-t^3\\) term in \\(x\\) suggests that the maximum will be at time \\(\\frac{5}{3}+\\frac{2\\sqrt{7}}{3}\\), but it is prudent to check the second derivative of \\(x\\) (which is \\(a\\)). \\[a(\\frac{5}{3}-\\frac{2\\sqrt{7}}{3}) = 4\\sqrt{7}\\\\ a(\\frac{5}{3}+\\frac{2\\sqrt{7}}{3}) = -4\\sqrt{7}\\] And hence the maximum displacement does indeed occur at time \\(t=\\frac{5}{3}+\\frac{2\\sqrt{7}}{3}\\), and \\(x(\\frac{5}{3}+\\frac{2\\sqrt{7}}{3}) \\approx 21.9\\) To find when the particle returns to the origin, we must solve for \\(x(t) = 0\\). \\[-t^3 + 5t^2 + t = 0\\\\ -t^2 + 5t + 1 = 0\\\\ t = \\frac{5 \\pm \\sqrt{29}}{2}\\] Where one solution is for a negative \\(t\\), so the particle returns to the origin at \\(t = \\frac{5 +\\sqrt{29}}{2} \\approx 5.19\\) Find the points of inflection of the following functions. \\(f(x)=\\dfrac{x^3}{3}-\\dfrac{x^2}{2}-2x+5\\) \\(f(x)=x+\\sin(x)\\) Answer: To find points of inflection, we find the points at which the second derivative is zero and the third derivative is non-zero. \\[f(x) = \\frac{x^3}{3} - \\frac{x^2}{2} - 2x + 5\\\\ f&#39;(x) = x^2 - x - 2\\\\ f&#39;(x) = 0 = x^2 - x- 2 = (x+1)(x-2)\\\\ f&#39;&#39;(x) = 2x-1\\\\ f&#39;&#39;&#39;(x)=2\\] The second derivative is zero at \\(x=\\frac{1}{2}\\), and the second derivative also changes signs either side of this point. Alternatively we simply note that \\(f&#39;&#39;(x=\\frac{1}{2}) = 0\\) and \\(f&#39;&#39;&#39;(x=\\frac{1}{2})\\neq 0\\), so it is an inflection point by the Third Derivative Test for Inflection Points. \\[f(x) = \\sin (x) + x\\\\ f&#39;(x) = \\cos(x)+1\\] So there are stationary points at \\(x = \\frac{\\pi}{2} + 2\\pi k\\) for any integer \\(k\\). \\(f&#39;&#39;(x) = -\\sin(x)\\), which is zero whenever \\(x = k\\pi\\) for integer \\(k\\). The third derivative, \\(f&#39;&#39;&#39;(x) = -\\cos(x)\\) is non zero whenever \\(x \\neq \\frac{\\pi}{2} + 2\\pi k\\). So the points of inflection, by the Third Derivative Test, are whenever \\(x=2\\pi k\\) for integer \\(k\\). Note that half of these are also stationary points. A rectangular box with no lid is made from a thin sheet of metal. The base is \\(2x\\text{ mm}\\) long and \\(x\\text{ mm}\\) wide, and the volume is \\(48000\\text{ mm}^3\\). Show that the area \\(A\\) of metal used is given by \\[A=2x^2+144000x^{-1} \\text{ mm}^2.\\] Find the value of \\(x\\) for which the minimum area of metal is used along with the value of the minimum area. Answer: The volume of the box is given by \\(2x\\times x \\times h = 48000\\text{ mm}^3\\). We write the height in terms of volume and base area: \\(h = \\frac{48000}{2x^2}\\) The area of metal used is given by \\[A = 2x^2 + 2\\times 2x\\times \\frac{48000}{x^2} + 2 \\times x \\times \\frac{48000}{2x^2}\\\\ =2x^2 + \\frac{96000}{x} + \\frac{48000}{x}\\\\ = 2x^2 + \\frac{144000}{x}\\] We wish to minimise the area of metal used to create the appropriate volume of box. This requires finding the minimum of the Area function. \\[A&#39; = 4x - \\frac{144000}{x^2}\\\\ A&#39; = 0 \\Rightarrow 4x^3 = 144000\\\\ \\Rightarrow x = 33.019mm\\] We check this is a minimum by looking at the second derivative \\[A&#39;&#39;=4+\\frac{288000}{x^3}\\] which is positive for any postive \\(x\\) value, hence we do have a minimum by the second derivative test. The corresponding area is \\[ A= 6541.63\\text{mm}^3.\\] "],["exercise-set-9.html", "Exercise Set 9", " Exercise Set 9 Find \\(\\dfrac{dy}{dx}\\) for the following by implicit differentiation. \\(x^2y+3xy^3-x=3\\) \\(\\sin(x^2y^2)=x\\) \\(xy^2=y+e^{xy}\\) Consider the curve \\(x^2+3xy+y^2+2x-7=0\\). Use implicit differentiation to find \\(\\dfrac{dy}{dx}\\) and hence compute the equation of the tangent at the point \\((1,1)\\). Find the slope of the circle \\(x^2+y^2=25\\) at the point \\((-3,-4)\\). Hence find the equations of the tangent and normal at that point. Find \\(\\dfrac{dy}{dx}\\) and \\(\\dfrac{d^2y}{dx^2}\\) for the following by implicit differentiation. \\(y+\\sin(y)=x\\) \\(x^2-xy+y^2=3\\) Find \\(\\dfrac{dy}{dx}\\) and \\(\\dfrac{d^2y}{dx^2}\\) for the following parametric equations. \\(y=\\cos(2t)\\) and \\(x=\\sin(t)\\) \\(y=\\dfrac{3+2t}{1+t}\\) and \\(x=\\dfrac{2-3t}{1+t}\\) \\(y=3\\sin(\\theta)-\\sin^3(\\theta)\\) and \\(x=\\cos^3(\\theta)\\) Find the derivative of the following inverse functions by implicit differentiation. \\(y=\\sin^{-1}(x)\\) \\(y=\\cosh^{-1}(3x)\\) \\(y=\\tan^{-1}(4x^2)\\) Find \\(\\dfrac{dy}{dx}\\) for the following using logarithmic differentiation. \\(y=a^x\\) \\(y=\\dfrac{(x-4)^7(2x+3)^2}{(4x+7)^3}\\) Find \\(\\dfrac{dy}{dx}\\) for the following. \\(y=x^x\\) \\(y=(\\tanh(x))^x\\) \\(x^3+\\sin(xy)=xy^2\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
